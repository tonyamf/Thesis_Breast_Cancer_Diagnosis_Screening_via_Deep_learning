{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Thesis_seg.ipynb","provenance":[{"file_id":"1w_6VJ4t_ZJOvj6Fpn2A_ag567XplpUwO","timestamp":1628588813197},{"file_id":"1WVv6ONxZPFOolt8plebjKRoahWp3Fu04","timestamp":1627995752898},{"file_id":"1CBDBm79WjbAgXH9kJRI04fWV3dcGjkhT","timestamp":1627750778184},{"file_id":"1yhEeWMmcKO-sOIAo3tTeAQiOdbPwQPgu","timestamp":1627719755769}],"collapsed_sections":["dExRVQI8aLbl","7ld0iL6l7sd7"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVsrGkj4Zn2L","executionInfo":{"status":"ok","timestamp":1628808060964,"user_tz":-60,"elapsed":34,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"2a1fa628-82fa-4f35-aff4-35d848bd9612"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jEOxHu9xCjK","executionInfo":{"status":"ok","timestamp":1628808065666,"user_tz":-60,"elapsed":3920,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"343086ee-df02-4041-ba9a-4d9a43a044a0"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n","\u001b[?25l\r\u001b[K     |▌                               | 10 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 42.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 43.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 679 kB 12.0 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.13.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z_AwPGsuiVnc"},"source":["try:\n","  %tensorflow_version 2.x\n","  import tensorflow as tf\n","  device_name = tf.test.gpu_device_name()\n","  if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","  print('Found GPU at: {}'.format(device_name))\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_pQCOmISAQBu"},"source":["## Enabling and testing the TPU\n","\n","First, you'll need to enable TPUs for the notebook:\n","\n","- Navigate to Edit→Notebook Settings\n","- select TPU from the Hardware Accelerator drop-down\n","\n","Next, we'll check that we can connect to the TPU:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpvUOuC3j27n","executionInfo":{"status":"ok","timestamp":1628701744341,"user_tz":-60,"elapsed":364,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"b3e1e873-5898-4d0f-adbb-487df29d4e5e"},"source":["try:\n","  %tensorflow_version 2.x\n","  import tensorflow as tf\n","  print(\"Tensorflow version \" + tf.__version__)\n","\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except:\n","  pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tensorflow version 2.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vFIMfPmgQa0h"},"source":["import re\n","import numpy as np\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dExRVQI8aLbl"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"K_X6Tj9TaBOF","executionInfo":{"status":"ok","timestamp":1628808136849,"user_tz":-60,"elapsed":4537,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"d698a0ea-de4a-416d-b0c8-cfbd789f4034"},"source":["#@title Libraries\n","%pylab inline\n","import imgaug as ia\n","import imgaug.augmenters as iaa\n","import imutils\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import pylab as pylab\n","import matplotlib.image as mpimg\n","from PIL import Image as im\n","# import segmentation_models_pytorch as smp\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from scipy import ndimage, misc\n","from skimage.filters import threshold_otsu\n","from skimage.segmentation import clear_border\n","from skimage.measure import label, regionprops\n","from skimage.morphology import closing, square\n","from skimage.color import label2rgb\n","import matplotlib.patches as mpatches\n","from scipy.misc import face\n","from scipy.signal.signaltools import wiener\n","import sys\n","import numpy as np\n","import skimage.color\n","import skimage.filters\n","import skimage.io\n","import skimage.viewer\n","from skimage import feature, io, color, filters\n","from skimage.transform import hough_line, hough_line_peaks\n","from skimage.feature import canny\n","from skimage.filters import sobel\n","from skimage.draw import polygon\n","from skimage import exposure\n","from skimage.transform import resize\n","from PIL import Image\n","import scipy.ndimage as snd\n","import tensorflow as tf\n","#from meta-pseudo-labels.\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Viewer requires Qt\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Bf0ES9ZmG0DP"},"source":["# Mount file syste,"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2yW4Ik2Gy22","executionInfo":{"status":"ok","timestamp":1628808116855,"user_tz":-60,"elapsed":48084,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"79fbc530-1022-4260-b801-0342fb61a8f4"},"source":["#@title Driver mount\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True, timeout_ms=12000000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w7kfgMnUadk0"},"source":["# Import data"]},{"cell_type":"code","metadata":{"id":"X30cgZKOacpN"},"source":["#@title Default title text\n","\n","# import os\n","# input_dir = \"/content/drive/MyDrive/Thesis/MINI-DDSM-Complete-JPEG-8/Data.xlsx\"\n","root = '\\\\content\\\\drive\\\\MyDrive\\\\Thesis\\\\MINI-DDSM-Complete-JPEG-8\\\\'\n","# dfAll = pd.read_excel(input_dir)\n","# dfAll.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5mfjVVbJXZ2B"},"source":["# Selecting and Sampling"]},{"cell_type":"code","metadata":{"id":"igm2upJaCm5u"},"source":["# data = dfAll.loc[(dfAll.Status == 'Benign') & (dfAll.Tumour_Contour != '-')].copy()\n","# data.reset_index(inplace = True, drop = True)\n","# Cancer_data = dfAll.loc[(dfAll.Status == 'Cancer') & (dfAll.Tumour_Contour != '-')].copy()\n","# Cancer_data.reset_index(inplace = True, drop = True)\n","# Normal_data = dfAll.loc[(dfAll.Status == 'Normal')].copy()\n","# Normal_data.reset_index(inplace = True, drop = True)\n","# data = data.append(Cancer_data, ignore_index = True)\n","# data = data.append(Normal_data, ignore_index = True)\n","# data = data.sample(frac=1).reset_index(drop=True)\n","# data.head()to"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mToRCE_3ACK8"},"source":["data = pd.read_csv('/content/drive/MyDrive/Thesis/pos/data.csv')\n","# dfc_ = pd.read_csv('/content/drive/MyDrive/Thesis/pos/dataS.csv')\n","dfPS = pd.read_csv('/content/drive/MyDrive/Thesis/pos/PSdata.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"oi68VREkUhxU","executionInfo":{"status":"ok","timestamp":1628808200141,"user_tz":-60,"elapsed":60616,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"d948dfd8-1928-4047-daec-220417c17015"},"source":["cv = cv2.imread(dfPS['mask'][0])\n","plt.imshow(cv)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f66795820d0>"]},"metadata":{"tags":[]},"execution_count":9},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAALwAAAD8CAYAAADNEc7HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOe0lEQVR4nO3dX4zU5X7H8fcHKIinnLirLUEOaZcjN9yoFDkYjTk1ccFNBI1Jw4mpGzVyLiRpk2pcey5KchISSOyFiZFguhGrZWPSGshJra7GRL0AwQblz+nCApLjuiwxqxQlwLLz7cU8a6Z7dpk/Ozuz8HxeyZP5zfP7Mb/vMp/M/uZhmK8iArNczGp2AWaN5MBbVhx4y4oDb1lx4C0rDrxlpeGBl7RWUp+kfkldjT6/5U2NXIeXNBs4BjwAfAXsB34VEUcbVoRlrdGv8KuA/og4GRGXgR5gfYNrsIzNafD5FgN/KLn/FfCL0gMkbQQ2prt/1aC67PryTUT82UQ7Gh34siJiB7ADQJI/92C1OD3ZjkZf0gwAS0ru/yzNmTVEowO/H1gmqU3SXGADsKfBNVjGGnpJExFXJG0C3gVmA90RcaSRNVjeGrosWS1fw1uNPouIlRPt8L+0WlYceMuKA29ZceAtKw68ZcWBt6w48JYVB96y4sBbVhx4y4oDb1lx4C0rDrxlxYG3rDjwlhUH3rLiwFtWHHjLigNvWXHgLSsOvGXFgbesOPCWFQfesuLAW1YceMuKA29ZceAtK1MKvKQvJR2SdFDSgTTXKqlX0vF025LmJeml1MzsC0kr6vEDmFWjHq/wfx0Rd5R8W2sX8EFELAM+SPcBHgSWpbEReKUO5zarynRc0qwHdqbtncDDJfOvR9Fe4CZJi6bh/GaTmmrgA3hP0mepGRnAwogYTNtngIVpe6KGZovHP6CkjZIOjF0imdXTVDuA3BsRA5L+HOiV9D+lOyMiqm1q4KZmNp2m9AofEQPp9izwNsU+rENjlyrp9mw63A3NrOlqDrykn0haMLYNtAOHKTYp60yHdQK70/Ye4PG0WrMaOFdy6WPWEFO5pFkIvC1p7HH+LSL+S9J+4C1JT1Hsl/k36fj/BDqAfuAC8MQUzm1WEzc1s+uRm5qZgQNvmXHgLSsOvGXFgbesOPCWFQfesuLAW1YceMuKA29ZceAtKw68ZcWBt6w48JYVB96y4sBbVhx4y4oDb1lx4C0rDrxlxYG3rDjwlhUH3rLiwFtWHHjLigNvWXHgLStlAy+pW9JZSYdL5qru4ySpMx1/XFLnROcym3YRcdUB3AesAA6XzG0DutJ2F7A1bXcA7wACVgP70nwrcDLdtqTtlgrOHR4eNYwDk2Wq7Ct8RHwEDI+brraP0xqgNyKGI+JboBdYW+7cZvVW6zV8tX2cKurvZDbdptrjqaY+TleTmqNtLHugWQ1qfYWvto9Txf2dImJHRKyc7Avtzaai1sBX28fpXaBdUkta0WlPc2aNVcFKyS5gEBiheO39FHAzxS7bx4H3gdZ0rICXgRPAIWBlyeM8SbG/Uz/wRLnzepXGYwpj0lUa93iy65F7PJmBA2+ZceAtKw68ZcWBt6w48JYVB96y4sBbVhx4y4oDb1lx4C0rU/48vOVtzpw53HrrrcyZM4c5c+YQERQKBc6cOcMPP/zQ7PL+iANvVbvhhhu4/fbbWbt2LUuXLqWjo4O5c+cya1bxgqFQKPDhhx+ybds29u7dS6FQaHLFJSr5mG6zBs3/mKlHyZg1a1bcc8890d3dHV9//XUUCoUoFAoxkUKhEIODg/HQQw81o9ZJPx7c9FA78NfGmD9/frz22msxNDQUo6OjVw17aegHBgbi/vvvH/tvoA68A39tjPb29vjmm2/Khnyi0J8+fTruvvvuGRF4r9JYWfPnz+fpp5+mtbUVSVX9WUksWbKEu+66a5qqq44Db2XdeeedPPDAA1WHfUxEcOHChTpXVRsH3spat24dP/3pT2v+8xHBd999V8eKaufAW1m1vrKPOXfuHN9++22dqpkaB97KGhoaGltEqFpEcPr0aT755JM6V1UbB97KOnHiBGfPnq0p9IVCgV27dnH58uVpqKwGzV569LLkzB+S4rHHHovh4eGqliULhUL09fVFW1tbo2uedFnSHy2wsiKCXbt2ceONN7J9+/aKrukjgsuXL9PT08OpU6caUGWFmv0q7lf4a2csWrQo9u3bV/Gre3d3d8ybN68ZtfofnmzqBgcH6enp4cqVK1c9LiIYHh7mjTfe4NKlSw2qrjIOvFVl9+7dHDt2rPS38ITee+89Pv744wZWVhkH3qpy8uRJHn30Ufr7+yc95sKFC7z55puMjIw0sLLK1NrUbLOkAUkH0+go2fdCamrWJ2lNyfzaNNcvqav+P4o1yokTJ9izZw+jo6MT7t+/fz+9vb0NrqpCFbxxnKip2Wbg2QmOXQ58DswD2ih+bfbsNE4AS4G56ZjlftN67Y4FCxbE9u3bY2Rk5MelykKhEJcuXYotW7Y0+uPA40fty5IR8ZGkvyx3XLIe6ImIS8ApSf3AqrSvPyJOAkjqSccerfBxbYY5f/48zz33HN9//z3r1q3jtttuY2RkhC1btrB169aa/2V2uk1lHX6TpMeBA8A/RLE732Jgb8kxpc3Lxjc1+8VED+oeT9eO8+fP8/zzz9Pd3c2qVau4ePEiu3fv5uLFi80ubVK1Bv4V4LcUf338FniRYoePKYuIHcAOcEOEa8Ho6ChHjx7l6NFr45d1TYGPiKGxbUmvAr9Ld6/WvKyipmZm06mmZcmxDn7JI8DYCs4eYIOkeZLagGXAp8B+YJmkNklzgQ3pWLOGKvsKL2kX8EvgFklfAf8E/FLSHRQvab4Efg0QEUckvUXxzegV4JmIGE2Ps4li577ZQHdEHKn7T2NWhpua2fXITc3MwIG3zDjwlhUH3rLiwFtWHHjLigNvWXHgLSsOvGXFgbesOPCWFQfesuLAW1YceMuKA29ZceAtKw68ZcWBt6w48JYVB96y4sBbVhx4y4oDb1lx4C0rDrxlxYG3rDjwlhUH3rJSSVOzJZI+lHRU0hFJf5fmWyX1SjqeblvSvCS9lJqXfSFpRcljdabjj0vqnL4fy2wSFTQWWwSsSNsLgGMUm5dtA7rSfBewNW13AO8AAlYD+9J8K3Ay3bak7RY3NfOYhlF7J+6IGIyI/07b54HfU+zbtB7YmQ7bCTycttcDr6cO5HuBm1IDhTVAb0QMp35QvcDacuc3q6eqWt6kbn53AvuAhRExmHadARam7cX8cQOzxVeZH38ONzWzaVPxm1ZJfwr8O/D3EfG/pfuieP0R9SgoInZExMrJvtDebCoqCrykP6EY9jcj4j/S9NBYr6d0ezbNT9bY7GoNz8waopJVGgH/Avw+Iv65ZNceoDNtdwK7S+YfT6s1q4Fz6dLnXaBdUkta0WlPc2aNU8Eqzb0UL1e+AA6m0QHcDHwAHAfeB1rT8QJepthq/hCwsuSxngT603jCrec9pmlMukrjpmZ2PXJTMzNw4C0zDrxlxYG3rDjwlhUH3rLiwFtWHHjLigNvWXHgLSsOvGXFgbesOPCWFQfesuLAW1YceMuKA29ZceAtKw68ZcWBt6w48JYVB96y4sBbVhx4y4oDb1lx4C0rDrxlZSo9njZLGpB0MI2Okj/zQurx1CdpTcn82jTXL6lren4ks6uYQo+nzcCzExy/HPgcmAe0UfwW4dlpnACWAnPTMcv97cEe0zAm/fbgsi1v0ne7D6bt85LGejxNZj3QExGXgFOS+oFVaV9/RJwEkNSTjj1argazeqnqGn5cjyeATak1ZfdY20rq0ONJ0gFJB6qpzawSU+nx9Arwc+AOir8BXqxHQe7xZNOpoi5+E/V4ioihkv2vAr9Ld6/Wy8k9nqypau7xNNbQLHkEOJy29wAbJM2T1AYsAz4F9gPLJLVJmgtsSMeaNUwlr/D3AH8LHJJ0MM39I/ArSXdQfFf8JfBrgIg4Iuktim9GrwDPRMQogKRNFBuZzQa6I+JIHX8Ws7Lc48muR+7xZAYOvGXGgbesOPCWFQfesuLAW1YceMuKA29ZceAtKw68ZcWBt6w48JYVB96y4sBbVhx4y4oDb1lx4C0rDrxlxYG3rDjwlhUH3rLiwFtWHHjLigNvWXHgLSsOvGXFgbesOPCWFQfeslJRQ4Qm+h7oa3YR49wCfNPsIsaZaTU1u56/mGzHTA9830xrfSPpgGu6uplWTylf0lhWHHjLykwP/I5mFzAB11TeTKvnRzO65Y1Zvc30V3izunLgLSszNvCS1krqk9QvqauB5/1S0iFJByUdSHOtknolHU+3LWlekl5KNX4haUWdauiWdFbS4ZK5qmuQ1JmOPy6pcxpq2ixpIP1dHZTUUbLvhVRTn6Q1JfNNeV5/FBEzblDs43oCWArMBT4Hljfo3F8Ct4yb2wZ0pe0uYGva7gDeAQSsBvbVqYb7gBXA4VprAFqBk+m2JW231LmmzcCzExy7PD1n84C29FzObubzOjZm6iv8KqA/Ik5GxGWgB1jfxHrWAzvT9k7g4ZL516NoL3DTuA7lNYmIj4DhKdawBuiNiOGI+BboBdbWuabJrAd6IuJSRJwC+ik+p01/Xmdq4BcDfyi5/1Waa4QA3pP0maSNaW5hRAym7TPAwrTdyDqrraFRtW1Kl1LdY5dZM6CmSc3UwDfTvRGxAngQeEbSfaU7o/g7u6lruTOhhuQV4OfAHcAg8GJzyylvpgZ+AFhScv9naW7aRcRAuj0LvE3x1/DQ2KVKuj3bhDqrrWHaa4uIoYgYjYgC8CrFv6um1lTOTA38fmCZpDZJc4ENwJ7pPqmkn0haMLYNtAOH07nHVjk6gd1pew/weFopWQ2cK7nsqLdqa3gXaJfUki412tNc3Yx7v/IIxb+rsZo2SJonqQ1YBnxKk57X/6eR75CrXBXoAI5RfFf/mwadcynFlYPPgSNj5wVuBj4AjgPvA61pXsDLqcZDwMo61bGL4iXCCMXr3KdqqQF4kuIbxn7giWmo6V/TOb+gGNxFJcf/JtXUBzzYzOe1dPijBZaVmXpJYzYtHHjLigNvWXHgLSsOvGXFgbesOPCWlf8DAyHaqEtDqRUAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"ChyVx_LCwEBg"},"source":["dfSCM2 = pd.DataFrame(columns=['image', 'mask', 'label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"5DEVOhArNWlY"},"source":["#@title check_path\n","\n","def check_path(path):\n","  path = path.replace('\\\\', '/')\n","  try:\n","    # mask_ = imread(path,0)    \n","    im1 = Image.open(path)\n","    #rgb_im = mask_.convert('RGB')\n","    im1.save(path.replace('.png', '.jpg'))\n","\n","    return path.replace('.png', '.jpg')\n","  except :\n","  \n","    try:\n","      # mask_ = imread(path,0) \n","      path = path.replace('MASK', 'Mask')\n","      path = path.replace('.png', '.jpg')   \n","      im1 = Image.open(path)\n","\n","      return path\n","    except :\n","      try:\n","        # mask_ = imread(path,0) \n","        path = path.replace('Mask', 'MASK')\n","        path = path.replace('.png', '.jpg')   \n","        im1 = Image.open(path)\n","\n","        return path\n","      except :\n","        try:\n","          path = path.replace('.png', '.jpg')\n","          return path\n","        except:\n","          pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQsRNXnLnRR5"},"source":["# for i in range(0, len(data)): \n","i =0 \n","for i in range(2538, len(data)):\n","    path = root + data[\"fullPath\"][i]#2499 2\n","    path = path.replace('\\\\', '/')\n","    path = path.replace('.png', '.jpg')\n","    img = cv2.imread(path)\n","    Main_mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.uint8)\n","    masks = []\n","    if data['Tumour_Contour'][i] != '-':\n","      masks.append(check_path(root + data['Tumour_Contour'][i]))\n","    if data['Tumour_Contour2'][i] != '-':\n","      masks.append(check_path(root + data['Tumour_Contour2'][i]))\n","    if pd.isnull(data[\"Tumour_Contour3\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour3'][i]))\n","    if pd.isnull(data[\"Tumour_Contour4\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour4'][i]))\n","    if pd.isnull(data[\"Tumour_Contour5\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour5'][i]))\n","    if pd.isnull(data[\"Tumour_Contour6\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour6'][i]))\n","    for mask in masks:\n","      ini_img = img.copy()\n","      mask_ = imread(mask)\n","      mask_ = cv2.resize(mask_, (ini_img.shape[0], ini_img.shape[1]))\n","      mask_[np.where(mask_ !=0)] = 255\n","      ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","      contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE) \n","      for cnt in contours:      \n","        (x,y,w,h) = cv2.boundingRect(cnt)\n","        cv2.drawContours(mask_, [cnt], 0,(255, 0, 0), 2)\n","      mask_ = np.expand_dims(resize(mask_, (img.shape[0], img.shape[1]), mode='constant',  \n","                                  preserve_range=True), axis=-1)\n","      Main_mask = np.maximum(Main_mask, mask_)\n","      Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 255\n","    if data['Status'][i] == 'Cancer':\n","      Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 2\n","    if data['Status'][i] == 'Benign':\n","      Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 1\n","\n","    # plt.imshow(np.squeeze(Main_mask))\n","    cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i) + \".jpg\", np.squeeze(Main_mask))\n","    dfSCM2.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i) + \".jpg\", data['Status'][i]]\n","    # i = i + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZisEQhPyDJNd"},"source":["dfSCM2 = pd.read_csv('/content/drive/MyDrive/Thesis/pos/SCM2data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYdEH080HFEp"},"source":["dfSCML3 = pd.DataFrame(columns=['image', 'mask', 'label'])\n","dfSCML2 = pd.DataFrame(columns=['image', 'mask', 'label'])\n","dfSML2 = pd.DataFrame(columns=['image', 'mask', 'label'])\n","dfCA = pd.DataFrame(columns=['image'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"U6CNbMBE0Vr8","executionInfo":{"status":"ok","timestamp":1628518897152,"user_tz":-60,"elapsed":322,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"ab61b426-55e7-4da9-dcff-85f709b5158b"},"source":["dfPS['mask'][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Thesis/pos/yS/0.jpg'"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"sZQGznZuI2tP"},"source":["from tensorflow.keras.utils import to_categorical\n","for i in range(0, len(dfSCM2)):\n","\n","    # mask = cv2.imread(dfSCM2['mask'][i])\n","    mask_ =cv2.imread('/content/drive/MyDrive/Thesis/pos/yS/'+str(i)+'.jpg')\n","\n","    # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n","    mask_ = cv2.cvtColor(mask_, cv2.COLOR_BGR2GRAY)\n","\n","    img = cv2.imread(dfSCM2['image'][i])\n","    ini_img = img.copy()\n","\n","    # imgc3 = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","    # imgc3[:,:,0][np.where(mask==0)] = 1\n","    # imgc3[:,:,1][np.where(mask==1)] = 1 \n","    # imgc3[:,:,2][np.where(mask==2)] = 1  \n","\n","    # imgc2 = np.zeros((mask.shape[0], mask.shape[1], 2), dtype=np.uint8)\n","    # imgc2[:,:,0][np.where(mask==0)] = 1\n","    # imgc2[:,:,1][np.where(mask!=0)] = 1 \n","    # # img3[:,:,2][np.where(mask==2)] = 1 \n","\n","    # img2 = np.zeros((mask_.shape[0], mask_.shape[1], 2), dtype=np.uint8)\n","    # img2[:,:,0][np.where(mask_==0)] = 1\n","    # img2[:,:,1][np.where(mask_!=0)] = 1 \n","    # # img2[:,:,2][np.where(mask==2)] = 1  \n","\n","    # mask_ = cv2.imread(dfS['mask'][i], 0)\n","    mask_ = cv2.resize(mask_, (ini_img.shape[0], ini_img.shape[1]))\n","    ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","    contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n","\n","    if dfSCM2[\"label\"][i] != 'Normal':\n","      for cnt in contours:      \n","        (x,y,w,h) = cv2.boundingRect(cnt)\n","        # cv2.drawContours(mask_, [cnt], 0,(255, 0, 0),-1)\n","      # ini_img[np.where(mask_==0)] = 0\n","      cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n","      try:\n","        cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xCA/\" + str(len(dfCA)) + \".jpg\", ini_img[y:y+h,x:x+w])\n","        dfCA.loc[len(dfCA)] = [\"/content/drive/MyDrive/Thesis/pos/xCA/\" + str(len(dfCA)) + \".jpg\", dfSCM2['label'][i]]\n","      except:\n","        pass\n","        # gray = np.zeros((mask_.shape[0], mask_.shape[1], 1), dtype=np.uint8)\n","        # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xCA/\" + str(len(dfCA)) + \".jpg\", gray)\n","     \n","\n","    # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/cm3/\" + str(i) + \".jpg\", imgc3)\n","    # dfSCML3.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/cm3/\" + str(i) + \".jpg\", dfSCM2['label'][i]]\n","\n","    # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/cm2/\" + str(i) + \".jpg\", imgc2)\n","    # dfSCML2.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/cm2/\" + str(i) + \".jpg\", dfSCM2['label'][i]]\n","\n","    # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/m2/\" + str(i) + \".jpg\", img2)\n","    # dfSML2.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/m2/\" + str(i) + \".jpg\", dfSCM2['label'][i]]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kAuxt6-Fsrw"},"source":["dfc = pd.read_csv('/content/drive/MyDrive/Thesis/pos/Cdata.csv')\n","dfc_ = pd.read_csv('/content/drive/MyDrive/Thesis/pos/dataC.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nY3RdD-j1TIq"},"source":["# dfSCML3.to_csv('/content/drive/MyDrive/Thesis/pos/SCML3data.csv', index = False)\n","# dfSCML2.to_csv('/content/drive/MyDrive/Thesis/pos/SCML2data.csv', index = False)\n","# dfSML2.to_csv('/content/drive/MyDrive/Thesis/pos/SML2data.csv', index = False)\n","dfCA = pd.read_csv('/content/drive/MyDrive/Thesis/pos/CAdata.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8W0VYpt5fHhG"},"source":["dfUno = pd.DataFrame(columns=['image'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVDQRt-b_WQE"},"source":["from os import listdir\n","from PIL import Image as PImage\n","\n","imagesList = listdir(\"/content/drive/MyDrive/Thesis/tumor/\")\n","path = \"/content/drive/MyDrive/Thesis/tumor/\"\n","i=0\n","for image in imagesList:\n","    cp = path + image\n","    dfUno.loc[i] = [cp]\n","    i=i+1\n","    print(cp)\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7vKhsRxgC7S"},"source":["dfUno.to_csv('/content/drive/MyDrive/Thesis/pos/UData.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZ0gNq_M_rQx","executionInfo":{"status":"ok","timestamp":1628589181446,"user_tz":-60,"elapsed":116689,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"4a231206-0277-42ec-a59b-de11b5afcec8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GEOpEwRZhD8l"},"source":["dfCA.to_csv('/content/drive/MyDrive/Thesis/pos/CAdata.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"JOt5pCcpPyJE","executionInfo":{"status":"ok","timestamp":1628589908911,"user_tz":-60,"elapsed":13225,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"6bff5e37-7f10-44d8-8b4e-e0d5d06d48ae"},"source":["im = cv2.imread(dfCA['image'][6])\n","plt.imshow(im)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fb83164f110>"]},"metadata":{"tags":[]},"execution_count":17},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"m2nMbS40gHlM"},"source":["# EfficientUnet"]},{"cell_type":"markdown","metadata":{"id":"nIJUmq6zgXPz"},"source":["## Efficientnet"]},{"cell_type":"code","metadata":{"cellView":"form","id":"q8dhNhDdgW58"},"source":["#@title Efficientnet\n","from keras import models, layers\n","# import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from tensorflow.keras.utils import get_file\n","# from utils import *\n","\n","__all__ = ['get_model_by_name', 'get_efficientnet_b0_encoder', 'get_efficientnet_b1_encoder',\n","           'get_efficientnet_b2_encoder', 'get_efficientnet_b3_encoder', 'get_efficientnet_b4_encoder',\n","           'get_efficientnet_b5_encoder', 'get_efficientnet_b6_encoder', 'get_efficientnet_b7_encoder']\n","\n","\n","def _efficientnet(input_shape, blocks_args_list, global_params):\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","\n","    # Stem part\n","    model_input = layers.Input(shape=input_shape)\n","    x = layers.Conv2D(\n","        filters=round_filters(32, global_params),\n","        kernel_size=[3, 3],\n","        strides=[2, 2],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='stem_conv2d'\n","    )(model_input)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='stem_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='stem_swish')(x)\n","\n","    # Blocks part\n","    idx = 0\n","    drop_rate = global_params.drop_connect_rate\n","    n_blocks = sum([blocks_args.num_repeat for blocks_args in blocks_args_list])\n","    drop_rate_dx = drop_rate / n_blocks\n","\n","    for blocks_args in blocks_args_list:\n","        assert blocks_args.num_repeat > 0\n","        # Update block input and output filters based on depth multiplier.\n","        blocks_args = blocks_args._replace(\n","            input_filters=round_filters(blocks_args.input_filters, global_params),\n","            output_filters=round_filters(blocks_args.output_filters, global_params),\n","            num_repeat=round_repeats(blocks_args.num_repeat, global_params)\n","        )\n","\n","        # The first block needs to take care of stride and filter size increase.\n","        x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","        idx += 1\n","\n","        if blocks_args.num_repeat > 1:\n","            blocks_args = blocks_args._replace(input_filters=blocks_args.output_filters, strides=[1, 1])\n","\n","        for _ in range(blocks_args.num_repeat - 1):\n","            x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","            idx += 1\n","\n","    # Head part\n","    x = layers.Conv2D(\n","        filters=round_filters(1280, global_params),\n","        kernel_size=[1, 1],\n","        strides=[1, 1],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='head_conv2d'\n","    )(x)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='head_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='head_swish')(x)\n","\n","    x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n","\n","    if global_params.dropout_rate > 0:\n","        x = layers.Dropout(global_params.dropout_rate)(x)\n","\n","    x = layers.Dense(\n","        global_params.num_classes,\n","        kernel_initializer=dense_kernel_initializer,\n","        activation='softmax',\n","        name='head_dense'\n","    )(x)\n","\n","    model = models.Model(model_input, x)\n","\n","    return model\n","\n","\n","def get_model_by_name(model_name, input_shape, classes=3, pretrained=False):\n","    \"\"\"Get an EfficientNet model by its name.\n","    \"\"\"\n","    blocks_args, global_params = get_efficientnet_params(model_name, override_params={'num_classes': classes})\n","    model = _efficientnet(input_shape, blocks_args, global_params)\n","\n","    try:\n","        if pretrained:\n","            weights = IMAGENET_WEIGHTS[model_name]\n","            weights_path = get_file(\n","                weights['name'],\n","                weights['url'],\n","                cache_subdir='models',\n","                md5_hash=weights['md5'],\n","            )\n","            model.load_weights(weights_path)\n","    except KeyError as e:\n","        print(\"NOTE: Currently model {} doesn't have pretrained weights, therefore a model with randomly initialized\"\n","              \" weights is returned.\".format(e))\n","\n","    return model\n","\n","\n","def _get_efficientnet_encoder(model_name, input_shape, pretrained=False):\n","    model = get_model_by_name(model_name, input_shape, pretrained=pretrained)\n","    encoder = models.Model(model.input, model.get_layer('global_average_pooling2d').output)\n","    encoder.layers.pop()  # remove GAP layer\n","    return encoder\n","\n","\n","def get_efficientnet_b0_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b0', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b1_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b1', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b2_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b2', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b3_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b3', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b4_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b4', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b5_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b5', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b6_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b6', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b7_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b7', input_shape, pretrained=pretrained)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iz1mgr0_goYL"},"source":["## Utils"]},{"cell_type":"code","metadata":{"cellView":"form","id":"qje92-TUkfjU"},"source":["#@title number of classes\n","\n","n_classes=2 #@param {type:\"integer\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"PR4BA07zgNwD"},"source":["#@title Utils\n","import re\n","from collections import namedtuple\n","from keras import layers\n","import keras.backend as K\n","import tensorflow as tf\n","import math\n","import numpy as np\n","\n","GlobalParams = namedtuple('GlobalParams', ['batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'num_classes',\n","                                           'width_coefficient', 'depth_coefficient', 'depth_divisor', 'min_depth',\n","                                           'drop_connect_rate'])\n","global_params = None\n","GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n","\n","BlockArgs = namedtuple('BlockArgs', ['kernel_size', 'num_repeat', 'input_filters', 'output_filters', 'expand_ratio',\n","                                     'id_skip', 'strides', 'se_ratio'])\n","BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n","\n","IMAGENET_WEIGHTS = {\n","\n","    'efficientnet-b0': {\n","        'name': 'efficientnet-b0_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000.h5',\n","        'md5': 'bca04d16b1b8a7c607b1152fe9261af7',\n","    },\n","\n","    'efficientnet-b1': {\n","        'name': 'efficientnet-b1_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000.h5',\n","        'md5': 'bd4a2b82f6f6bada74fc754553c464fc',\n","    },\n","\n","    'efficientnet-b2': {\n","        'name': 'efficientnet-b2_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000.h5',\n","        'md5': '45b28b26f15958bac270ab527a376999',\n","    },\n","\n","    'efficientnet-b3': {\n","        'name': 'efficientnet-b3_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000.h5',\n","        'md5': 'decd2c8a23971734f9d3f6b4053bf424',\n","    },\n","\n","    'efficientnet-b4': {\n","        'name': 'efficientnet-b4_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000.h5',\n","        'md5': '01df77157a86609530aeb4f1f9527949',\n","    },\n","\n","    'efficientnet-b5': {\n","        'name': 'efficientnet-b5_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000.h5',\n","        'md5': 'c31311a1a38b5111e14457145fccdf32',\n","    }\n","\n","}\n","\n","\n","def round_filters(filters, global_params):\n","    \"\"\"Round number of filters.\"\"\"\n","    multiplier = global_params.width_coefficient\n","    divisor = global_params.depth_divisor\n","    min_depth = global_params.min_depth\n","    if not multiplier:\n","        return filters\n","\n","    filters *= multiplier\n","    min_depth = min_depth or divisor\n","    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_filters < 0.9 * filters:\n","        new_filters += divisor\n","    return int(new_filters)\n","\n","\n","def round_repeats(repeats, global_params):\n","    \"\"\"Round number of repeats.\"\"\"\n","    multiplier = global_params.depth_coefficient\n","    if not multiplier:\n","        return repeats\n","    return int(math.ceil(multiplier * repeats))\n","\n","\n","def get_efficientnet_params(model_name, override_params=None):\n","    \"\"\"Get efficientnet params based on model name.\"\"\"\n","    params_dict = {\n","        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n","        # Note: the resolution here is just for reference, its values won't be used.\n","        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n","        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n","        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n","        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n","        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n","        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n","        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n","        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n","    }\n","    if model_name not in params_dict.keys():\n","        raise KeyError('There is no model named {}.'.format(model_name))\n","\n","    width_coefficient, depth_coefficient, _, dropout_rate = params_dict[model_name]\n","\n","    blocks_args = [\n","        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n","        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n","        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n","        'r1_k3_s11_e6_i192_o320_se0.25',\n","    ]\n","    global_params = GlobalParams(\n","        batch_norm_momentum=0.99,\n","        batch_norm_epsilon=1e-3,\n","        dropout_rate=dropout_rate,\n","        drop_connect_rate=0.2,\n","        num_classes=n_classes,\n","        width_coefficient=width_coefficient,\n","        depth_coefficient=depth_coefficient,\n","        depth_divisor=8,\n","        min_depth=None)\n","\n","    if override_params:\n","        global_params = global_params._replace(**override_params)\n","\n","    decoder = BlockDecoder()\n","    return decoder.decode(blocks_args), global_params\n","\n","\n","class BlockDecoder(object):\n","    \"\"\"Block Decoder for readability.\"\"\"\n","\n","    @staticmethod\n","    def _decode_block_string(block_string):\n","        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n","        assert isinstance(block_string, str)\n","        ops = block_string.split('_')\n","        options = {}\n","        for op in ops:\n","            splits = re.split(r'(\\d.*)', op)\n","            if len(splits) >= 2:\n","                key, value = splits[:2]\n","                options[key] = value\n","\n","        if 's' not in options or len(options['s']) != 2:\n","            raise ValueError('Strides options should be a pair of integers.')\n","\n","        return BlockArgs(\n","            kernel_size=int(options['k']),\n","            num_repeat=int(options['r']),\n","            input_filters=int(options['i']),\n","            output_filters=int(options['o']),\n","            expand_ratio=int(options['e']),\n","            id_skip=('noskip' not in block_string),\n","            se_ratio=float(options['se']) if 'se' in options else None,\n","            strides=[int(options['s'][0]), int(options['s'][1])]\n","        )\n","\n","    @staticmethod\n","    def _encode_block_string(block):\n","        \"\"\"Encodes a block to a string.\"\"\"\n","        args = [\n","            'r%d' % block.num_repeat,\n","            'k%d' % block.kernel_size,\n","            's%d%d' % (block.strides[0], block.strides[1]),\n","            'e%s' % block.expand_ratio,\n","            'i%d' % block.input_filters,\n","            'o%d' % block.output_filters\n","        ]\n","        if 0 < block.se_ratio <= 1:\n","            args.append('se%s' % block.se_ratio)\n","        if block.id_skip is False:\n","            args.append('noskip')\n","        return '_'.join(args)\n","\n","    def decode(self, string_list):\n","        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n","        Args:\n","          string_list: a list of strings, each string is a notation of block.\n","        Returns:\n","          A list of namedtuples to represent blocks arguments.\n","        \"\"\"\n","        assert isinstance(string_list, list)\n","        blocks_args = []\n","        for block_string in string_list:\n","            blocks_args.append(self._decode_block_string(block_string))\n","        return blocks_args\n","\n","    def encode(self, blocks_args):\n","        \"\"\"Encodes a list of Blocks to a list of strings.\n","        Args:\n","          blocks_args: A list of namedtuples to represent blocks arguments.\n","        Returns:\n","          a list of strings, each string is a notation of block.\n","        \"\"\"\n","        block_strings = []\n","        for block in blocks_args:\n","            block_strings.append(self._encode_block_string(block))\n","        return block_strings\n","\n","\n","class Swish(layers.Layer):\n","    def __init__(self, name=None, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","    def call(self, inputs, **kwargs):\n","        return tf.nn.silu(inputs)#tf.nn.swish I have changed this why I don't know yet\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['name'] = self.name\n","        return config\n","\n","\n","def SEBlock(block_args, **kwargs):\n","    num_reduced_filters = max(\n","        1, int(block_args.input_filters * block_args.se_ratio))\n","    filters = block_args.input_filters * block_args.expand_ratio\n","\n","    spatial_dims = [1, 2]\n","\n","    try:\n","        block_name = kwargs['block_name']\n","    except KeyError:\n","        block_name = ''\n","\n","    def block(inputs):\n","        x = inputs\n","        x = layers.Lambda(lambda a: K.mean(a, axis=spatial_dims, keepdims=True))(x)\n","        x = layers.Conv2D(\n","            num_reduced_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_reduce_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = Swish(name=block_name + 'se_swish')(x)\n","\n","        x = layers.Conv2D(\n","            filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_expand_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = layers.Activation('sigmoid')(x)\n","        out = layers.Multiply()([x, inputs])\n","        return out\n","\n","    return block\n","\n","\n","class DropConnect(layers.Layer):\n","\n","    def __init__(self, drop_connect_rate, **kwargs):\n","        super().__init__(**kwargs)\n","        self.drop_connect_rate = drop_connect_rate\n","\n","    def call(self, inputs, **kwargs):\n","        def drop_connect():\n","            keep_prob = 1.0 - self.drop_connect_rate\n","\n","            # Compute drop_connect tensor\n","            batch_size = tf.shape(inputs)[0]\n","            random_tensor = keep_prob\n","            random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n","            binary_tensor = tf.floor(random_tensor)\n","            output = tf.math.divide(inputs, keep_prob) * binary_tensor\n","            return output\n","\n","        return K.in_train_phase(drop_connect(), inputs, training=None)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['drop_connect_rate'] = self.drop_connect_rate\n","        return config\n","\n","\n","def conv_kernel_initializer(shape, dtype=K.floatx()):\n","    \"\"\"Initialization for convolutional kernels.\n","    The main difference with tf.variance_scaling_initializer is that\n","    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n","    standard deviation, whereas here we use a normal distribution. Similarly,\n","    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n","    a corrected standard deviation.\n","    Args:\n","        shape: shape of variable\n","        dtype: dtype of variable\n","    Returns:\n","        an initialization for the variable\n","    \"\"\"\n","    kernel_height, kernel_width, _, out_filters = shape\n","    fan_out = int(kernel_height * kernel_width * out_filters)\n","    return tf.random.normal(\n","        shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n","\n","\n","def dense_kernel_initializer(shape, dtype=K.floatx()):\n","    init_range = 1.0 / np.sqrt(shape[1])\n","    return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)\n","\n","\n","def MBConvBlock(block_args, global_params, idx, drop_connect_rate=None):\n","    filters = block_args.input_filters * block_args.expand_ratio\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","    has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n","\n","    block_name = 'blocks_' + str(idx) + '_'\n","\n","    def block(inputs):\n","        x = inputs\n","\n","        # Expansion phase\n","        if block_args.expand_ratio != 1:\n","            expand_conv = layers.Conv2D(filters,\n","                                        kernel_size=[1, 1],\n","                                        strides=[1, 1],\n","                                        kernel_initializer=conv_kernel_initializer,\n","                                        padding='same',\n","                                        use_bias=False,\n","                                        name=block_name + 'expansion_conv2d'\n","                                        )(x)\n","            bn0 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                            epsilon=batch_norm_epsilon,\n","                                            name=block_name + 'expansion_batch_norm')(expand_conv)\n","\n","            x = Swish(name=block_name + 'expansion_swish')(bn0)\n","\n","        # Depth-wise convolution phase\n","        kernel_size = block_args.kernel_size\n","        depthwise_conv = layers.DepthwiseConv2D(\n","            [kernel_size, kernel_size],\n","            strides=block_args.strides,\n","            depthwise_initializer=conv_kernel_initializer,\n","            padding='same',\n","            use_bias=False,\n","            name=block_name + 'depthwise_conv2d'\n","        )(x)\n","        bn1 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                        epsilon=batch_norm_epsilon,\n","                                        name=block_name + 'depthwise_batch_norm'\n","                                        )(depthwise_conv)\n","        x = Swish(name=block_name + 'depthwise_swish')(bn1)\n","\n","        if has_se:\n","            x = SEBlock(block_args, block_name=block_name)(x)\n","\n","        # Output phase\n","        project_conv = layers.Conv2D(\n","            block_args.output_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'output_conv2d',\n","            use_bias=False)(x)\n","        x = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                      epsilon=batch_norm_epsilon,\n","                                      name=block_name + 'output_batch_norm'\n","                                      )(project_conv)\n","        if block_args.id_skip:\n","            if all(\n","                    s == 1 for s in block_args.strides\n","            ) and block_args.input_filters == block_args.output_filters:\n","                # only apply drop_connect if skip presents.\n","                if drop_connect_rate:\n","                    x = DropConnect(drop_connect_rate)(x)\n","                x = layers.add([x, inputs])\n","\n","        return x\n","\n","    return block\n","\n","\n","def freeze_efficientunet_first_n_blocks(model, n):\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            model.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","\n","    all_block_names = ['blocks_{}_output_batch_norm'.format(i) for i in range(mbblock_nr)]\n","    all_block_index = []\n","    for idx, layer in enumerate(model.layers):\n","        if layer.name == all_block_names[0]:\n","            all_block_index.append(idx)\n","            all_block_names.pop(0)\n","            if len(all_block_names) == 0:\n","                break\n","    n_blocks = len(all_block_index)\n","\n","    if n <= 0:\n","        print('n is less than or equal to 0, therefore no layer will be frozen.')\n","        return\n","    if n > n_blocks:\n","        raise ValueError(\"There are {} blocks in total, n cannot be greater than {}.\".format(n_blocks, n_blocks))\n","\n","    idx_of_last_block_to_be_frozen = all_block_index[n - 1]\n","    for layer in model.layers[:idx_of_last_block_to_be_frozen + 1]:\n","        layer.trainable = False\n","\n","\n","def unfreeze_efficientunet(model):\n","    for layer in model.layers:\n","        layer.trainable = True\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZ29zXFqh5sm"},"source":["## *Efficientunet*"]},{"cell_type":"code","metadata":{"id":"TGg2DkR-g-2L","cellView":"form"},"source":["#@markdown Efficientnet-unet\n","import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from keras.layers import *\n","from keras import models\n","# from efficientnet import *\n","# from utils import conv_kernel_initializer\n","\n","\n","__all__ = ['get_efficient_unet_b0', 'get_efficient_unet_b1', 'get_efficient_unet_b2', 'get_efficient_unet_b3',\n","           'get_efficient_unet_b4', 'get_efficient_unet_b5', 'get_efficient_unet_b6', 'get_efficient_unet_b7',\n","           'get_blocknr_of_skip_candidates']\n","\n","\n","def get_blocknr_of_skip_candidates(encoder, verbose=False):\n","    \"\"\"\n","    Get block numbers of the blocks which will be used for concatenation in the Unet.\n","    :param encoder: the encoder\n","    :param verbose: if set to True, the shape information of all blocks will be printed in the console\n","    :return: a list of block numbers\n","    \"\"\"\n","    shapes = []\n","    candidates = []\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","            shape = int(mbblock.shape[1]), int(mbblock.shape[2])\n","            if shape not in shapes:\n","                shapes.append(shape)\n","                candidates.append(mbblock_nr)\n","            if verbose:\n","                print('blocks_{}_output_shape: {}'.format(mbblock_nr, shape))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","    return candidates\n","\n","\n","def DoubleConv(filters, kernel_size, initializer='glorot_uniform'):\n","\n","    def layer(x):\n","\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","def UpSampling2D_block(filters, kernel_size=(3, 3), upsample_rate=(2, 2), interpolation='bilinear',\n","                       initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = UpSampling2D(size=upsample_rate, interpolation=interpolation)(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","    return layer\n","\n","\n","def Conv2DTranspose_block(filters, kernel_size=(3, 3), transpose_kernel_size=(2, 2), upsample_rate=(2, 2),\n","                          initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate, padding='same')(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","# noinspection PyTypeChecker\n","def _get_efficient_unet(encoder, out_channels=2, block_type='upsampling', concat_input=True):\n","    MBConvBlocks = []\n","\n","    skip_candidates = get_blocknr_of_skip_candidates(encoder)\n","\n","    for mbblock_nr in skip_candidates:\n","        mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","        MBConvBlocks.append(mbblock)\n","\n","    # delete the last block since it won't be used in the process of concatenation\n","    MBConvBlocks.pop()\n","\n","    input_ = encoder.input\n","    head = encoder.get_layer('head_swish').output\n","    blocks = [input_] + MBConvBlocks + [head]\n","\n","    if block_type == 'upsampling':\n","        UpBlock = UpSampling2D_block\n","    else:\n","        UpBlock = Conv2DTranspose_block\n","\n","    o = blocks.pop()\n","    o = UpBlock(512, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(256, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(128, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(64, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    if concat_input:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    else:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=None)(o)\n","    o = Conv2D(out_channels, (1, 1), padding='same', kernel_initializer=conv_kernel_initializer)(o)\n","\n","    model = models.Model(encoder.input, o)\n","\n","    return model\n","\n","\n","def get_efficient_unet_b0(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B0 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B0 model\n","    \"\"\"\n","    encoder = get_efficientnet_b0_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b1(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B1 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B1 model\n","    \"\"\"\n","    encoder = get_efficientnet_b1_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b2(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B2 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B2 model\n","    \"\"\"\n","    encoder = get_efficientnet_b2_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b3(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B3 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B3 model\n","    \"\"\"\n","    encoder = get_efficientnet_b3_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b4(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B4 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B4 model\n","    \"\"\"\n","    encoder = get_efficientnet_b4_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b5(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B5 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B5 model\n","    \"\"\"\n","    encoder = get_efficientnet_b5_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b6(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B6 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B6 model\n","    \"\"\"\n","    encoder = get_efficientnet_b6_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b7(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B7 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B7 model\n","    \"\"\"\n","    encoder = get_efficientnet_b7_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWn7h2XETHDn"},"source":["# C model"]},{"cell_type":"code","metadata":{"id":"iSbvK6VVTlsl"},"source":["Channels =  3#@param {type:\"integer\"}\n","Img_size = 384#@param {type:\"integer\"}\n","input_shape = (Img_size, Img_size, Channels) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fkid-gMTElW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628590476099,"user_tz":-60,"elapsed":2682,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"6f9e9877-68ca-4a74-eabe-9fd50cb9b8f2"},"source":["classifier =  get_efficientnet_b0_encoder(input_shape)\n","CmodelS = models.Sequential()\n","CmodelS.add(classifier)\n","CmodelS.add(layers.Dense(2))\n","CmodelS.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_1 (Functional)         (None, 1280)              4049564   \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 2562      \n","=================================================================\n","Total params: 4,052,126\n","Trainable params: 4,010,110\n","Non-trainable params: 42,016\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEO7EWtzjo_y","executionInfo":{"status":"ok","timestamp":1628590556522,"user_tz":-60,"elapsed":2339,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"c35e1b27-5914-4284-ed44-8ec6d1c589bd"},"source":["classifier =  get_efficientnet_b0_encoder(input_shape)\n","CmodelT = models.Sequential()\n","CmodelT.add(classifier)\n","CmodelT.add(layers.Dense(2))\n","CmodelT.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_3 (Functional)         (None, 1280)              4049564   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 2562      \n","=================================================================\n","Total params: 4,052,126\n","Trainable params: 4,010,110\n","Non-trainable params: 42,016\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nQ9eU0pqja8e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ld0iL6l7sd7"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"pNmm_kkr5RcZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628590020026,"user_tz":-60,"elapsed":263,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"c5f372d0-ca0a-48c0-a329-3599e22ee3b3"},"source":["from tensorflow.python.client import device_lib\n","\n","def get_available_gpus():\n","    local_device_protos = device_lib.list_local_devices()\n","    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n","get_available_gpus()    "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"1iJF97nG5Rcb"},"source":["#%cd /content/drive/MyDrive/Thesis/MPL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3Igabyt7sd9","cellView":"form"},"source":["#@title MPL config\n","import tensorflow as tf\n","\n","\n","\n","\n","# about dataset\n","IMG_SIZE = 384#@param {type:\"integer\"}\n","BATCH_SIZE = 4#@param {type:\"integer\"}\n","# LABEL_FILE_PATH = '/content/cifar/label4000.csv' # google\n","# UNLABEL_FILE_PATH = '/content/cifar/train.csv'\n","\n","_MAX_LEVEL = 10\n","CUTOUT_CONST = 40.\n","TRANSLATE_CONST = 100.\n","REPLACE_COLOR = [128, 128, 128]\n","\n","\n","# LABEL_FILE_PATH = '../input/cifar10/cifar/label4000.csv'  # kaggle\n","# UNLABEL_FILE_PATH = '../input/cifar10/cifar/train.csv'\n","\n","\n","AUGMENT_MAGNITUDE = 8\n","SHUFFLE_SIZE = BATCH_SIZE * 16\n","DATA_LEN = 400  # 数据集的总长度\n","\n","# about model\n","NUM_XLA_SHARDS = -1\n","BATCH_NORM_EPSILON = 1e-3\n","BATCH_NORM_DECAY = 0.999\n","DROPOUT_RATE = 0.\n","DROPOUT = 0.2\n","NUM_CLASSES = 2#@param {type:\"integer\"}\n","NUM_CLASS = 2#@param {type:\"integer\"}\n","\n","# about training\n","LOG_EVERY = 20\n","SAVE_EVERY = 5\n","TEA_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/CT2'\n","STD_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/CS2'\n","\n","MAX_EPOCHS = 1920\n","MAX_STEPS = MAX_EPOCHS * (int(DATA_LEN / BATCH_SIZE)-1)\n","UDA_WEIGHT = 8  # uda的权重\n","UDA_STEPS = 2000\n","TEST_EVERY = 2\n","GRAD_BOUND = 1e9\n","EMA = 0.995\n","\n","\n","# continue train\n","TEA_CONTINUE = True\n","STD_CONTINUE = True\n","TEA_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/CT2'\n","STD_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/CS2'\n","CONTINUE_EPOCH = 885\n","\n","\n","# about testing\n","# TEST_FILE_PATH = '/content/cifar/test.csv'\n","# TEST_FILE_PATH = '../input/cifar10/cifar/test.csv'\n","TEST_MODEL_PATH = '/content/drive/MyDrive/Thesis/weights/PS'\n","\n","# about UdaCrossEntroy\n","UDA_DATA = 1\n","LABEL_SMOOTHING = 0.15\n","UDA_TEMP = 0.7\n","UDA_THRESHOLD = 0.6\n","\n","# about learning rate\n","STUDENT_LR = 0.0005  # student\n","STUDENT_LR_WARMUP_STEPS = 4000\n","STUDENT_LR_WAIT_STEPS = 2000\n","TEACHER_LR = 0.0005  # teacher\n","TEACHER_LR_WARMUP_STEPS = 1000\n","TEACHER_NUM_WAIT_STEPS = 0\n","\n","LR_DECAY_TYPE = 'cosine'  # constant, exponential, cosine\n","NUM_DECAY_STEPS = 300\n","LR_DECAY_RATE = 0.97\n","\n","# about optimizer\n","OPTIM_TYPE = 'sgd'  # sgd, momentum, rmsprop\n","WEIGHT_DECAY = 5e-4\n","\n","\n","# dtype\n","DTYPE = tf.float32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"6XgCSbvH7sd-"},"source":["#@title Self_aug_func\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import math\n","import tensorflow_addons.image as image_ops\n","\n","# import \n","\n","\n","def autocontrast(image):\n","    lo = tf.cast(tf.reduce_min(image, axis=[0, 1]), tf.float32)\n","    hi = tf.cast(tf.reduce_max(image, axis=[0, 1]), tf.float32)\n","    scale = tf.math.divide(255.0, (hi - lo))\n","    offset = tf.math.multiply(-lo, scale)\n","    image = tf.math.add(\n","        tf.math.multiply(tf.cast(image, tf.float32), scale),\n","        offset\n","    )\n","    image = tf.clip_by_value(image, 0.0, 255.0)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def equalize(image):\n","    # image = tf.cast(image, tf.int32)\n","    # channel = tf.shape(image)[-1]\n","    # for i in range(channel):\n","    #     im = tf.cast(image[:, :, i], tf.int32)\n","    #     histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","    #     nonzero = tf.where(tf.not_equal(histo, 0))\n","    #     nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","    #     step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","    #     print(step)\n","    #     if step == 0:\n","    #         pass\n","    #     else:\n","    #         lut = (tf.cumsum(histo) + (step // 2)) // step\n","    #         lut = tf.concat([[0], lut[:-1]], 0)\n","    #         lut = tf.clip_by_value(lut, 0, 255)\n","    #         # print(lut)\n","    #         image[:, :, i] = tf.gather(lut, image[:, :, i])\n","    #         # image[:, :, i] = im\n","    #     # image[:, :, i] = im\n","\n","    def scale_channel(im, c=0):\n","        \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n","        im = tf.cast(im[:, :, 0], tf.int32)\n","        # Compute the histogram of the image channel.\n","        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","\n","        # For the purposes of computing the step, filter out the nonzeros.\n","        nonzero = tf.where(tf.not_equal(histo, 0))\n","        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","\n","        def build_lut(histo, step):\n","            # Compute the cumulative sum, shifting by step // 2\n","            # and then normalization by step.\n","            lut = (tf.cumsum(histo) + (step // 2)) // step\n","            # Shift lut, prepending with 0.\n","            lut = tf.concat([[0], lut[:-1]], 0)\n","            # Clip the counts to be in range.  This is done\n","            # in the C code for image.point.\n","            return tf.clip_by_value(lut, 0, 255)\n","\n","        # If step is zero, return the original image.  Otherwise, build\n","        # lut from the full histogram and step and then index from it.\n","        result = tf.cond(tf.equal(step, 0),\n","                         lambda: im,\n","                         lambda: tf.gather(build_lut(histo, step), im))\n","        return tf.cast(result, tf.uint8)\n","\n","    s1 = scale_channel(image, 0)\n","    s2 = scale_channel(image, 1)\n","    s3 = scale_channel(image, 2)\n","    image = tf.stack([s1, s2, s3], 2)\n","\n","    return image\n","\n","\n","def invert(image):\n","    image = 255 - image\n","    return image\n","\n","\n","def rotate(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    degree = tf.cond(should_filp, lambda: level, lambda: -level)\n","    degree_to_radians = tf.convert_to_tensor(math.pi / 180., tf.float32)\n","    radians = tf.math.multiply(degree, degree_to_radians)\n","    new_imgsize = tf.cast(tf.math.abs(tf.divide(IMG_SIZE, radians)), tf.int32)\n","    image = tf.image.resize(image, (new_imgsize, new_imgsize))\n","    image = image_ops.rotate(image, radians, fill_mode='constant')\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def posterize(image):\n","    bit = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 4, tf.float32)\n","    shift = tf.cast(8 - bit, image.dtype)\n","    image = tf.bitwise.right_shift(image, shift)\n","    image = tf.bitwise.left_shift(image, shift)\n","    return image\n","\n","\n","def solarize_arg(image):\n","    threahold = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 22, tf.float32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def solarize_add(image, threahold=128):\n","    addition = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 2, tf.int32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.add(tf.cast(image, tf.int32), addition)\n","    image = tf.cast(tf.clip_by_value(image, 0, 255), tf.uint8)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def color(image, degenetate=None):\n","    if degenetate is None:\n","        degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.8 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def contrast(image):\n","    degenerate = tf.image.rgb_to_grayscale(image)\n","    degenerate = tf.cast(degenerate, tf.int32)\n","\n","    hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n","    mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.\n","    degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n","    degenerate = tf.clip_by_value(degenerate, 0., 255.)\n","    degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.6 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def brightness(image):\n","    image = tf.image.adjust_brightness(image, 0.25)\n","    return image\n","\n","\n","def sharpness(image):\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.6 + 0.1, tf.float32)\n","    image = tf.cast(image, tf.float32)\n","    image = image_ops.sharpness(image, factor)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_x(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.2, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_x(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_y(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.1, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_y(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def translate_x(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [-pixels, 0])\n","    return image\n","\n","\n","def translate_y(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [0, -pixels])\n","    return image\n","\n","\n","def cutout(image):\n","    pad_size = tf.cast(\n","        tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * CUTOUT_CONST,\n","        tf.int32\n","    )\n","    image_height = tf.shape(image)[0]\n","    image_width = tf.shape(image)[1]\n","\n","    # Samples the center location in the image where the zero mask is applied.\n","    cutout_center_height = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_height,\n","        dtype=tf.int32)\n","\n","    cutout_center_width = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_width,\n","        dtype=tf.int32)\n","\n","    lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n","    upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n","    left_pad = tf.maximum(0, cutout_center_width - pad_size)\n","    right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n","\n","    cutout_shape = [image_height - (lower_pad + upper_pad),\n","                    image_width - (left_pad + right_pad)]\n","    padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n","    mask = tf.pad(\n","        tf.zeros(cutout_shape, dtype=image.dtype),\n","        padding_dims, constant_values=1)\n","    mask = tf.expand_dims(mask, -1)\n","    mask = tf.tile(mask, [1, 1, 3])\n","    image = tf.where(\n","        tf.equal(mask, 0),\n","        tf.ones_like(image, dtype=image.dtype) * REPLACE_COLOR,\n","        image)\n","    return image\n","\n","\n","def identity(image):\n","    return tf.identity(image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"6qxxtHeZ7seC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628590124985,"user_tz":-60,"elapsed":291,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"220a2da3-649d-4269-bfeb-380193542fbc"},"source":["#@title Self_aug_util\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import tensorflow as tf\n","\n","# from self_aug_func import *\n","\n","_MAX_LEVEL = 10\n","\n","\n","\n","def _enhance_level_to_arg(level):\n","    return (tf.cast((level / _MAX_LEVEL) * 1.8 + 0.1, tf.float32),)\n","\n","\n","def _translate_level_to_arg(level, translate_const):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * float(translate_const), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    final_tensor = tf.cond(should_flip, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _rotate_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _shear_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 0.3, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def level_to_arg(cutout_const, translate_const):\n","    '''\n","    将对image做变化的函数所用到的参数整理成字典形式\n","    :param cutout_const:\n","    :param translate_const:\n","    :return: type:dict\n","    '''\n","    no_arg = lambda level: ()\n","    posterize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 4,\n","        tf.float32\n","    )\n","    solarize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 256,\n","        tf.float32\n","    )\n","    solarize_add_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 110,\n","        tf.float32\n","    )\n","    cutout_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * cutout_const,\n","        tf.float32\n","    )\n","    translate_arg = lambda level: _translate_level_to_arg(level, translate_const)\n","\n","    args = {\n","        'Identity': no_arg,\n","        'AutoContrast': no_arg,\n","        'Equalize': no_arg,\n","        'Invert': no_arg,\n","        'Rotate': _rotate_level_to_arg,\n","        'Posterize': posterize_arg,\n","        'Solarize': solarize_arg,\n","        'SplarizeAdd': solarize_add_arg,\n","        'Color': _enhance_level_to_arg,\n","        'Contrast': _enhance_level_to_arg,\n","        'Brightness': _enhance_level_to_arg,\n","        'Sharpness': _enhance_level_to_arg,\n","        'ShearX': _shear_level_to_arg,\n","        'ShearY': _shear_level_to_arg,\n","        'Cutout': cutout_arg,\n","        'TranslateX': translate_arg,\n","        'TranslateY': translate_arg,\n","    }\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    NAME_TO_FUNC = {\n","        'AutoContrast': autocontrast,\n","        'Equalize': equalize,\n","        'Invert': invert,\n","        'Rotate': rotate,\n","        'Posterize': posterize,\n","        'Solarize': solarize_arg,\n","        'SolarizeAdd': solarize_add,\n","        'Color': color,\n","        'Contrast': contrast,\n","        'Brightness': brightness,\n","        'Sharpness': sharpness,\n","        'ShearX': shear_x,\n","        'ShearY': shear_y,\n","        'TranslateX': translate_x,\n","        'TranslateY': translate_y,\n","        'Cutout': cutout,\n","        'Identity': identity,\n","    }\n","\n","    available_ops = [\n","        'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","        'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","    ]\n","\n","    for (i, op_name) in enumerate(available_ops):\n","        func = NAME_TO_FUNC[op_name]\n","        args = level_to_arg(4, 4)[op_name](16)\n","        print(args)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["()\n","()\n","()\n","tf.Tensor(48.0, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(409.6, shape=(), dtype=float32)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","tf.Tensor(-0.48, shape=(), dtype=float32)\n","tf.Tensor(-0.48, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"form","id":"wKiP8pWx7seE"},"source":["#@title Self_augment\n","'''\n","reference:\n","https://github.com/google-research/google-research/tree/1f1741a985a0f2e6264adae985bde664a7993bd2/flax_models/cifar/datasets\n","'''\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","\n","'''\n","可能augment.py中的内容有问题 涉及文件augment.py的line 53，54\n","引用的库不一样，因为tensorflow.contrib已经停用，\n","使用的第三方：pip install tensorflow-addons\n","'''\n","import os\n","\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","\n","# from self_aug_func import *\n","\n","# 将对图片做augment的函数变成一个字典\n","NAME_TO_FUNC = {\n","    'AutoContrast': autocontrast,\n","    'Equalize': equalize,\n","    'Invert': invert,\n","    'Rotate': rotate,\n","    'Posterize': posterize,\n","    'Solarize': solarize_arg,\n","    'SolarizeAdd': solarize_add,\n","    'Color': color,\n","    'Contrast': contrast,\n","    'Brightness': brightness,\n","    'Sharpness': sharpness,\n","    'ShearX': shear_x,\n","    'ShearY': shear_y,\n","    'TranslateX': translate_x,\n","    'TranslateY': translate_y,\n","    'Cutout': cutout,\n","    'Identity': identity,\n","}\n","# 在某些函数中有一些需要一个替换的值，比如旋转中有一些位置的像素值需要补充\n","REPLACE_FUNCS = frozenset({\n","    'Rotate',\n","    'TranslateX',\n","    'ShearX',\n","    'SHearY',\n","    'TranslateY',\n","    'Cutout',\n","})\n","\n","\n","class RandAugment(object):\n","    def __init__(self, num_layers=2, magnitude=None, cutout_const=40, translate_const=100., available_ops=None):\n","        '''\n","        reference: https://arxiv.org/abs/1909.13719\n","        :param num_layers:\n","        :param magnitude:\n","        :param cutout_const:\n","        :param translate_const:\n","        :param avalilable_ops:\n","        '''\n","        super(RandAugment, self).__init__()\n","        self.num_layers = num_layers\n","        self.cutout_const = float(cutout_const)\n","        self.translate_const = float(translate_const)\n","        if available_ops is None:\n","            available_ops = [\n","                'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","                'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","                'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","            ]\n","        self.available_ops = available_ops\n","        self.magnitude = magnitude\n","\n","    def distort(self, image):\n","        '''\n","\n","        :param image:  shape:[HWC] C=3\n","        :return: 返回一个经过变化后的图片\n","        '''\n","        input_image_type = image.dtype\n","        image = tf.clip_by_value(image, tf.cast(0, input_image_type), tf.cast(255, input_image_type))\n","        image = tf.cast(image, tf.uint8)\n","\n","        prob = tf.random.uniform([], 0.2, 0.8, tf.float32)\n","\n","        for _ in range(self.num_layers):\n","            op_to_select = tf.random.uniform([], minval=0, maxval=len(self.available_ops), dtype=tf.int32)\n","            for (i, op_name) in enumerate(self.available_ops):\n","                func = NAME_TO_FUNC[op_name]  # 得到函数名称\n","                if i == op_to_select:\n","                    flag = tf.random.uniform([], 0., 1., prob.dtype)\n","                    if tf.math.greater_equal(prob, flag):\n","                        image = func(image)\n","\n","        image = tf.cast(image, dtype=input_image_type)\n","        return image\n","\n","\n","def unlabel_image(img_file, label):\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","\n","    aug_image, some_info = aug.distort(img)\n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, tf.float32) / 255.0\n","    ori_image = tf.cast(ori_image, tf.float32) / 255.0\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pesc1AwM7seG","cellView":"form"},"source":["#@title UDa\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","\n","# import config\n","# from Model import Wrn28k\n","\n","\n","def UdaCrossEntroy(all_logits, l_labels, global_step):\n","    batch_size = BATCH_SIZE\n","    uda_data = UDA_DATA\n","    logits = {}\n","    labels = {}\n","    cross_entroy = {}\n","    masks = {}\n","    # 将网络的输出结果区分成 label ori aug 三个部分\n","    logits['l'], logits['ori'], logits['aug'] = tf.split(\n","        all_logits,\n","        [batch_size, batch_size * uda_data, batch_size * uda_data],\n","        axis=0,\n","    )\n","    # 对标签进行处理\n","    labels['l'] = l_labels\n","\n","    # ------------loss的计算---------\n","    # part1：有监督部分\n","    cross_entroy['l'] = tf.losses.CategoricalCrossentropy(\n","        from_logits=True,\n","        label_smoothing=LABEL_SMOOTHING,\n","        reduction=keras.losses.Reduction.NONE,)(labels['l'], logits['l'])\n","    '''\n","    probs = tf.nn.softmax(logits['l'], axis=-1)  # 将每张图片对应10个类别的输出转化为概率的形式\n","    correct_probs = tf.reduce_sum(labels['l'] * probs, axis=-1)  # 根据图片对应的label和概率计算出 预测正确类别的概率\n","    # 计算一个阈值l_threshold\n","    r = tf.cast(global_step, tf.float32) / tf.convert_to_tensor(MAX_STEPS, dtype=tf.float32)\n","    num_classes = tf.convert_to_tensor(NUM_CLASSES, tf.float32)\n","    l_threshold = r * (1. - 1. / num_classes) + 1. / num_classes\n","    masks['l'] = tf.math.less_equal(correct_probs, l_threshold)\n","    masks['l'] = tf.cast(masks['l'], tf.float32)\n","    masks['l'] = tf.stop_gradient(masks['l'])  # 如果对某图片预测的概率小于l_threahold,输出1，否则是0\n","    '''\n","    cross_entroy['l'] = tf.reduce_sum(cross_entroy['l']) / float(batch_size)\n","\n","    # part2: 无监督部分\n","    labels['ori'] = tf.nn.softmax(logits['ori'] / tf.convert_to_tensor(UDA_TEMP), axis=-1)\n","    labels['ori'] = tf.stop_gradient(labels['ori'])\n","    # tf.nn.log_softmax: 设一张图片对应3个类别的输出为o1，o2，o3 ==>\n","    # b = log(sum(exp(o1) + exp(o2) + exp(o3)))  new_o1=o1-b, new_o2=o2-b ... 恒负，大小关系不变\n","    cross_entroy['u'] = (\n","            labels['ori'] * tf.nn.log_softmax(logits['aug'], axis=-1)\n","    )\n","\n","    largest_probs = tf.reduce_max(labels['ori'], axis=-1, keepdims=True)\n","\n","    masks['u'] = tf.math.greater_equal(largest_probs, tf.constant(UDA_THRESHOLD))  # 判断最大概率是否大于阈值\n","    masks['u'] = tf.cast(masks['u'], DTYPE)\n","    masks['u'] = tf.stop_gradient(masks['u'])\n","    # 极端情况，当ori的预测完全准确，即class i = 1, 其他类别为0时，\n","    # aug的class i最大，即最大的负数，两者相乘再取负，就是一个非常接近于0的数字\n","    cross_entroy['u'] = tf.reduce_sum(-cross_entroy['u'] * masks['u']) / \\\n","                        tf.convert_to_tensor((batch_size * uda_data), dtype=DTYPE)\n","\n","    return logits, labels, masks, cross_entroy\n","\n","\n","# if __name__ == '__main__':\n","#     # 制作数据\n","#     l_images = np.random.random((1, 32, 32, 3))\n","#     l_images = tf.convert_to_tensor(l_images, dtype=DTYPE)\n","#     ori_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     ori_images = tf.convert_to_tensor(ori_images, dtype=DTYPE)\n","#     aug_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     aug_images = tf.convert_to_tensor(aug_images, dtype=DTYPE)\n","#     all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [3, 32, 32, 3]\n","\n","#     l_labels = np.array([2])\n","#     l_labels = tf.convert_to_tensor(l_labels, dtype=tf.int32)\n","#     l_labels = tf.raw_ops.OneHot(indices=l_labels, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","#     l_labels = tf.cast(l_labels, DTYPE)\n","\n","#     # 构建teacher模型，产生输出\n","#     teacher = Wrn28k(num_inp_filters=3, k=2)\n","#     output = teacher(x=all_images)  # shape=[15, 10]\n","\n","#     logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, 1)\n","#     print('logits: ', logits.keys())\n","#     print('labels: ', labels.keys())\n","#     print('masks: ', masks.keys())\n","#     # print('cross entroy: ', cross_entroy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4z4XaIqi7seI","cellView":"form"},"source":["#@title Test\n","import os\n","\n","# from WideResnet import WideResnet\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import pandas as pd\n","\n","# import config\n","\n","\n","def test(student, file_paths, labels):\n","    student.training = False\n","    # 准备数据\n","    # df_label = pd.read_csv(TEST_FILE_PATH)\n","    # file_paths = df_label['file_name'].values\n","    # labels = df_label['label'].values\n","\n","    # testing\n","    total_num = int(len(labels)/2)\n","    corrent_num = 0\n","    for i in range(total_num):\n","        img_file = file_paths[i]\n","        label = int(labels[i])\n","\n","        # 对图片的处理\n","        img = tf.io.read_file(img_file)\n","        img = tf.image.decode_jpeg(img, channels=3)\n","        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","        img = tf.cast(img, dtype=DTYPE) / 255.0\n","        img = tf.expand_dims(img, axis=0)\n","        mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","        std = tf.expand_dims(tf.convert_to_tensor([0.0737, 0.0737, 0.0737], dtype=DTYPE), axis=0)\n","        img = (img - mean) / std\n","\n","        # 网络\n","        output = student(img)\n","        output = tf.nn.softmax(output)\n","        class_index = tf.squeeze(tf.math.argmax(output, axis=1))\n","\n","        if class_index == label:\n","            corrent_num += 1\n","    accuracy = float(corrent_num) / float(total_num) * 100.\n","    student.training = True\n","    return accuracy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"aLLiFSNQ7seJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628590131639,"user_tz":-60,"elapsed":14,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"1e3b6075-1f8c-466d-e996-ef9c6d0b517c"},"source":["#@title learning rate\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","# import config\n","\n","\n","class LearningRate(object):\n","    def __init__(self, initial_lr, num_warmup_steps, num_wait_steps=None):\n","        if initial_lr is None:\n","            raise ValueError(f'initial_lr is error in learningRate file')\n","        if num_warmup_steps is None:\n","            raise ValueError(f'num_warmup_steps is error in learningRate file')\n","        if num_wait_steps is None:\n","            raise ValueError(f'num_wait_steps is error in learningRate file')\n","\n","        # initial_lr = initial_lr * BATCH_SIZE / 256\n","        self.initial_lr = initial_lr\n","        self.num_warmup_steps = num_warmup_steps\n","        self.num_wait_steps = num_wait_steps\n","\n","        if LR_DECAY_TYPE == 'constant':\n","            self.lr = tf.constant(self.initial_lr, dtype=tf.float32)\n","\n","        elif LR_DECAY_TYPE == 'exponential':\n","            self.lr = keras.optimizers.schedules.ExponentialDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=NUM_DECAY_STEPS,\n","                decay_rate=LR_DECAY_RATE,\n","            )\n","\n","        elif LR_DECAY_TYPE == 'cosine':\n","            self.lr = keras.experimental.CosineDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=MAX_STEPS - self.num_wait_steps - self.num_warmup_steps,\n","                alpha=0.0\n","            )\n","        else:\n","            raise ValueError(f'unknown lr_decay_type in py')\n","\n","    def __call__(self, global_step):\n","        global_step = global_step - self.num_wait_steps\n","        if LR_DECAY_TYPE == 'constant':\n","            learn_rate = self.lr\n","        else:\n","            learn_rate = self.lr.__call__(global_step)\n","\n","        r = tf.constant((global_step + 1), tf.float32) / tf.constant(self.num_warmup_steps, tf.float32)\n","        warmup_lr = self.initial_lr * r\n","        lr = tf.cond(\n","            tf.cast(global_step, tf.int32) < tf.cast(self.num_warmup_steps, tf.int32),\n","            lambda: warmup_lr,\n","            lambda: learn_rate,\n","        )\n","        lr = tf.cond(global_step < 0, lambda: tf.constant(0., tf.float32), lambda: lr)\n","        return lr\n","\n","\n","'''\n","def LearningRate(initial_lr, num_warmup_steps, num_wait_steps):\n","    if initial_lr is None:\n","        raise ValueError(f'initial_lr is error in learningRate file')\n","    if num_warmup_steps is None:\n","        raise ValueError(f'num_warmup_steps is error in learningRate file')\n","    if num_wait_steps is None:\n","        raise ValueError(f'num_wait_steps is error in learningRate file')\n","    initial_lr = initial_lr * BATCH_SIZE / 256\n","    if LR_DECAY_TYPE == 'constant':\n","        lr = tf.constant(initial_lr, dtype=tf.float32)\n","    elif LR_DECAY_TYPE == 'exponential':\n","        lr = keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=NUM_DECAY_STEPS,\n","            decay_rate=LR_DECAY_RATE,\n","        )\n","    elif LR_DECAY_TYPE == 'cosine':\n","        lr = keras.experimental.CosineDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=MAX_STEPS - num_wait_steps - num_warmup_steps,\n","            alpha=0.0\n","        )\n","    else:\n","        raise ValueError(f'unknown lr_decay_type in py')\n","    return lr\n","'''\n","\n","import math\n","def lr_lambda(current_step):\n","    if current_step < 0:\n","        return float(current_step) / float(max(1, 0))\n","\n","    progress = float(current_step - 0) / \\\n","               float(max(1, 10 - 0))\n","    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress)))\n","\n","\n","if __name__ == '__main__':\n","    for i in range(10):\n","        print(lr_lambda(i))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.0\n","0.9755282581475768\n","0.9045084971874737\n","0.7938926261462366\n","0.6545084971874737\n","0.5\n","0.34549150281252633\n","0.2061073738537635\n","0.09549150281252633\n","0.024471741852423234\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Bd-iazA7seK","cellView":"form"},"source":["#@title C Dataset MLP\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# import config\n","import sys  \n","sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL')\n","\n","# from Self_augment import RandAugment\n","\n","\n","def normalize_image(img, label):\n","    '''\n","    图片的归一化\n","    :param img:\n","    :param label:\n","    :return:\n","    '''\n","    return tf.cast(img, tf.float32) / 255.0, label\n","\n","\n","# 制作有标签的数据集\n","def label_image(img_file, label):\n","    '''\n","    获取图片，对图片做水平翻转 随机剪裁等， label变为onehot\n","    :param img_file:\n","    :param label:\n","    :return:\n","    '''\n","    # 对图片的处理\n","    img = tf.io.read_file(img_file)\n","    # img = tf.image.grayscale_to_rgb(img, name=None)\n","    # img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE, 3] )\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.random_flip_left_right(img)\n","    img = tf.image.rot90(img) \n","    img = tf.image.random_flip_up_down(img)\n","    img = tf.image.resize(img, (IMG_SIZE + 5, IMG_SIZE + 5))\n","    img = tf.image.random_crop(img, (IMG_SIZE, IMG_SIZE, 3))\n","    img = tf.cast(img, DTYPE) / 255.0\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.0740, 0.0740, 0.0740], dtype=DTYPE), axis=0)\n","    img = (img-mean)/std\n","\n","\n","    # 对标签的处理\n","    label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","    label = tf.cast(label, dtype=DTYPE)\n","\n","    return {'images': img, 'labels': label}\n","\n","\n","# 制作无标签的数据集\n","def unlabel_image(img_file, label):\n","    '''\n","    处理无标签数据\n","    :param img_file:\n","    :param label:\n","    :return: 两张图片，一张经过轻微变换后的图片称为ori_image 一张经过较为剧烈变化后的图片，称为aug_images\n","    '''\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","    # aug_image = mask_label(img)\n","    aug_image = aug.distort(img)\n","    \n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, DTYPE) / 255.0\n","    ori_image = tf.cast(ori_image, DTYPE) / 255.0\n","\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.2153,0.2153,0.2153], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.2196, 0.2196, 0.2196], dtype=DTYPE), axis=0)\n","\n","    aug_image = (aug_image-mean)/std\n","    ori_image = (ori_image-mean)/std\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n","def merge_dataset(label_data, unlabel_data):\n","    return label_data['images'], label_data['labels'], unlabel_data['ori_images'], unlabel_data['aug_images']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQiFJD8MiXTb","executionInfo":{"status":"ok","timestamp":1628590165936,"user_tz":-60,"elapsed":244,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"ed130c94-f7cf-447d-e9cc-aa64e29601b2"},"source":["unique(dfCA['label'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Benign', 'Cancer'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"bCcQzhaeE6Yu"},"source":["dfc_.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEB-BzfizIZI"},"source":["from google.colab import files\n","files.download('/content/drive/MyDrive/Thesis') \n","files.download('/content/drive/MyDrive/Colab Notebooks') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mn1VQd5Q7seL","colab":{"base_uri":"https://localhost:8080/","height":1000},"cellView":"form","executionInfo":{"status":"error","timestamp":1628674858374,"user_tz":-60,"elapsed":22680492,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"17e079be-195e-494e-ee31-3abcf3bb2ec9"},"source":["#@title Region of tumour classification Meta Pseudo Labels Train\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import tensorflow_addons as tfa\n","# from WideResnet import WideResnet\n","from copy import deepcopy\n","import sklearn\n","from sklearn import preprocessing\n","\n","# import config\n","# from Model import Wrn28k\n","# from UdaCrossEntroy import UdaCrossEntroy\n","# from learningRate import LearningRate\n","# from Dataset import label_image\n","# from Dataset import unlabel_image\n","# from Dataset import merge_dataset\n","# from test import test\n","\n","\n","def my_update(model, model_):\n","    for i in range(len(model_)):\n","        model.weights[i] = model.weights[i].assign(\n","            model.weights[i]*(1-EMA)+model_[i]*EMA)\n","    model_ = deepcopy(model.weights)\n","    return model, model_\n","\n","\n","if __name__ == '__main__':\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","    # 有标签的数据集 batch_size=BATCH_SIZE\n","    # df_label = pd.read_csv(LABEL_FILE_PATH)\n","    le = preprocessing.LabelEncoder()\n","    dfCA['label'] = le.fit_transform(dfCA.label.values)\n","\n","    u_file_paths = dfUno['image'].values\n","    u_labels = dfUno['image'].values\n","    # for i in range(0, len(data)): \n","    #   path = root + data[\"fullPath\"][i]#2499\n","    #   path = path.replace('\\\\', '/')\n","    #   path = path.replace('.png', '.jpg')\n","    #   u_file_paths.append(path)\n","    #   if data[\"Status\"][i] ==\"Cancer\":\n","    #     u_labels.append(3)\n","    #   elif data[\"Status\"][i] == \"Normal\":\n","    #     u_labels.append(1)\n","    #   elif data[\"Status\"][i] == \"Benign\" :\n","    #     u_labels.append(2)\n","  \n","    train_dfCA = dfCA[:int(len(dfCA)*0.7)] \n","    test_dfCA = dfCA[-int(len(dfCA)*0.7):]\n","    t_file_paths = test_dfCA['image'].values\n","    t_labels = test_dfCA['label'].values\n","    file_paths = train_dfCA['image'].values\n","    labels = train_dfCA['label'].values\n","    ds_label_train = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n","    ds_label_train = ds_label_train \\\n","        .map(label_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=500) \\\n","        .batch(BATCH_SIZE, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 无标签的数据集 batch_size=BATCH_SIZE*UDA_DATA\n","    # df_unlabel = pd.read_csv(UNLABEL_FILE_PATH)\n","    # file_paths = df_unlabel['name'].values\n","    # labels = df_unlabel['label'].values\n","    ds_unlabel_train = tf.data.Dataset.from_tensor_slices((u_file_paths, u_labels))\n","    ds_unlabel_train = ds_unlabel_train \\\n","        .map(unlabel_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=500) \\\n","        .batch(BATCH_SIZE * UDA_DATA, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 将有标签数据和无标签数据整合成最终的数据形式\n","    ds_train = tf.data.Dataset.zip((ds_label_train, ds_unlabel_train))\n","    ds_train = ds_train.map(merge_dataset)\n","\n","    # 构建teacher模型\n","    if TEA_CONTINUE:\n","        print('continue teacher training')\n","        teacher = CmodelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","        teacher.load_weights(TEA_LOAD_PATH)\n","        teacher.training = True\n","    else:\n","        # teacher = Wrn28k(num_inp_filters=3, k=2)\n","        teacher =  CmodelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","\n","    # 构建student模型\n","    if STD_CONTINUE:\n","        print('continue student training')\n","        student =  CmodelS#efficientunet.get_efficient_unet_b0(input_shape,  pretrained=False)\n","        student.load_weights(STD_LOAD_PATH)\n","        student.training = True\n","        try:\n","          student = tf.saved_model.load(STD_LOAD_PATH)\n","        except:\n","          pass\n","    else:\n","        # student = Wrn28k(num_inp_filters=3, k=2)\n","        student =  CmodelS#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","    student_ = student.weights\n","\n","    # 定义teacher的损失函数，损失函数之一为UdaCrossEntroy\n","    mpl_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","    # 定义student的损失函数， PS：teacher的损失函数为UdaCrossEntroy\n","    s_unlabel_loss = tf.losses.CategoricalCrossentropy(\n","        label_smoothing=LABEL_SMOOTHING,\n","        from_logits=True,\n","        reduction=tf.keras.losses.Reduction.NONE,\n","    )\n","\n","    s_label_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.keras.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","\n","    # 定义teacher的学习率\n","    Tea_lr_fun = LearningRate(\n","        TEACHER_LR,\n","        TEACHER_LR_WARMUP_STEPS,\n","        TEACHER_NUM_WAIT_STEPS\n","    )\n","    # 定义student的学习率\n","    Std_lr_fun = LearningRate(\n","        STUDENT_LR,\n","        STUDENT_LR_WARMUP_STEPS,\n","        STUDENT_LR_WAIT_STEPS\n","    )\n","\n","    global_step = 62*CONTINUE_EPOCH\n","    print(f'start training from global step {global_step}......')\n","    TBacc = 0.78\n","    Tacc = 0\n","    SBacc = 0.31\n","    Sacc = 0\n","    epochs = MAX_EPOCHS - CONTINUE_EPOCH\n","    for epoch in range(epochs):\n","        TLOSS = 0\n","        TLOSS_1 = 0\n","        TLOSS_2 = 0\n","        TLOSS_3 = 0\n","        SLOSS = 0\n","        for batch_idx, (l_images, l_labels, ori_images, aug_images) in enumerate(ds_train):\n","            global_step += 1\n","            all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [15, 32, 32, 3]\n","            u_aug_and_l_images = tf.concat([aug_images, l_images], axis=0)\n","            # step1：经过teacher，得到输出\n","            with tf.GradientTape() as t_tape:\n","                output = teacher(all_images)  # shape=[15, 10]\n","                logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, global_step)\n","            # step2：1st call student -----------------------------\n","            with tf.GradientTape() as s_tape:\n","                logits['s_on_aug_and_l'] = student(u_aug_and_l_images)  # shape=[8, 10]\n","                logits['s_on_u'], logits['s_on_l_old'] = tf.split(\n","                    logits['s_on_aug_and_l'],\n","                    [aug_images.shape[0], l_images.shape[0]],\n","                    axis=0\n","                )\n","                cross_entroy['s_on_u'] = s_unlabel_loss(\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], -1)),\n","                    y_pred=logits['s_on_u']\n","                )\n","                # 计算损失函数\n","                cross_entroy['s_on_u'] = tf.reduce_sum(cross_entroy['s_on_u']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=tf.float32)\n","                SLOSS += cross_entroy['s_on_u']\n","                # for taylor\n","                cross_entroy['s_on_l_old'] = s_label_loss(\n","                    y_true=labels['l'],\n","                    y_pred=logits['s_on_l_old']\n","                )\n","\n","                cross_entroy['s_on_l_old'] = tf.reduce_sum(cross_entroy['s_on_l_old']) / \\\n","                                             tf.convert_to_tensor(BATCH_SIZE, dtype=tf.float32)\n","            # 反向传播，更新student的参数-------\n","            StudentLR = Std_lr_fun.__call__(global_step=global_step)\n","            StdOptim = keras.optimizers.SGD(\n","                learning_rate=StudentLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # StdOptim = keras.optimizers.Adam(learning_rate=StudentLR)\n","            GStud_unlabel = s_tape.gradient(cross_entroy['s_on_u'], student.trainable_variables)\n","            GStud_unlabel, _ = tf.clip_by_global_norm(GStud_unlabel, GRAD_BOUND)\n","            StdOptim.apply_gradients(zip(GStud_unlabel, student.trainable_variables))\n","            # 如何更新参数\n","            student, student_ = my_update(student, student_)\n","\n","            # step3: 2nd call student ------------------------------\n","            logits['s_on_l_new'] = student(l_images)\n","            cross_entroy['s_on_l_new'] = s_label_loss(\n","                y_true=labels['l'],\n","                y_pred=logits['s_on_l_new']\n","            )\n","            cross_entroy['s_on_l_new'] = tf.reduce_sum(cross_entroy['s_on_l_new']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE, dtype=DTYPE)\n","            dot_product = cross_entroy['s_on_l_new'] - cross_entroy['s_on_l_old']\n","            limit = 3.0**(0.5)\n","            moving_dot_product = tf.random_uniform_initializer(minval=-limit, maxval=limit)(shape=dot_product.shape)\n","            moving_dot_product = tf.Variable(initial_value=moving_dot_product, trainable=False, dtype=DTYPE)\n","            moving_dot_product_update = moving_dot_product.assign_sub(0.01 * (moving_dot_product - dot_product))\n","            dot_product = dot_product - moving_dot_product\n","            dot_product = tf.stop_gradient(dot_product)\n","            # step4: 求teacher的损失函数\n","            with t_tape:\n","                # label = tf.math.argmax(tf.nn.softmax(logits['aug'], axis=-1), axis=-1)\n","                # label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","                cross_entroy['mpl'] = mpl_loss(\n","                    # y_true=tf.stop_gradient(label),\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], axis=-1)),\n","                    y_pred=logits['aug']\n","                )  # 恒正\n","                cross_entroy['mpl'] = tf.reduce_sum(cross_entroy['mpl']) / \\\n","                                      tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=DTYPE)\n","                uda_weight = UDA_WEIGHT * tf.math.minimum(\n","                    1., tf.cast(global_step, DTYPE) / float(UDA_STEPS)\n","                )\n","                # if StudentLR == 0:\n","                #     dot_product = 0\n","                teacher_loss = cross_entroy['u'] * uda_weight + \\\n","                               cross_entroy['l'] + \\\n","                               cross_entroy['mpl'] * dot_product\n","\n","                TLOSS += teacher_loss\n","                TLOSS_1 += (cross_entroy['u'] * uda_weight)\n","                TLOSS_2 += cross_entroy['l']\n","                TLOSS_3 += cross_entroy['mpl'] * dot_product\n","            # 反向传播，更新teacher的参数-------\n","            TeacherLR = Tea_lr_fun.__call__(global_step=global_step)\n","            TeaOptim = keras.optimizers.SGD(\n","                learning_rate=TeacherLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # TeaOptim = keras.optimizers.Adam(learning_rate=TeacherLR)\n","            GTea = t_tape.gradient(teacher_loss, teacher.trainable_variables)\n","            GTea, _ = tf.clip_by_global_norm(GTea, GRAD_BOUND)\n","            TeaOptim.apply_gradients(zip(GTea, teacher.trainable_variables))\n","\n","            if (batch_idx + 1) % LOG_EVERY == 0:\n","                TLOSS = TLOSS / LOG_EVERY\n","                TLOSS_1 = TLOSS_1 / LOG_EVERY\n","                TLOSS_2 = TLOSS_2 / LOG_EVERY\n","                TLOSS_3 = TLOSS_3 / LOG_EVERY\n","                SLOSS = SLOSS / LOG_EVERY\n","                print(f'global: %4d' % global_step + ',[epoch:%4d/' % (epoch+CONTINUE_EPOCH) + 'EPOCH: %4d] \\t' % epochs\n","                      + '[U:%.4f' % (TLOSS_1) + ', L:%.4f' % (TLOSS_2) + ', M:%.4f' % (\n","                          TLOSS_3) + ']' + '[TLoss: %.4f]' % TLOSS + '/[SLoss: %.4f]' % SLOSS\n","                      + '\\t[TLR: %.6f' % TeacherLR + ']/[SLR: %.6f]' % StudentLR)\n","                TLOSS = 0\n","                TLOSS_1 = 0\n","                TLOSS_2 = 0\n","                TLOSS_3 = 0\n","                SLOSS = 0\n","        # 测试teacher在test上的acc\n","        if epoch % 5 == 0:\n","            Tacc = test(teacher, t_file_paths, t_labels)\n","            print(f'testing teacher model ... acc: {Tacc}')\n","        # 测试student在test上的acc，当student开始训练的时候\n","        if (StudentLR > 0) and (epoch % 5 == 0):\n","            Sacc = test(student, t_file_paths, t_labels)\n","            print(f'testing ... acc: {Sacc}')\n","        # 保存weights\n","        if Tacc > TBacc:\n","            Tsave_path = TEA_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            teacher.save_weights(Tsave_path)\n","            # tf.saved_model.save(teacher, Tsave_path)\n","            TBacc = Tacc\n","            print(f'saving for TBacc {TBacc}, Tpath:{Tsave_path}')\n","        if Sacc > SBacc:\n","            Ssave_path = STD_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            student.save_weights(Ssave_path)\n","            SBacc = Sacc\n","            print(f'saving for SBacc {SBacc}, Spath:{Ssave_path}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["global: 59210,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6886, M:0.0813][TLoss: 0.7699]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59230,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6876, M:0.1529][TLoss: 0.8405]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59250,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6939, M:0.3886][TLoss: 1.0825]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59270,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6939, M:0.1725][TLoss: 0.8664]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59290,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6849, M:0.0212][TLoss: 0.7062]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59310,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6940, M:0.1738][TLoss: 0.8678]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59330,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6948, M:-0.0870][TLoss: 0.6078]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59350,[epoch: 897/EPOCH: 1035] \t[U:0.0000, L:0.6898, M:0.0987][TLoss: 0.7885]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000390]\n","global: 59375,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6915, M:-0.0121][TLoss: 0.6794]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000389]\n","global: 59395,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6948, M:0.0609][TLoss: 0.7557]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000389]\n","global: 59415,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6940, M:-0.0835][TLoss: 0.6105]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000389]\n","global: 59435,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6923, M:-0.1875][TLoss: 0.5048]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000389]\n","global: 59455,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6923, M:-0.0526][TLoss: 0.6397]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000389]\n","global: 59475,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6971, M:0.2168][TLoss: 0.9139]/[SLoss: 0.6931]\t[TLR: 0.000388]/[SLR: 0.000389]\n","global: 59495,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6900, M:-0.2150][TLoss: 0.4750]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59515,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6890, M:-0.0042][TLoss: 0.6848]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59535,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6889, M:-0.0069][TLoss: 0.6820]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59555,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6949, M:-0.0948][TLoss: 0.6001]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59575,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6871, M:-0.2153][TLoss: 0.4718]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59595,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6984, M:0.1934][TLoss: 0.8917]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59615,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6940, M:-0.1890][TLoss: 0.5050]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59635,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6881, M:0.0284][TLoss: 0.7165]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000389]\n","global: 59655,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6974, M:0.1064][TLoss: 0.8037]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000388]\n","global: 59675,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6873, M:0.0094][TLoss: 0.6967]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000388]\n","global: 59695,[epoch: 898/EPOCH: 1035] \t[U:0.0000, L:0.6932, M:-0.1135][TLoss: 0.5797]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000388]\n","global: 59720,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6906, M:-0.0861][TLoss: 0.6045]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000388]\n","global: 59740,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6932, M:-0.0379][TLoss: 0.6553]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000388]\n","global: 59760,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6852, M:0.1443][TLoss: 0.8295]/[SLoss: 0.6931]\t[TLR: 0.000387]/[SLR: 0.000388]\n","global: 59780,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6959, M:-0.1208][TLoss: 0.5751]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000388]\n","global: 59800,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6860, M:0.2734][TLoss: 0.9594]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000388]\n","global: 59820,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6847, M:-0.2276][TLoss: 0.4570]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000388]\n","global: 59840,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.7000, M:-0.0571][TLoss: 0.6429]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000388]\n","global: 59860,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6969, M:-0.0186][TLoss: 0.6783]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000388]\n","global: 59880,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6923, M:-0.0826][TLoss: 0.6098]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000388]\n","global: 59900,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6878, M:-0.1436][TLoss: 0.5441]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000388]\n","global: 59920,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.7013, M:-0.2672][TLoss: 0.4341]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000387]\n","global: 59940,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6897, M:0.0452][TLoss: 0.7349]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000387]\n","global: 59960,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6896, M:0.0056][TLoss: 0.6952]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000387]\n","global: 59980,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6914, M:-0.0764][TLoss: 0.6151]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000387]\n","global: 60000,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6985, M:0.0040][TLoss: 0.7025]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000387]\n","global: 60020,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6897, M:-0.0599][TLoss: 0.6298]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000387]\n","global: 60040,[epoch: 899/EPOCH: 1035] \t[U:0.0000, L:0.6975, M:0.0264][TLoss: 0.7239]/[SLoss: 0.6931]\t[TLR: 0.000386]/[SLR: 0.000387]\n","global: 60065,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6931, M:-0.0113][TLoss: 0.6818]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000387]\n","global: 60085,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6939, M:-0.0276][TLoss: 0.6663]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000387]\n","global: 60105,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6923, M:0.1617][TLoss: 0.8540]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000387]\n","global: 60125,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6947, M:-0.0542][TLoss: 0.6405]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000387]\n","global: 60145,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6891, M:0.0005][TLoss: 0.6896]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000387]\n","global: 60165,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6923, M:-0.1929][TLoss: 0.4995]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000387]\n","global: 60185,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6956, M:-0.0861][TLoss: 0.6095]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000387]\n","global: 60205,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6954, M:0.0821][TLoss: 0.7775]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60225,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6931, M:-0.1267][TLoss: 0.5664]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60245,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6938, M:-0.5176][TLoss: 0.1763]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60265,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6931, M:0.0797][TLoss: 0.7728]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60285,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6894, M:0.2393][TLoss: 0.9286]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60305,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6884, M:-0.2267][TLoss: 0.4616]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60325,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6931, M:-0.0944][TLoss: 0.5988]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60345,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6907, M:-0.0795][TLoss: 0.6112]/[SLoss: 0.6931]\t[TLR: 0.000385]/[SLR: 0.000386]\n","global: 60365,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6948, M:0.0479][TLoss: 0.7426]/[SLoss: 0.6931]\t[TLR: 0.000384]/[SLR: 0.000386]\n","global: 60385,[epoch: 900/EPOCH: 1035] \t[U:0.0000, L:0.6891, M:-0.1162][TLoss: 0.5729]/[SLoss: 0.6931]\t[TLR: 0.000384]/[SLR: 0.000386]\n","testing teacher model ... acc: 52.38784370477568\n","testing ... acc: 52.38784370477568\n","global: 60410,[epoch: 901/EPOCH: 1035] \t[U:0.0000, L:0.6854, M:-0.0593][TLoss: 0.6261]/[SLoss: 0.6931]\t[TLR: 0.000384]/[SLR: 0.000386]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-e36426992dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mStdOptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGStud_unlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# 如何更新参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;31m# step3: 2nd call student ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-e36426992dba>\u001b[0m in \u001b[0;36mmy_update\u001b[0;34m(model, model_)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         model.weights[i] = model.weights[i].assign(\n\u001b[0;32m---> 28\u001b[0;31m             model.weights[i]*(1-EMA)+model_[i]*EMA)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mmodel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \"\"\"\n\u001b[0;32m-> 2421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2427\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_tracked_trackables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m       \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2430\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_trainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mvariables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2183\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m     \"\"\"\n\u001b[0;32m-> 2185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \"\"\"\n\u001b[0;32m-> 2421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2427\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_tracked_trackables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m       \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2430\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_trainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mvariables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2183\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m     \"\"\"\n\u001b[0;32m-> 2185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1300\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \"\"\"\n\u001b[0;32m-> 1302\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \"\"\"\n\u001b[1;32m   1268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_variables'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2792\u001b[0m       return list(\n\u001b[1;32m   2793\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2794\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2795\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2792\u001b[0m       return list(\n\u001b[1;32m   2793\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2794\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2795\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"PQlCOS4_QCkg"},"source":["teacher.save_weights(Tsave_path)\n","student.save_weights(Ssave_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gg6Xw9x-iHkA"},"source":["# Create model"]},{"cell_type":"code","metadata":{"id":"mdJWcg-biObH"},"source":["Channels =  3#@param {type:\"integer\"}\n","Img_size = 384#@param {type:\"integer\"}\n","input_shape = (Img_size, Img_size, Channels) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPWwAz8SkX7P"},"source":["tf.keras.backend.clear_session()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rml63YcFiLxP","cellView":"form"},"source":["#@markdown Model Efficient \n","\n","# classifier =  get_efficient_unet_b4(input_shape, out_channels=2)\n","# modelS = models.Sequential()\n","# modelS.add(classifier)\n","# modelS.add(layers.Dense(n_classes))\n","# modelS.summary()\n","\n","modelS = get_efficient_unet_b4(input_shape, out_channels=2)\n","modelS.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkXW82PKibDl","cellView":"form"},"source":["#@markdown Model Efficient  teacher\n","\n","# classifier_t =  get_efficient_unet_b4(input_shape, out_channels=2)\n","\n","modelT = get_efficient_unet_b4(input_shape, out_channels=2)\n","modelT.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-j9x5-Py5RcX"},"source":["# Model(Segmentation)"]},{"cell_type":"code","metadata":{"id":"TvcW5Pc2_Cur","cellView":"form"},"source":["#@title MPL config\n","\n","import tensorflow as tf\n","\n","\n","\n","\n","# about dataset\n","IMG_SIZE = 384#@param {type:\"integer\"}\n","BATCH_SIZE = 2\n","# LABEL_FILE_PATH = '/content/cifar/label4000.csv' # google\n","# UNLABEL_FILE_PATH = '/content/cifar/train.csv'\n","\n","_MAX_LEVEL = 10\n","CUTOUT_CONST = 40.\n","TRANSLATE_CONST = 100.\n","REPLACE_COLOR = [128, 128, 128]\n","\n","\n","# LABEL_FILE_PATH = '../input/cifar10/cifar/label4000.csv'  # kaggle\n","# UNLABEL_FILE_PATH = '../input/cifar10/cifar/train.csv'\n","\n","\n","AUGMENT_MAGNITUDE = 8\n","SHUFFLE_SIZE = BATCH_SIZE * 16\n","DATA_LEN = 400  # 数据集的总长度\n","\n","# about model\n","NUM_XLA_SHARDS = -1\n","BATCH_NORM_EPSILON = 1e-3\n","BATCH_NORM_DECAY = 0.999\n","DROPOUT_RATE = 0.\n","DROPOUT = 0.2\n","NUM_CLASSES = 2#@param {type:\"integer\"}\n","NUM_CLASS = 2#@param {type:\"integer\"}\n","\n","# about training\n","LOG_EVERY = 20\n","SAVE_EVERY = 5\n","TEA_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/b4ST2'\n","STD_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/b4SS2'\n","\n","MAX_EPOCHS = 1920\n","MAX_STEPS = MAX_EPOCHS * (int(DATA_LEN / BATCH_SIZE)-1)\n","UDA_WEIGHT = 8  # uda的权重\n","UDA_STEPS = 2000\n","TEST_EVERY = 2\n","GRAD_BOUND = 1e9\n","EMA = 0.995\n","\n","\n","# continue train\n","TEA_CONTINUE = False\n","STD_CONTINUE = False\n","TEA_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/b4ST2'\n","STD_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/b4SS2'\n","CONTINUE_EPOCH = 885\n","\n","\n","# about testing\n","# TEST_FILE_PATH = '/content/cifar/test.csv'\n","# TEST_FILE_PATH = '../input/cifar10/cifar/test.csv'\n","TEST_MODEL_PATH = '/content/drive/MyDrive/Thesis/weights/SS'\n","\n","# about UdaCrossEntroy\n","UDA_DATA = 1\n","LABEL_SMOOTHING = 0.15\n","UDA_TEMP = 0.7\n","UDA_THRESHOLD = 0.6\n","\n","# about learning rate\n","STUDENT_LR = 0.0005  # student\n","STUDENT_LR_WARMUP_STEPS = 4000\n","STUDENT_LR_WAIT_STEPS = 2000\n","TEACHER_LR = 0.0005  # teacher\n","TEACHER_LR_WARMUP_STEPS = 1000\n","TEACHER_NUM_WAIT_STEPS = 0\n","\n","LR_DECAY_TYPE = 'cosine'  # constant, exponential, cosine\n","NUM_DECAY_STEPS = 300\n","LR_DECAY_RATE = 0.97\n","\n","# about optimizer\n","OPTIM_TYPE = 'sgd'  # sgd, momentum, rmsprop\n","WEIGHT_DECAY = 5e-4\n","\n","\n","# dtype\n","DTYPE = tf.float32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"JzUpbjZDSWDh"},"source":["#@title Self_aug_func\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import math\n","import tensorflow_addons.image as image_ops\n","\n","# import \n","\n","\n","def autocontrast(image):\n","    lo = tf.cast(tf.reduce_min(image, axis=[0, 1]), tf.float32)\n","    hi = tf.cast(tf.reduce_max(image, axis=[0, 1]), tf.float32)\n","    scale = tf.math.divide(255.0, (hi - lo))\n","    offset = tf.math.multiply(-lo, scale)\n","    image = tf.math.add(\n","        tf.math.multiply(tf.cast(image, tf.float32), scale),\n","        offset\n","    )\n","    image = tf.clip_by_value(image, 0.0, 255.0)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def equalize(image):\n","    # image = tf.cast(image, tf.int32)\n","    # channel = tf.shape(image)[-1]\n","    # for i in range(channel):\n","    #     im = tf.cast(image[:, :, i], tf.int32)\n","    #     histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","    #     nonzero = tf.where(tf.not_equal(histo, 0))\n","    #     nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","    #     step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","    #     print(step)\n","    #     if step == 0:\n","    #         pass\n","    #     else:\n","    #         lut = (tf.cumsum(histo) + (step // 2)) // step\n","    #         lut = tf.concat([[0], lut[:-1]], 0)\n","    #         lut = tf.clip_by_value(lut, 0, 255)\n","    #         # print(lut)\n","    #         image[:, :, i] = tf.gather(lut, image[:, :, i])\n","    #         # image[:, :, i] = im\n","    #     # image[:, :, i] = im\n","\n","    def scale_channel(im, c=0):\n","        \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n","        im = tf.cast(im[:, :, 0], tf.int32)\n","        # Compute the histogram of the image channel.\n","        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","\n","        # For the purposes of computing the step, filter out the nonzeros.\n","        nonzero = tf.where(tf.not_equal(histo, 0))\n","        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","\n","        def build_lut(histo, step):\n","            # Compute the cumulative sum, shifting by step // 2\n","            # and then normalization by step.\n","            lut = (tf.cumsum(histo) + (step // 2)) // step\n","            # Shift lut, prepending with 0.\n","            lut = tf.concat([[0], lut[:-1]], 0)\n","            # Clip the counts to be in range.  This is done\n","            # in the C code for image.point.\n","            return tf.clip_by_value(lut, 0, 255)\n","\n","        # If step is zero, return the original image.  Otherwise, build\n","        # lut from the full histogram and step and then index from it.\n","        result = tf.cond(tf.equal(step, 0),\n","                         lambda: im,\n","                         lambda: tf.gather(build_lut(histo, step), im))\n","        return tf.cast(result, tf.uint8)\n","\n","    s1 = scale_channel(image, 0)\n","    s2 = scale_channel(image, 1)\n","    s3 = scale_channel(image, 2)\n","    image = tf.stack([s1, s2, s3], 2)\n","\n","    return image\n","\n","\n","def invert(image):\n","    image = 255 - image\n","    return image\n","\n","\n","def rotate(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    degree = tf.cond(should_filp, lambda: level, lambda: -level)\n","    degree_to_radians = tf.convert_to_tensor(math.pi / 180., tf.float32)\n","    radians = tf.math.multiply(degree, degree_to_radians)\n","    new_imgsize = tf.cast(tf.math.abs(tf.divide(IMG_SIZE, radians)), tf.int32)\n","    image = tf.image.resize(image, (new_imgsize, new_imgsize))\n","    image = image_ops.rotate(image, radians, fill_mode='constant')\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def posterize(image):\n","    bit = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 4, tf.float32)\n","    shift = tf.cast(8 - bit, image.dtype)\n","    image = tf.bitwise.right_shift(image, shift)\n","    image = tf.bitwise.left_shift(image, shift)\n","    return image\n","\n","\n","def solarize_arg(image):\n","    threahold = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 22, tf.float32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def solarize_add(image, threahold=128):\n","    addition = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 2, tf.int32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.add(tf.cast(image, tf.int32), addition)\n","    image = tf.cast(tf.clip_by_value(image, 0, 255), tf.uint8)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def color(image, degenetate=None):\n","    if degenetate is None:\n","        degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.8 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def contrast(image):\n","    degenerate = tf.image.rgb_to_grayscale(image)\n","    degenerate = tf.cast(degenerate, tf.int32)\n","\n","    hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n","    mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.\n","    degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n","    degenerate = tf.clip_by_value(degenerate, 0., 255.)\n","    degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.6 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def brightness(image):\n","    image = tf.image.adjust_brightness(image, 0.25)\n","    return image\n","\n","\n","def sharpness(image):\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.6 + 0.1, tf.float32)\n","    image = tf.cast(image, tf.float32)\n","    image = image_ops.sharpness(image, factor)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_x(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.2, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_x(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_y(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.1, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_y(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def translate_x(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [-pixels, 0])\n","    return image\n","\n","\n","def translate_y(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [0, -pixels])\n","    return image\n","\n","\n","def cutout(image):\n","    pad_size = tf.cast(\n","        tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * CUTOUT_CONST,\n","        tf.int32\n","    )\n","    image_height = tf.shape(image)[0]\n","    image_width = tf.shape(image)[1]\n","\n","    # Samples the center location in the image where the zero mask is applied.\n","    cutout_center_height = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_height,\n","        dtype=tf.int32)\n","\n","    cutout_center_width = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_width,\n","        dtype=tf.int32)\n","\n","    lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n","    upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n","    left_pad = tf.maximum(0, cutout_center_width - pad_size)\n","    right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n","\n","    cutout_shape = [image_height - (lower_pad + upper_pad),\n","                    image_width - (left_pad + right_pad)]\n","    padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n","    mask = tf.pad(\n","        tf.zeros(cutout_shape, dtype=image.dtype),\n","        padding_dims, constant_values=1)\n","    mask = tf.expand_dims(mask, -1)\n","    mask = tf.tile(mask, [1, 1, 3])\n","    image = tf.where(\n","        tf.equal(mask, 0),\n","        tf.ones_like(image, dtype=image.dtype) * REPLACE_COLOR,\n","        image)\n","    return image\n","\n","\n","def identity(image):\n","    return tf.identity(image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"8e7vEGndSLme","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628511314559,"user_tz":-60,"elapsed":26,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"876a4fd8-a610-495a-8802-c18c4965ceca"},"source":["#@title Self_aug_util\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import tensorflow as tf\n","\n","# from self_aug_func import *\n","\n","_MAX_LEVEL = 10\n","\n","\n","\n","def _enhance_level_to_arg(level):\n","    return (tf.cast((level / _MAX_LEVEL) * 1.8 + 0.1, tf.float32),)\n","\n","\n","def _translate_level_to_arg(level, translate_const):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * float(translate_const), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    final_tensor = tf.cond(should_flip, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _rotate_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _shear_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 0.3, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def level_to_arg(cutout_const, translate_const):\n","    '''\n","    将对image做变化的函数所用到的参数整理成字典形式\n","    :param cutout_const:\n","    :param translate_const:\n","    :return: type:dict\n","    '''\n","    no_arg = lambda level: ()\n","    posterize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 4,\n","        tf.float32\n","    )\n","    solarize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 256,\n","        tf.float32\n","    )\n","    solarize_add_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 110,\n","        tf.float32\n","    )\n","    cutout_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * cutout_const,\n","        tf.float32\n","    )\n","    translate_arg = lambda level: _translate_level_to_arg(level, translate_const)\n","\n","    args = {\n","        'Identity': no_arg,\n","        'AutoContrast': no_arg,\n","        'Equalize': no_arg,\n","        'Invert': no_arg,\n","        'Rotate': _rotate_level_to_arg,\n","        'Posterize': posterize_arg,\n","        'Solarize': solarize_arg,\n","        'SplarizeAdd': solarize_add_arg,\n","        'Color': _enhance_level_to_arg,\n","        'Contrast': _enhance_level_to_arg,\n","        'Brightness': _enhance_level_to_arg,\n","        'Sharpness': _enhance_level_to_arg,\n","        'ShearX': _shear_level_to_arg,\n","        'ShearY': _shear_level_to_arg,\n","        'Cutout': cutout_arg,\n","        'TranslateX': translate_arg,\n","        'TranslateY': translate_arg,\n","    }\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    NAME_TO_FUNC = {\n","        'AutoContrast': autocontrast,\n","        'Equalize': equalize,\n","        'Invert': invert,\n","        'Rotate': rotate,\n","        'Posterize': posterize,\n","        'Solarize': solarize_arg,\n","        'SolarizeAdd': solarize_add,\n","        'Color': color,\n","        'Contrast': contrast,\n","        'Brightness': brightness,\n","        'Sharpness': sharpness,\n","        'ShearX': shear_x,\n","        'ShearY': shear_y,\n","        'TranslateX': translate_x,\n","        'TranslateY': translate_y,\n","        'Cutout': cutout,\n","        'Identity': identity,\n","    }\n","\n","    available_ops = [\n","        'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","        'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","    ]\n","\n","    for (i, op_name) in enumerate(available_ops):\n","        func = NAME_TO_FUNC[op_name]\n","        args = level_to_arg(4, 4)[op_name](16)\n","        print(args)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["()\n","()\n","()\n","tf.Tensor(-48.0, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(409.6, shape=(), dtype=float32)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","tf.Tensor(-0.48, shape=(), dtype=float32)\n","tf.Tensor(0.48, shape=(), dtype=float32)\n","tf.Tensor(-6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"form","id":"8lCvf2VERloi"},"source":["#@title Self_augment\n","'''\n","reference:\n","https://github.com/google-research/google-research/tree/1f1741a985a0f2e6264adae985bde664a7993bd2/flax_models/cifar/datasets\n","'''\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","\n","'''\n","可能augment.py中的内容有问题 涉及文件augment.py的line 53，54\n","引用的库不一样，因为tensorflow.contrib已经停用，\n","使用的第三方：pip install tensorflow-addons\n","'''\n","import os\n","\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","\n","# from self_aug_func import *\n","\n","# 将对图片做augment的函数变成一个字典\n","NAME_TO_FUNC = {\n","    'AutoContrast': autocontrast,\n","    'Equalize': equalize,\n","    'Invert': invert,\n","    'Rotate': rotate,\n","    'Posterize': posterize,\n","    'Solarize': solarize_arg,\n","    'SolarizeAdd': solarize_add,\n","    'Color': color,\n","    'Contrast': contrast,\n","    'Brightness': brightness,\n","    'Sharpness': sharpness,\n","    'ShearX': shear_x,\n","    'ShearY': shear_y,\n","    'TranslateX': translate_x,\n","    'TranslateY': translate_y,\n","    'Cutout': cutout,\n","    'Identity': identity,\n","}\n","# 在某些函数中有一些需要一个替换的值，比如旋转中有一些位置的像素值需要补充\n","REPLACE_FUNCS = frozenset({\n","    'Rotate',\n","    'TranslateX',\n","    'ShearX',\n","    'SHearY',\n","    'TranslateY',\n","    'Cutout',\n","})\n","\n","\n","class RandAugment(object):\n","    def __init__(self, num_layers=2, magnitude=None, cutout_const=40, translate_const=100., available_ops=None):\n","        '''\n","        reference: https://arxiv.org/abs/1909.13719\n","        :param num_layers:\n","        :param magnitude:\n","        :param cutout_const:\n","        :param translate_const:\n","        :param avalilable_ops:\n","        '''\n","        super(RandAugment, self).__init__()\n","        self.num_layers = num_layers\n","        self.cutout_const = float(cutout_const)\n","        self.translate_const = float(translate_const)\n","        if available_ops is None:\n","            available_ops = [\n","                'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","                'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","                'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","            ]\n","        self.available_ops = available_ops\n","        self.magnitude = magnitude\n","\n","    def distort(self, image):\n","        '''\n","\n","        :param image:  shape:[HWC] C=3\n","        :return: 返回一个经过变化后的图片\n","        '''\n","        input_image_type = image.dtype\n","        image = tf.clip_by_value(image, tf.cast(0, input_image_type), tf.cast(255, input_image_type))\n","        image = tf.cast(image, tf.uint8)\n","\n","        prob = tf.random.uniform([], 0.2, 0.8, tf.float32)\n","\n","        for _ in range(self.num_layers):\n","            op_to_select = tf.random.uniform([], minval=0, maxval=len(self.available_ops), dtype=tf.int32)\n","            for (i, op_name) in enumerate(self.available_ops):\n","                func = NAME_TO_FUNC[op_name]  # 得到函数名称\n","                if i == op_to_select:\n","                    flag = tf.random.uniform([], 0., 1., prob.dtype)\n","                    if tf.math.greater_equal(prob, flag):\n","                        image = func(image)\n","\n","        image = tf.cast(image, dtype=input_image_type)\n","        return image\n","\n","\n","def unlabel_image(img_file, label):\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","\n","    aug_image, some_info = aug.distort(img)\n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, tf.float32) / 255.0\n","    ori_image = tf.cast(ori_image, tf.float32) / 255.0\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AltjO-ZBKXIm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"fdGriJZoWdGq"},"source":["#@title Default title text\n","class Augment(tf.keras.layers.Layer):\n","  def __init__(self, seed=42):\n","    super().__init__()\n","    # both use the same seed, so they'll make the same randomn changes.\n","    self.augment_inputs = preprocessing.RandomFlip(mode=\"horizontal\", seed=seed)\n","    self.augment_labels = preprocessing.RandomFlip(mode=\"horizontal\", seed=seed)\n","\n","  def call(self, inputs, labels):\n","    inputs = self.augment_inputs(inputs)\n","    labels = self.augment_labels(labels)\n","    return inputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"RBd2NMyUXXH2"},"source":["#@title Tversky_loss\n","# ''''\n","# def tversky_loss(y_true, y_pred):\n","#     beta =0.7\n","#     y_true = tf.cast(y_true, tf.float32)\n","#     y_pred = tf.math.sigmoid(y_pred)\n","#     numerator = y_true * y_pred\n","#     denominator = y_true * y_pred + beta * (1 - y_true) * y_pred + (1 - beta) * y_true * (1 - y_pred)\n","\n","#     return 1 - tf.reduce_sum(numerator) / tf.reduce_sum(denominator)\n","# ''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSpsmsJEDxj1","cellView":"form"},"source":["#@title UDa\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","\n","# import config\n","# from Model import Wrn28k\n","\n","\n","def UdaCrossEntroy(all_logits, l_labels, global_step):\n","    batch_size = BATCH_SIZE\n","    uda_data = UDA_DATA\n","    logits = {}\n","    labels = {}\n","    cross_entroy = {}\n","    masks = {}\n","    # 将网络的输出结果区分成 label ori aug 三个部分\n","    logits['l'], logits['ori'], logits['aug'] = tf.split(\n","        all_logits,\n","        [batch_size, batch_size * uda_data, batch_size * uda_data],\n","        axis=0,\n","    )\n","    # 对标签进行处理\n","    labels['l'] = l_labels\n","\n","    # ------------loss的计算---------\n","    # part1：有监督部分\n","    #     reduction=keras.losses.Reduction.NONE,)(labels['l'], logits['l'])\n","    # logits['l'] = tf.nn.softmax( logits['l'])\n","    cross_entroy['l'] = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n","         reduction=keras.losses.Reduction.NONE,\n","        )(labels['l'], logits['l'])\n","    # # tf.losses.CategoricalCrossentropy(\n","    #     from_logits=True,\n","    #     label_smoothing=LABEL_SMOOTHING,\n","    #     reduction=keras.losses.Reduction.NONE,)(labels['l'], logits['l'])\n","    \n","    '''\n","    probs = tf.nn.softmax(logits['l'], axis=-1)  # 将每张图片对应10个类别的输出转化为概率的形式\n","    correct_probs = tf.reduce_sum(labels['l'] * probs, axis=-1)  # 根据图片对应的label和概率计算出 预测正确类别的概率\n","    # 计算一个阈值l_threshold\n","    r = tf.cast(global_step, tf.float32) / tf.convert_to_tensor(MAX_STEPS, dtype=tf.float32)\n","    num_classes = tf.convert_to_tensor(NUM_CLASSES, tf.float32)\n","    l_threshold = r * (1. - 1. / num_classes) + 1. / num_classes\n","    masks['l'] = tf.math.less_equal(correct_probs, l_threshold)\n","    masks['l'] = tf.cast(masks['l'], tf.float32)\n","    masks['l'] = tf.stop_gradient(masks['l'])  # 如果对某图片预测的概率小于l_threahold,输出1，否则是0\n","    '''\n","    cross_entroy['l'] = tf.reduce_sum(cross_entroy['l']) / float(batch_size)\n","\n","    # part2: 无监督部分\n","    labels['ori'] = tf.nn.softmax(logits['ori'] / tf.convert_to_tensor(UDA_TEMP), axis=-1)\n","    labels['ori'] = tf.stop_gradient(labels['ori'])\n","    # tf.nn.log_softmax: 设一张图片对应3个类别的输出为o1，o2，o3 ==>\n","    # b = log(sum(exp(o1) + exp(o2) + exp(o3)))  new_o1=o1-b, new_o2=o2-b ... 恒负，大小关系不变\n","    cross_entroy['u'] = (\n","            labels['ori'] * tf.nn.log_softmax(logits['aug'], axis=-1)\n","    )\n","\n","    largest_probs = tf.reduce_max(labels['ori'], axis=-1, keepdims=True)\n","\n","    masks['u'] = tf.math.greater_equal(largest_probs, tf.constant(UDA_THRESHOLD))  # 判断最大概率是否大于阈值\n","    masks['u'] = tf.cast(masks['u'], DTYPE)\n","    masks['u'] = tf.stop_gradient(masks['u'])\n","    # 极端情况，当ori的预测完全准确，即class i = 1, 其他类别为0时，\n","    # aug的class i最大，即最大的负数，两者相乘再取负，就是一个非常接近于0的数字\n","    cross_entroy['u'] = tf.reduce_sum(-cross_entroy['u'] * masks['u']) / \\\n","                        tf.convert_to_tensor((batch_size * uda_data), dtype=DTYPE)\n","\n","    return logits, labels, masks, cross_entroy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"NEFmWImkDNid"},"source":["#@title Test\n","import os\n","\n","# from WideResnet import WideResnet\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import pandas as pd\n","\n","# import config\n","\n","\n","def test(student, file_paths, labels):\n","    student.training = False\n","    # 准备数据\n","    # df_label = pd.read_csv(TEST_FILE_PATH)\n","    # file_paths = df_label['file_name'].values\n","    # labels = df_label['label'].values\n","\n","    # testing\n","    total_num = int(len(labels)/2)\n","    corrent_num = 0\n","    for i in range(total_num):\n","        img_file = file_paths[i]\n","        label = labels[i]\n","\n","        label = tf.io.read_file(label)\n","        label = tf.image.decode_jpeg(label, channels=1)\n","        label = tf.image.resize(label, (IMG_SIZE, IMG_SIZE))\n","        # 对图片的处理\n","        img = tf.io.read_file(img_file)\n","        img = tf.image.decode_jpeg(img, channels=3)\n","        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","        img = tf.cast(img, dtype=DTYPE) / 255.0\n","        img = tf.expand_dims(img, axis=0)\n","        mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","        std = tf.expand_dims(tf.convert_to_tensor([0.0737, 0.0737,0.0737], dtype=DTYPE), axis=0)\n","        img = (img - mean) / std\n","\n","        # 网络\n","        output = student.predict(img)\n","        # output = tf.nn.sigmoid(output)\n","        # output = tf.nn.softmax(output)\n","        # output = tf.squeeze(tf.math.argmax(output, axis=1))\n","        # output = tf.where(tf.math.greater(output, 0), x=1.0, y=0.0)\n","        # if class_index == label:\n","        # plt.imshow(np.squeeze(output))\n","        m = tf.keras.metrics.MeanIoU(num_classes=3)\n","        m = update_state(label, output)#dice_coeff(label, output)#\n","        m.result().numpy()\n","        # print(m)\n","        corrent_num += m#mean_iou(label, output)\n","    accuracy = float(corrent_num) / float(total_num) * 100.\n","    student.training = True\n","    return accuracy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"8xd5evraDERY","executionInfo":{"status":"ok","timestamp":1628512170579,"user_tz":-60,"elapsed":25,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"43b78866-9db0-4191-e8bc-4113ca419c61"},"source":["#@title learning rate\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","# import config\n","\n","\n","class LearningRate(object):\n","    def __init__(self, initial_lr, num_warmup_steps, num_wait_steps=None):\n","        if initial_lr is None:\n","            raise ValueError(f'initial_lr is error in learningRate file')\n","        if num_warmup_steps is None:\n","            raise ValueError(f'num_warmup_steps is error in learningRate file')\n","        if num_wait_steps is None:\n","            raise ValueError(f'num_wait_steps is error in learningRate file')\n","\n","        # initial_lr = initial_lr * BATCH_SIZE / 256\n","        self.initial_lr = initial_lr\n","        self.num_warmup_steps = num_warmup_steps\n","        self.num_wait_steps = num_wait_steps\n","\n","        if LR_DECAY_TYPE == 'constant':\n","            self.lr = tf.constant(self.initial_lr, dtype=tf.float32)\n","\n","        elif LR_DECAY_TYPE == 'exponential':\n","            self.lr = keras.optimizers.schedules.ExponentialDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=NUM_DECAY_STEPS,\n","                decay_rate=LR_DECAY_RATE,\n","            )\n","\n","        elif LR_DECAY_TYPE == 'cosine':\n","            self.lr = keras.experimental.CosineDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=MAX_STEPS - self.num_wait_steps - self.num_warmup_steps,\n","                alpha=0.0\n","            )\n","        else:\n","            raise ValueError(f'unknown lr_decay_type in py')\n","\n","    def __call__(self, global_step):\n","        global_step = global_step - self.num_wait_steps\n","        if LR_DECAY_TYPE == 'constant':\n","            learn_rate = self.lr\n","        else:\n","            learn_rate = self.lr.__call__(global_step)\n","\n","        r = tf.constant((global_step + 1), tf.float32) / tf.constant(self.num_warmup_steps, tf.float32)\n","        warmup_lr = self.initial_lr * r\n","        lr = tf.cond(\n","            tf.cast(global_step, tf.int32) < tf.cast(self.num_warmup_steps, tf.int32),\n","            lambda: warmup_lr,\n","            lambda: learn_rate,\n","        )\n","        lr = tf.cond(global_step < 0, lambda: tf.constant(0., tf.float32), lambda: lr)\n","        return lr\n","\n","\n","'''\n","def LearningRate(initial_lr, num_warmup_steps, num_wait_steps):\n","    if initial_lr is None:\n","        raise ValueError(f'initial_lr is error in learningRate file')\n","    if num_warmup_steps is None:\n","        raise ValueError(f'num_warmup_steps is error in learningRate file')\n","    if num_wait_steps is None:\n","        raise ValueError(f'num_wait_steps is error in learningRate file')\n","    initial_lr = initial_lr * BATCH_SIZE / 256\n","    if LR_DECAY_TYPE == 'constant':\n","        lr = tf.constant(initial_lr, dtype=tf.float32)\n","    elif LR_DECAY_TYPE == 'exponential':\n","        lr = keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=NUM_DECAY_STEPS,\n","            decay_rate=LR_DECAY_RATE,\n","        )\n","    elif LR_DECAY_TYPE == 'cosine':\n","        lr = keras.experimental.CosineDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=MAX_STEPS - num_wait_steps - num_warmup_steps,\n","            alpha=0.0\n","        )\n","    else:\n","        raise ValueError(f'unknown lr_decay_type in py')\n","    return lr\n","'''\n","\n","import math\n","def lr_lambda(current_step):\n","    if current_step < 0:\n","        return float(current_step) / float(max(1, 0))\n","\n","    progress = float(current_step - 0) / \\\n","               float(max(1, 10 - 0))\n","    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress)))\n","\n","\n","if __name__ == '__main__':\n","    for i in range(10):\n","        print(lr_lambda(i))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.0\n","0.9755282581475768\n","0.9045084971874737\n","0.7938926261462366\n","0.6545084971874737\n","0.5\n","0.34549150281252633\n","0.2061073738537635\n","0.09549150281252633\n","0.024471741852423234\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"iXXSwGDtlUYl","executionInfo":{"status":"error","timestamp":1628507036625,"user_tz":-60,"elapsed":346,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"b937a0e1-fe1a-438f-ba75-41702787642f"},"source":["tf.enable_eager_execution()\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-118-ddf3115bdcc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'enable_eager_execution'"]}]},{"cell_type":"code","metadata":{"id":"9gnHT8aeA7sb","cellView":"form"},"source":["#@title C Dataset MLP\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.utils import to_categorical\n","# from tf.keras.utils import to_categorical\n","# import config\n","import sys  \n","sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL')\n","\n","# from Self_augment import RandAugment\n","\n","\n","def aug_image(img, label):\n","    '''\n","    图片的归一化\n","    :param img:\n","    :param label:\n","    :return:\n","    '''\n","\n","    return tf.cast(img, tf.float32) / 255.0, label\n","\n","\n","\n","# 制作有标签的数据集\n","def label_image(img_file, label):\n","    '''\n","    获取图片，对图片做水平翻转 随机剪裁等， label变为onehot\n","    :param img_file:\n","    :param label:\n","    :return:\n","    '''\n","    # 对图片的处理\n","    \n","      # img = cv2.imread(path, 0)\n","    img = tf.io.read_file(img_file)\n","    label = tf.io.read_file(label)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    # img = tf.image.random_flip_left_right(img)\n","    img = tf.image.resize(img, (IMG_SIZE , IMG_SIZE ))\n","\n","    #label = tf.io.read_file(label)\n","     # img = tf.image.grayscale_to_rgb(img, name=None)\n","    # img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE, 3] )\n","    label = tf.image.decode_jpeg(label, channels=3)\n","    label = tf.image.rgb_to_grayscale(label)\n","    # label = to_categorical(label, NUM_CLASSES)\n","    # label = tf.image.reshape(label, (IMG_SIZE, IMG_SIZE, 1, NUM_CLASSES))\n","    # label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","    label = tf.image.resize(label, (IMG_SIZE , IMG_SIZE ))\n","    label = tf.cast(label, DTYPE) / 255.0\n","    # label = tf.math.greater(label, 0).eval() = 1\n","    # img = tf.image.random_crop(img, (IMG_SIZE, IMG_SIZE, 3))\n","    img = tf.cast(img, DTYPE) / 255.0\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.0740, 0.0740, 0.0740], dtype=DTYPE), axis=0)\n","    img = (img-mean)/std\n","    if random.uniform(0, 1) > 0.5 :\n","      img = tf.image.flip_left_right(img)\n","      label = tf.image.flip_left_right(label)\n","    if random.uniform(0, 1) > 0.5 :\n","      img = tf.image.rot90(img) \n","      label = tf.image.rot90(label) \n","    if random.uniform(0, 1) > 0.5:\n","      img = tf.image.flip_up_down(img)\n","      label = tf.image.flip_up_down(label)\n","    label = tf.where(tf.math.greater(label, 0), x=1.0, y=0.0)\n","    \n","    # img, label = trainAug((img, label), seed)\n","    # label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","    # label = tf.cast(label, dtype=DTYPE)\n","    return {'images': img, 'labels': label}\n","\n","\n","# 制作无标签的数据集\n","def unlabel_image(img_file, label):\n","    '''\n","    处理无标签数据\n","    :param img_file:\n","    :param label:\n","    :return: 两张图片，一张经过轻微变换后的图片称为ori_image 一张经过较为剧烈变化后的图片，称为aug_images\n","    '''\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","    # aug_image = mask_label(img)\n","    aug_image = aug.distort(img)\n","    \n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, DTYPE) / 255.0\n","    ori_image = tf.cast(ori_image, DTYPE) / 255.0\n","\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.2153,0.2153,0.2153], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.2196, 0.2196, 0.2196], dtype=DTYPE), axis=0)\n","\n","    aug_image = (aug_image-mean)/std\n","    ori_image = (ori_image-mean)/std\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n","def merge_dataset(label_data, unlabel_data):\n","    return label_data['images'], label_data['labels'], unlabel_data['ori_images'], unlabel_data['aug_images']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"oY1C_uAlkYJG"},"source":["#@title Update student\n","\n","def my_update(model, model_):\n","    for i in range(len(model_)):\n","        model.weights[i] = model.weights[i].assign(\n","            model.weights[i]*(1-EMA)+model_[i]*EMA)\n","    model_ = deepcopy(model.weights)\n","    return model, model_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"xJLriYXX1oZU"},"source":["#@title Mean IOU \n","from keras import backend as K\n","def mean_iou(y_true, y_pred):\n","    yt0 = y_true[:,:,:,0]\n","    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n","    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n","    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n","    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n","    return iou"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szXtVViXw3eD","cellView":"form"},"source":["#@title TensorFlow IoU loss, not shown is the sigmoid non-linearity at output in lieu of ReLU\n","def loss(labels, logits, num_classes=3):\n","    with tf.name_scope('loss mine'):\n","        # logits = tf.to_float(tf.reshape(logits, [-1, num_classes]))\n","\n","        #CHANGE labels type to int, for sparse_softmax...\n","        # labels = tf.to_int64(tf.reshape(labels, [-1]))\n","\n","        # print ('shape of logits: %s' % str(logits.get_shape()))\n","        # print ('shape of labels: %s' % str(labels.get_shape()))\n","\n","        # cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='Cross_Entropy')\n","        # cross_entropy_mean = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n","        # tf.add_to_collection('losses', cross_entropy_mean)\n","\n","        # loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n","\n","        # logits=tf.reshape(logits, [-1])#(-1, num_classes)\n","        # labels=tf.reshape(labels, [-1])\n","        # cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits,labels,name='x_ent')\n","        # loss=tf.reduce_mean(cross_entropy, name='x_ent_mean')\n","        # train_op=tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(loss,global_step=global_step)\n","\n","        # For inference/visualization, prediction is argmax across output 'channels'\n","        # prediction = tf.argmax(tf.reshape(tf.nn.softmax(logits), tf.shape(vgg.up)), dimension=3)\n","        logits=tf.reshape(logits, [-1])\n","        trn_labels=tf.reshape(labels, [-1])\n","\n","        '''\n","        Eq. (1) The intersection part - tf.mul is element-wise,\n","        if logits were also binary then tf.reduce_sum would be like a bitcount here.\n","        '''\n","        inter=tf.reduce_sum(tf.multiply(logits,trn_labels))\n","\n","        '''\n","        Eq. (2) The union part - element-wise sum and multiplication, then vector sum\n","        '''\n","        #union=tf.reduce_sum(tf.sub(tf.add(logits,trn_labels),tf.mul(logits,trn_labels)))\n","        union=tf.reduce_sum(tf.subtract(tf.add(logits,trn_labels),tf.multiply(logits,trn_labels)))\n","        loss=tf.multiply(tf.constant(1.0, dtype=tf.float32),tf.divide(inter,union))\n","# Eq. (4)\n","\n","        # logits=tf.reshape(logits, [-1])\n","        # trn_labels=tf.reshape(labels, [-1])\n","        # inter=tf.reduce_sum(tf.multiply(logits,trn_labels))\n","\n","        # '''\n","        # Eq. (2) The union part - element-wise sum and multiplication, then vector sum\n","        # '''\n","        # union=tf.reduce_sum(tf.subtract(tf.add(logits,trn_labels),tf.multiply(logits,trn_labels)))\n","\n","        # # Eq. (4)\n","        # loss=tf.multiply(tf.constant(1.0, dtype=tf.float32),tf.divide(inter,union))\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"KcGYkGXLyUme"},"source":["#@title Default title text\n","  \n","from keras import layers as KL\n","from keras import backend as K\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","def soft_erode(img):\n","    \"\"\"[This function performs soft-erosion operation on a float32 image]\n","    Args:\n","        img ([float32]): [image to be soft eroded]\n","    Returns:\n","        [float32]: [the eroded image]\n","    \"\"\"\n","    p1 = -KL.MaxPool3D(pool_size=(3, 3, 1), strides=(1, 1, 1), padding='same', data_format=None)(-img)\n","    p2 = -KL.MaxPool3D(pool_size=(3, 1, 3), strides=(1, 1, 1), padding='same', data_format=None)(-img)\n","    p3 = -KL.MaxPool3D(pool_size=(1, 3, 3), strides=(1, 1, 1), padding='same', data_format=None)(-img)\n","    return tf.math.minimum(tf.math.minimum(p1, p2), p3)\n","\n","\n","def soft_dilate(img):\n","    \"\"\"[This function performs soft-dilation operation on a float32 image]\n","    Args:\n","        img ([float32]): [image to be soft dialated]\n","    Returns:\n","        [float32]: [the dialated image]\n","    \"\"\"\n","    return KL.MaxPool3D(pool_size=(3, 3, 3), strides=(1, 1, 1), padding='same', data_format=None)(img)\n","\n","\n","def soft_open(img):\n","    \"\"\"[This function performs soft-open operation on a float32 image]\n","    Args:\n","        img ([float32]): [image to be soft opened]\n","    Returns:\n","        [float32]: [image after soft-open]\n","    \"\"\"\n","    img = soft_erode(img)\n","    img = soft_dilate(img)\n","    return img\n","\n","\n","def soft_skel(img, iters):\n","    \"\"\"[summary]\n","    Args:\n","        img ([float32]): [description]\n","        iters ([int]): [description]\n","    Returns:\n","        [float32]: [description]\n","    \"\"\"\n","    img1 = soft_open(img)\n","    skel = tf.nn.relu(img-img1)\n","\n","    for j in range(iters):\n","        img = soft_erode(img)\n","        img1 = soft_open(img)\n","        delta = tf.nn.relu(img-img1)\n","        intersect = tf.math.multiply(skel, delta)\n","        skel += tf.nn.relu(delta-intersect)\n","    return skel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVhRXZxMydCs","cellView":"form"},"source":["#@title Default title text\n","from keras import layers as KL\n","from keras import backend as K\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","def soft_clDice_loss(iter_ = 50):\n","    \"\"\"[function to compute dice loss]\n","    Args:\n","        iter_ (int, optional): [skeletonization iteration]. Defaults to 50.\n","    \"\"\"\n","    def loss(y_true, y_pred):\n","        \"\"\"[function to compute dice loss]\n","        Args:\n","            y_true ([float32]): [ground truth image]\n","            y_pred ([float32]): [predicted image]\n","        Returns:\n","            [float32]: [loss value]\n","        \"\"\"\n","        smooth = 1.\n","        skel_pred = soft_skel(y_pred, iter_)\n","        skel_true = soft_skel(y_true, iter_)\n","        pres = (K.sum(tf.math.multiply(skel_pred, y_true)[:,1:,:,:,:])+smooth)/(K.sum(skel_pred[:,1:,:,:,:])+smooth)    \n","        rec = (K.sum(tf.math.multiply(skel_true, y_pred)[:,1:,:,:,:])+smooth)/(K.sum(skel_true[:,1:,:,:,:])+smooth)    \n","        cl_dice = 1.- 2.0*(pres*rec)/(pres+rec)\n","        return cl_dice\n","    return loss\n","\n","\n","def soft_dice(y_true, y_pred):\n","    \"\"\"[function to compute dice loss]\n","    Args:\n","        y_true ([float32]): [ground truth image]\n","        y_pred ([float32]): [predicted image]\n","    Returns:\n","        [float32]: [loss value]\n","    \"\"\"\n","    smooth = 1\n","    intersection = K.sum((y_true * y_pred)[:,1:,:,:,:])\n","    coeff = (2. *  intersection + smooth) / (K.sum(y_true[:,1:,:,:,:]) + K.sum(y_pred[:,1:,:,:,:]) + smooth)\n","    return (1. - coeff)\n","\n","\n","def soft_dice_cldice_loss(iters = 15, alpha=0.5):\n","    \"\"\"[function to compute dice+cldice loss]\n","    Args:\n","        iters (int, optional): [skeletonization iteration]. Defaults to 15.\n","        alpha (float, optional): [weight for the cldice component]. Defaults to 0.5.\n","    \"\"\"\n","    def loss(y_true, y_pred):\n","        \"\"\"[summary]\n","        Args:\n","            y_true ([float32]): [ground truth image]\n","            y_pred ([float32]): [predicted image]\n","        Returns:\n","            [float32]: [loss value]\n","        \"\"\"\n","        smooth = 1.\n","        skel_pred = soft_skel(y_pred, iters)\n","        skel_true = soft_skel(y_true, iters)\n","        pres = (K.sum(tf.math.multiply(skel_pred, y_true)[:,1:,:,:,:])+smooth)/(K.sum(skel_pred[:,1:,:,:,:])+smooth)    \n","        rec = (K.sum(tf.math.multiply(skel_true, y_pred)[:,1:,:,:,:])+smooth)/(K.sum(skel_true[:,1:,:,:,:])+smooth)    \n","        cl_dice = 1.- 2.0*(pres*rec)/(pres+rec)\n","        dice = soft_dice(y_true, y_pred)\n","        return (1.0-alpha)*dice+alpha*cl_dice\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"olnAfH8W0tCM","cellView":"form"},"source":["#@title Default title text\n","from keras.losses import binary_crossentropy\n","import keras.backend as K\n","import tensorflow as tf \n","\n","epsilon = 1e-5\n","smooth = 1\n","\n","def dsc(y_true, y_pred):\n","    smooth = 1.\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    return score\n","\n","def dice_loss(y_true, y_pred):\n","    loss = 1 - dsc(y_true, y_pred)\n","    return loss\n","\n","def bce_dice_loss(y_true, y_pred):\n","    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n","    return loss\n","\n","def confusion(y_true, y_pred):\n","    smooth=1\n","    y_pred_pos = K.clip(y_pred, 0, 1)\n","    y_pred_neg = 1 - y_pred_pos\n","    y_pos = K.clip(y_true, 0, 1)\n","    y_neg = 1 - y_pos\n","    tp = K.sum(y_pos * y_pred_pos)\n","    fp = K.sum(y_neg * y_pred_pos)\n","    fn = K.sum(y_pos * y_pred_neg) \n","    prec = (tp + smooth)/(tp+fp+smooth)\n","    recall = (tp+smooth)/(tp+fn+smooth)\n","    return prec, recall\n","\n","def tp(y_true, y_pred):\n","    smooth = 1\n","    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n","    y_pos = K.round(K.clip(y_true, 0, 1))\n","    tp = (K.sum(y_pos * y_pred_pos) + smooth)/ (K.sum(y_pos) + smooth) \n","    return tp \n","\n","def tn(y_true, y_pred):\n","    smooth = 1\n","    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n","    y_pred_neg = 1 - y_pred_pos\n","    y_pos = K.round(K.clip(y_true, 0, 1))\n","    y_neg = 1 - y_pos \n","    tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth )\n","    return tn \n","\n","def tversky(y_true, y_pred):\n","    # print(y_true.shape)\n","    # print(y_pred.shape)\n","    # y_pred = tf.reshape(y_pred, (-1, 3))#(-1, num_classes)\n","    \n","    y_true_pos = K.flatten(y_true)\n","    y_pred_pos = K.flatten(y_pred)\n","    true_pos = K.sum(y_true_pos * y_pred_pos)\n","    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n","    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n","    alpha = 0.7\n","    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n","\n","def tversky_loss(y_true, y_pred):\n","    return 1 - tversky(y_true,y_pred)\n","\n","def focal_tversky(y_true,y_pred):\n","    pt_1 = tversky(y_true, y_pred)\n","    gamma = 0.75\n","    return K.pow((1-pt_1), gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGxkXxME0s1x"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWkIGAzaaSko","cellView":"form"},"source":["#@title Dice coe\n","def dice_coe(output, target, loss_type='jaccard', axis=(1, 2, 3), smooth=1e-5):\n","    \"\"\"Soft dice (Sørensen or Jaccard) coefficient for comparing the similarity\n","    of two batch of data, usually be used for binary image segmentation\n","    i.e. labels are binary. The coefficient between 0 to 1, 1 means totally match.\n","\n","    Parameters\n","    -----------\n","    output : Tensor\n","        A distribution with shape: [batch_size, ....], (any dimensions).\n","    target : Tensor\n","        The target distribution, format the same with `output`.\n","    loss_type : str\n","        ``jaccard`` or ``sorensen``, default is ``jaccard``.\n","    axis : tuple of int\n","        All dimensions are reduced, default ``[1,2,3]``.\n","    smooth : float\n","        This small value will be added to the numerator and denominator.\n","            - If both output and target are empty, it makes sure dice is 1.\n","            - If either output or target are empty (all pixels are background), dice = ```smooth/(small_value + smooth)``, then if smooth is very small, dice close to 0 (even the image values lower than the threshold), so in this case, higher smooth can have a higher dice.\n","\n","    Examples\n","    ---------\n","    >>> import tensorlayer as tl\n","    >>> outputs = tl.act.pixel_wise_softmax(outputs)\n","    >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_)\n","\n","    References\n","    -----------\n","    - `Wiki-Dice <https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient>`__\n","\n","    \"\"\"\n","    inse = tf.reduce_sum(output * target, axis=axis)\n","    if loss_type == 'jaccard':\n","        l = tf.reduce_sum(output * output, axis=axis)\n","        r = tf.reduce_sum(target * target, axis=axis)\n","    elif loss_type == 'sorensen':\n","        l = tf.reduce_sum(output, axis=axis)\n","        r = tf.reduce_sum(target, axis=axis)\n","    else:\n","        raise Exception(\"Unknow loss_type\")\n","    # old axis=[0,1,2,3]\n","    # dice = 2 * (inse) / (l + r)\n","    # epsilon = 1e-5\n","    # dice = tf.clip_by_value(dice, 0, 1.0-epsilon) # if all empty, dice = 1\n","    # new haodong\n","    dice = (2. * inse + smooth) / (l + r + smooth)\n","    ##\n","    dice = tf.reduce_mean(dice, name='dice_coe')\n","    return dice"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBmsbqZEeCLi"},"source":["from tensorflow.keras import layers\n","smoothness = 1.0\n","\n","def dice_coefficient(y1, y2):\n"," y1 = layers.Flatten(y1)\n"," y2 = layers.Flatten(y2)\n"," return (2. * tf.math.reduce_sum(y1 * y2) + smoothness) / (tf.math.reduce_sum(y1) +tf.math.reduce_sum(y2) +\n","smoothness)\n","def dice_coefficient_loss(y1, y2):\n"," return -dice_coefficient(y1, y2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t_8_hbHECUAW"},"source":["def dice_coeff(y_true, y_pred):\n","    smooth = 1.\n","    # Flatten\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.reshape(y_pred, [-1])\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n","    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n","    return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DgINhlpNaxP"},"source":["def dice_loss(y_true, y_pred):\n","    loss = 1 - dice_coeff(y_true, y_pred)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"udrfi9JGB-bL"},"source":["def bce_dice_loss(y_true, y_pred):\n","    loss = tf.keras.losses.BinaryCrossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"8nsCaSe87M0f","cellView":"form","executionInfo":{"status":"error","timestamp":1628513434755,"user_tz":-60,"elapsed":934172,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"8b6014d3-5566-4906-ef0e-30258fe255d8"},"source":["#@title C MLPTrain\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import tensorflow_addons as tfa\n","# from WideResnet import WideResnet\n","from copy import deepcopy\n","import sklearn\n","from sklearn import preprocessing\n","import tensorflow_datasets as tfds\n","import random\n","\n","# import config\n","# from Model import Wrn28k\n","# from UdaCrossEntroy import UdaCrossEntroy\n","# from learningRate import LearningRate\n","# from Dataset import label_image\n","# from Dataset import unlabel_image\n","# from Dataset import merge_dataset\n","# from test import test\n","\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","    # 有标签的数据集 batch_size=BATCH_SIZE\n","    # df_label = pd.read_csv(LABEL_FILE_PATH)\n","    # le = preprocessing.LabelEncoder()\n","    # dfPS['label'] = le.fit_transform(dfPS.label.values)\n","\n","    u_file_paths = []\n","    u_labels = []\n","    for i in range(0, len(data)): \n","      path = root + data[\"fullPath\"][i]#2499\n","      path = path.replace('\\\\', '/')\n","      path = path.replace('.png', '.jpg')\n","      u_file_paths.append(path)\n","      if data[\"Status\"][i] ==\"Cancer\":\n","        u_labels.append(3)\n","      elif data[\"Status\"][i] == \"Normal\":\n","        u_labels.append(1)\n","      elif data[\"Status\"][i] == \"Benign\" :\n","        u_labels.append(2)\n","  \n","    train_dfPS = dfPS[:int(len(dfPS)*0.7)] \n","    test_dfPS = dfPS[-int(len(dfPS)*0.7):]\n","    t_file_paths = test_dfPS['image'].values\n","    t_labels = test_dfPS['mask'].values\n","    file_paths = train_dfPS['mask'].values\n","    labels = train_dfPS['mask'].values\n","\n","    \n","    ds_label_train = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n","    ds_label_train = ds_label_train \\\n","        .map(label_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=500) \\\n","        .batch(BATCH_SIZE, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 无标签的数据集 batch_size=BATCH_SIZE*UDA_DATA\n","    # df_unlabel = pd.read_csv(UNLABEL_FILE_PATH)\n","    # file_paths = df_unlabel['name'].values\n","    # labels = df_unlabel['label'].values\n","    ds_unlabel_train = tf.data.Dataset.from_tensor_slices((u_file_paths, u_labels))\n","    ds_unlabel_train = ds_unlabel_train \\\n","        .map(unlabel_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=500) \\\n","        .batch(BATCH_SIZE * UDA_DATA, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 将有标签数据和无标签数据整合成最终的数据形式\n","    ds_train = tf.data.Dataset.zip((ds_label_train, ds_unlabel_train))\n","    ds_train = ds_train.map(merge_dataset)\n","\n","    # 构建teacher模型\n","    if TEA_CONTINUE:\n","        print('continue teacher training')\n","        teacher = modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","        teacher.load_weights(TEA_LOAD_PATH)\n","        teacher.training = True\n","    else:\n","        # teacher = Wrn28k(num_inp_filters=3, k=2)\n","        teacher =  modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","\n","    # 构建student模型\n","    if STD_CONTINUE:\n","        print('continue student training')\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape,  pretrained=False)\n","        student.load_weights(STD_LOAD_PATH)\n","        student.training = True\n","        # student = tf.saved_model.load(STD_LOAD_PATH)\n","    else:\n","        # student = Wrn28k(num_inp_filters=3, k=2)\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","    student_ = student.weights\n","\n","    # 定义teacher的损失函数，损失函数之一为UdaCrossEntroy\n","    mpl_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","    # 定义student的损失函数， PS：teacher的损失函数为UdaCrossEntroy\n","    s_unlabel_loss = tf.losses.CategoricalCrossentropy(\n","        label_smoothing=LABEL_SMOOTHING,\n","        from_logits=True,\n","        reduction=tf.keras.losses.Reduction.NONE,\n","    )\n","\n","    s_label_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=keras.losses.Reduction.NONE,)\n","    #tf.losses.CategoricalCrossentropy(\n","    #     from_logits=True,\n","    #     label_smoothing=LABEL_SMOOTHING,\n","    #     reduction=keras.losses.Reduction.NONE,)\n","    # tf.keras.losses.SparseCategoricalCrossentropy(\n","    #     from_logits=True, \n","    #     reduction=keras.losses.Reduction.NONE,\n","    # )\n","\n","    # 定义teacher的学习率\n","    Tea_lr_fun = LearningRate(\n","        TEACHER_LR,\n","        TEACHER_LR_WARMUP_STEPS,\n","        TEACHER_NUM_WAIT_STEPS\n","    )\n","    # 定义student的学习率\n","    Std_lr_fun = LearningRate(\n","        STUDENT_LR,\n","        STUDENT_LR_WARMUP_STEPS,\n","        STUDENT_LR_WAIT_STEPS\n","    )\n","\n","    global_step = 62*CONTINUE_EPOCH\n","    print(f'start training from global step {global_step}......')\n","    TBacc = 0.78\n","    Tacc = 0\n","    SBacc = 0.31\n","    Sacc = 0\n","    epochs = MAX_EPOCHS - CONTINUE_EPOCH\n","    for epoch in range(epochs):\n","        TLOSS = 0\n","        TLOSS_1 = 0\n","        TLOSS_2 = 0\n","        TLOSS_3 = 0\n","        SLOSS = 0\n","        for batch_idx, (l_images, l_labels, ori_images, aug_images) in enumerate(ds_train):\n","            global_step += 1\n","            all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [15, 32, 32, 3]\n","            u_aug_and_l_images = tf.concat([aug_images, l_images], axis=0)\n","            # step1：经过teacher，得到输出\n","            with tf.GradientTape() as t_tape:\n","                output = teacher(all_images)  # shape=[15, 10]\n","                logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, global_step)\n","            # step2：1st call student -----------------------------\n","            with tf.GradientTape() as s_tape:\n","                logits['s_on_aug_and_l'] = student(u_aug_and_l_images)  # shape=[8, 10]\n","                logits['s_on_u'], logits['s_on_l_old'] = tf.split(\n","                    logits['s_on_aug_and_l'],\n","                    [aug_images.shape[0], l_images.shape[0]],\n","                    axis=0\n","                )\n","                cross_entroy['s_on_u'] = s_unlabel_loss(\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], -1)),\n","                    y_pred=logits['s_on_u']\n","                )\n","                # 计算损失函数\n","                cross_entroy['s_on_u'] = tf.reduce_sum(cross_entroy['s_on_u']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=tf.float32)\n","                SLOSS += cross_entroy['s_on_u']\n","                # for taylor\n","                cross_entroy['s_on_l_old'] = s_label_loss(\n","                    labels['l'],\n","                    logits['s_on_l_old']\n","                )\n","\n","                cross_entroy['s_on_l_old'] = tf.reduce_sum(cross_entroy['s_on_l_old']) / \\\n","                                             tf.convert_to_tensor(BATCH_SIZE, dtype=tf.float32)\n","            # 反向传播，更新student的参数-------\n","            StudentLR = Std_lr_fun.__call__(global_step=global_step)\n","            StdOptim = keras.optimizers.SGD(\n","                learning_rate=StudentLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # StdOptim = keras.optimizers.Adam(learning_rate=StudentLR)\n","            GStud_unlabel = s_tape.gradient(cross_entroy['s_on_u'], student.trainable_variables)\n","            GStud_unlabel, _ = tf.clip_by_global_norm(GStud_unlabel, GRAD_BOUND)\n","            StdOptim.apply_gradients(zip(GStud_unlabel, student.trainable_variables))\n","            # 如何更新参数\n","            student, student_ = my_update(student, student_)\n","\n","            # step3: 2nd call student ------------------------------\n","            logits['s_on_l_new'] = student(l_images)\n","            cross_entroy['s_on_l_new'] = s_label_loss(\n","               labels['l'],\n","               logits['s_on_l_new']\n","            )\n","            cross_entroy['s_on_l_new'] = tf.reduce_sum(cross_entroy['s_on_l_new']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE, dtype=DTYPE)\n","            dot_product = cross_entroy['s_on_l_new'] - cross_entroy['s_on_l_old']\n","            limit = 3.0**(0.5)\n","            moving_dot_product = tf.random_uniform_initializer(minval=-limit, maxval=limit)(shape=dot_product.shape)\n","            moving_dot_product = tf.Variable(initial_value=moving_dot_product, trainable=False, dtype=DTYPE)\n","            moving_dot_product_update = moving_dot_product.assign_sub(0.01 * (moving_dot_product - dot_product))\n","            dot_product = dot_product - moving_dot_product\n","            dot_product = tf.stop_gradient(dot_product)\n","            # step4: 求teacher的损失函数\n","            with t_tape:\n","                # label = tf.math.argmax(tf.nn.softmax(logits['aug'], axis=-1), axis=-1)\n","                # label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","                cross_entroy['mpl'] = mpl_loss(\n","                    # y_true=tf.stop_gradient(label),\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], axis=-1)),\n","                    y_pred=logits['aug']\n","                )  # 恒正\n","                cross_entroy['mpl'] = tf.reduce_sum(cross_entroy['mpl']) / \\\n","                                      tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=DTYPE)\n","                uda_weight = UDA_WEIGHT * tf.math.minimum(\n","                    1., tf.cast(global_step, DTYPE) / float(UDA_STEPS)\n","                )\n","                # if StudentLR == 0:\n","                #     dot_product = 0\n","                teacher_loss = cross_entroy['u'] * uda_weight + \\\n","                               cross_entroy['l'] + \\\n","                               cross_entroy['mpl'] * dot_product\n","\n","                TLOSS += teacher_loss\n","                TLOSS_1 += (cross_entroy['u'] * uda_weight)\n","                TLOSS_2 += cross_entroy['l']\n","                TLOSS_3 += cross_entroy['mpl'] * dot_product\n","            # 反向传播，更新teacher的参数-------\n","            TeacherLR = Tea_lr_fun.__call__(global_step=global_step)\n","            TeaOptim = keras.optimizers.SGD(\n","                learning_rate=TeacherLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # TeaOptim = keras.optimizers.Adam(learning_rate=TeacherLR)\n","            GTea = t_tape.gradient(teacher_loss, teacher.trainable_variables)\n","            GTea, _ = tf.clip_by_global_norm(GTea, GRAD_BOUND)\n","            TeaOptim.apply_gradients(zip(GTea, teacher.trainable_variables))\n","\n","            if (batch_idx + 1) % LOG_EVERY == 0:\n","                TLOSS = TLOSS / LOG_EVERY\n","                TLOSS_1 = TLOSS_1 / LOG_EVERY\n","                TLOSS_2 = TLOSS_2 / LOG_EVERY\n","                TLOSS_3 = TLOSS_3 / LOG_EVERY\n","                SLOSS = SLOSS / LOG_EVERY\n","                print(f'global: %4d' % global_step + ',[epoch:%4d/' % (epoch+CONTINUE_EPOCH) + 'EPOCH: %4d] \\t' % epochs\n","                      + '[U:%.4f' % (TLOSS_1) + ', L:%.4f' % (TLOSS_2) + ', M:%.4f' % (\n","                          TLOSS_3) + ']' + '[TLoss: %.4f]' % TLOSS + '/[SLoss: %.4f]' % SLOSS\n","                      + '\\t[TLR: %.6f' % TeacherLR + ']/[SLR: %.6f]' % StudentLR)\n","                TLOSS = 0\n","                TLOSS_1 = 0\n","                TLOSS_2 = 0\n","                TLOSS_3 = 0\n","                SLOSS = 0\n","        # 测试teacher在test上的acc\n","        if epoch % 5 == 0:\n","            Tacc = test(teacher, t_file_paths, t_labels)\n","            print(f'testing teacher model ... acc: {Tacc}')\n","        # 测试student在test上的acc，当student开始训练的时候\n","        if (StudentLR > 0) and (epoch % 5 == 0):\n","            Sacc = test(student, t_file_paths, t_labels)\n","            print(f'testing ... acc: {Sacc}')\n","        # 保存weights\n","        if Tacc > TBacc:\n","            Tsave_path = TEA_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            teacher.save_weights(Tsave_path)\n","            # tf.saved_model.save(teacher, Tsave_path)\n","            TBacc = Tacc\n","            print(f'saving for TBacc {TBacc}, Tpath:{Tsave_path}')\n","        if Sacc > SBacc:\n","            Ssave_path = STD_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            student.save_weights(Ssave_path)\n","            SBacc = Sacc\n","            print(f'saving for SBacc {SBacc}, Spath:{Ssave_path}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["start training from global step 54870......\n","global: 54890,[epoch: 885/EPOCH: 1035] \t[U:nan, L:nan, M:nan][TLoss: nan]/[SLoss: nan]\t[TLR: 0.000475]/[SLR: 0.000476]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-40e75bb182e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m             )\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# TeaOptim = keras.optimizers.Adam(learning_rate=TeacherLR)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mGTea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mGTea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGTea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRAD_BOUND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mTeaOptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGTea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    588\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    591\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    592\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1245\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"DlUiI1ivISYz"},"source":["random.uniform(0, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGTtEhVRRXNF"},"source":["im = cv2.imread(dfSCM2['mask'][0])\n","im.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pL_L7KWGRhpP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GZgbbA83UaC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbIaUq-RBn91"},"source":["test(student, t_file_paths, t_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PnHNKSKvtrrp"},"source":["student.compile(optimizer='rmsprop', loss=dice_coefficient_loss, metrics=dice_coefficient)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_WEm8qNVXIc"},"source":["ds_label_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ziaPA96lF1D"},"source":["student.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcHP84E8rLR7"},"source":["# New section"]},{"cell_type":"code","metadata":{"id":"klSuHkmOrOfc"},"source":[""],"execution_count":null,"outputs":[]}]}
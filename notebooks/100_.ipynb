{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"100%.ipynb","provenance":[{"file_id":"1WVv6ONxZPFOolt8plebjKRoahWp3Fu04","timestamp":1627995752898},{"file_id":"1CBDBm79WjbAgXH9kJRI04fWV3dcGjkhT","timestamp":1627750778184},{"file_id":"1yhEeWMmcKO-sOIAo3tTeAQiOdbPwQPgu","timestamp":1627719755769}],"collapsed_sections":["_pQCOmISAQBu","dExRVQI8aLbl","m2nMbS40gHlM","WZ29zXFqh5sm"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVsrGkj4Zn2L","executionInfo":{"status":"ok","timestamp":1630036785241,"user_tz":-60,"elapsed":800,"user":{"displayName":"Antonio Franco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpZWpPk6ug7_9xt90pVX8z1iULSGekWi2cTqST=s64","userId":"09756366898940169615"}},"outputId":"fd88c2f3-a907-4ec9-f3e1-09e345b2e278"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jEOxHu9xCjK","executionInfo":{"status":"ok","timestamp":1630036793688,"user_tz":-60,"elapsed":4449,"user":{"displayName":"Antonio Franco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpZWpPk6ug7_9xt90pVX8z1iULSGekWi2cTqST=s64","userId":"09756366898940169615"}},"outputId":"bf4e8806-9228-4dab-bdb4-4035f62ee7fa"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 39.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 15.0 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.14.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z_AwPGsuiVnc"},"source":["try:\n","  %tensorflow_version 2.x\n","  import tensorflow as tf\n","  device_name = tf.test.gpu_device_name()\n","  if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","  print('Found GPU at: {}'.format(device_name))\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dExRVQI8aLbl"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_X6Tj9TaBOF","executionInfo":{"status":"ok","timestamp":1630036820242,"user_tz":-60,"elapsed":2766,"user":{"displayName":"Antonio Franco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpZWpPk6ug7_9xt90pVX8z1iULSGekWi2cTqST=s64","userId":"09756366898940169615"}},"outputId":"67eea0e4-bdd1-4e7f-fec4-a5a2faa5c967"},"source":["#@title Libraries\n","%pylab inline\n","import imgaug as ia\n","import imgaug.augmenters as iaa\n","import imutils\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import pylab as pylab\n","import matplotlib.image as mpimg\n","from PIL import Image as im\n","# import segmentation_models_pytorch as smp\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from scipy import ndimage, misc\n","from skimage.filters import threshold_otsu\n","from skimage.segmentation import clear_border\n","from skimage.measure import label, regionprops\n","from skimage.morphology import closing, square\n","from skimage.color import label2rgb\n","import matplotlib.patches as mpatches\n","from scipy.misc import face\n","from scipy.signal.signaltools import wiener\n","import sys\n","import numpy as np\n","import skimage.color\n","import skimage.filters\n","import skimage.io\n","import skimage.viewer\n","from skimage import feature, io, color, filters\n","from skimage.transform import hough_line, hough_line_peaks\n","from skimage.feature import canny\n","from skimage.filters import sobel\n","from skimage.draw import polygon\n","from skimage import exposure\n","from skimage.transform import resize\n","from PIL import Image\n","import scipy.ndimage as snd\n","import tensorflow as tf\n","import re\n","import numpy as np\n","from matplotlib import pyplot as plt\n","#from meta-pseudo-labels.\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Viewer requires Qt\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Bf0ES9ZmG0DP"},"source":["# Mount file syste,"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2yW4Ik2Gy22","executionInfo":{"status":"ok","timestamp":1630036903056,"user_tz":-60,"elapsed":75600,"user":{"displayName":"Antonio Franco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpZWpPk6ug7_9xt90pVX8z1iULSGekWi2cTqST=s64","userId":"09756366898940169615"}},"outputId":"14f61aea-de2e-4992-bcdc-2b149f39fb18"},"source":["#@title Driver mount\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True, timeout_ms=12000000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"au_JSmtDUCFG"},"source":["# ECX"]},{"cell_type":"code","metadata":{"id":"77BWHD4UUFlN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSXtKMN8qV72","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629995692111,"user_tz":-60,"elapsed":329,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"d59669c5-e6a7-40ce-8085-7a1a686b6eb8"},"source":["!curl -s https://course.fast.ai/setup/colab | bash"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bash: line 1: syntax error near unexpected token `newline'\n","bash: line 1: `<!DOCTYPE html>'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HBcZOg6aqgeJ"},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbqdaSr4sP7f"},"source":["from fastai import *\n","from fastai.basics import *\n","from fastai.vision import *\n","from fastai.metrics import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OhTr6vcsVku"},"source":["from fastai.callbacks.hooks import *\n","# from fastai.utils.mem import 7*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFI1ZrHTr0Yh","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1629995785526,"user_tz":-60,"elapsed":28035,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"8a9f0265-9faa-44c4-e141-28a2a2885450"},"source":["path=untar_data(URLs.FLOWERS)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2M1hLt2UgnI","executionInfo":{"status":"ok","timestamp":1629995805009,"user_tz":-60,"elapsed":335,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"ffc4ad4c-23b1-4f8f-a52c-0749d0630116"},"source":["path.ls()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PosixPath('/root/.fastai/data/oxford-102-flowers/test.txt'),\n"," PosixPath('/root/.fastai/data/oxford-102-flowers/train.txt'),\n"," PosixPath('/root/.fastai/data/oxford-102-flowers/valid.txt'),\n"," PosixPath('/root/.fastai/data/oxford-102-flowers/jpg')]"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"Jm4nL88XUkv3"},"source":["file = Path(path).glob('*test.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sh7lCCk0XxdP","executionInfo":{"status":"ok","timestamp":1629996726286,"user_tz":-60,"elapsed":405,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"42c9743f-ab88-4718-91f9-fbb04f6eb485"},"source":["file"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object Path.glob at 0x7f2330e9ca50>"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"ekP7sFbrZVJo"},"source":["\n","dfT = pd.DataFrame()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ijdxr6ebbN5c","executionInfo":{"status":"ok","timestamp":1629999444433,"user_tz":-60,"elapsed":350,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"0061ecb3-9416-4e02-f0cc-0bfe00430795"},"source":["pre[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'34'"]},"metadata":{},"execution_count":128}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"vVWUZweiZLB4","executionInfo":{"status":"ok","timestamp":1629997890927,"user_tz":-60,"elapsed":4,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"b8695400-8913-4c74-898c-5eaec9e4a458"},"source":["dfm =pd.read_csv(filename, sep=' ')\n","dfT = dfT.append(dfm)\n","pre = dfT.columns\n","dfT.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>jpg/image_06977.jpg</th>\n","      <th>34</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>jpg/image_00800.jpg</td>\n","      <td>80</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>jpg/image_05038.jpg</td>\n","      <td>58</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>jpg/image_06759.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>jpg/image_01133.jpg</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>jpg/image_07982.jpg</td>\n","      <td>100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   jpg/image_06977.jpg   34\n","0  jpg/image_00800.jpg   80\n","1  jpg/image_05038.jpg   58\n","2  jpg/image_06759.jpg    0\n","3  jpg/image_01133.jpg   45\n","4  jpg/image_07982.jpg  100"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","metadata":{"id":"6D7k2y2UboUu","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"error","timestamp":1630036915841,"user_tz":-60,"elapsed":475,"user":{"displayName":"Antonio Franco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpZWpPk6ug7_9xt90pVX8z1iULSGekWi2cTqST=s64","userId":"09756366898940169615"}},"outputId":"142f8ef9-8d4a-4bc6-a242-a4d5c4ddb65d"},"source":["dfT.rename(columns={'jpg/image_06977.jpg': 'image', '34':'label'}, inplace=True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-04e6d8a05658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'jpg/image_06977.jpg'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'34'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dfT' is not defined"]}]},{"cell_type":"code","metadata":{"id":"bnt0RzR6cr-W"},"source":["dfT.loc[len(dfT)] = ['jpg/image_06977.jpg', '34']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"bL4JJJfWec7S","executionInfo":{"status":"ok","timestamp":1629999697782,"user_tz":-60,"elapsed":318,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"4c66853f-e7a9-44ff-e95a-8f0b909fd704"},"source":["dfT.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>jpg/image_00800.jpg</td>\n","      <td>80</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>jpg/image_05038.jpg</td>\n","      <td>58</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>jpg/image_06759.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>jpg/image_01133.jpg</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>jpg/image_07982.jpg</td>\n","      <td>100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 image label\n","0  jpg/image_00800.jpg    80\n","1  jpg/image_05038.jpg    58\n","2  jpg/image_06759.jpg     0\n","3  jpg/image_01133.jpg    45\n","4  jpg/image_07982.jpg   100"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"V8JbiqfMb5lY","executionInfo":{"status":"ok","timestamp":1629999954998,"user_tz":-60,"elapsed":316,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"8b53ab37-a72a-4ab1-c979-ed8f639f55ea"},"source":["file = Path(path).glob('*train.txt')\n","dfTr = pd.DataFrame()\n","for filename in file:\n","  dfm = pd.read_csv(filename, sep=' ')\n","dfTr = dfTr.append(dfm)\n","pre = dfTr.columns\n","dfTr.rename(columns={pre[0]: 'image', pre[1]:'label'}, inplace=True)\n","dfTr.loc[len(dfT)] = [pre[0], pre[1]]\n","dfTr.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>jpg/image_03860.jpg</th>\n","      <th>16</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>jpg/image_06092.jpg</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>jpg/image_02400.jpg</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>jpg/image_02852.jpg</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>jpg/image_07710.jpg</td>\n","      <td>96</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>jpg/image_07191.jpg</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   jpg/image_03860.jpg  16\n","0  jpg/image_06092.jpg  13\n","1  jpg/image_02400.jpg  42\n","2  jpg/image_02852.jpg  55\n","3  jpg/image_07710.jpg  96\n","4  jpg/image_07191.jpg   5"]},"metadata":{},"execution_count":137}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FdurAxGXkh1v","executionInfo":{"status":"ok","timestamp":1630000007230,"user_tz":-60,"elapsed":301,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"69d5dc09-4dce-4a95-81a7-bdcfa0ea0458"},"source":["len(dfTr)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1019"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"Hhx-JDCZXN2f","executionInfo":{"status":"error","timestamp":1629997016368,"user_tz":-60,"elapsed":3,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"37451546-6af8-4499-86cc-f40e37369865"},"source":["dfs = []\n","for filename in file:\n","    dfs.append(pd.read_csv(filename, sep='\\t'))\n","\n","test = pd.concat(dfs)\n","dfs1.head()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-77-25bcae8767dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 )\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid"]}]},{"cell_type":"code","metadata":{"id":"Bjuzr9hqYUPm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"NWjPOHl8XDcG","executionInfo":{"status":"error","timestamp":1629996502399,"user_tz":-60,"elapsed":322,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"3ea20669-6054-4893-b392-403f0578a54e"},"source":["someDf = pd.read_csv(h)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-40067e6f0efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msomeDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'generator'>"]}]},{"cell_type":"markdown","metadata":{"id":"w7kfgMnUadk0"},"source":["# Import data"]},{"cell_type":"code","metadata":{"id":"X30cgZKOacpN"},"source":["#@title Default title text\n","\n","# import os\n","# input_dir = \"/content/drive/MyDrive/Thesis/MINI-DDSM-Complete-JPEG-8/Data.xlsx\"\n","root = '\\\\content\\\\drive\\\\MyDrive\\\\Thesis\\\\MINI-DDSM-Complete-JPEG-8\\\\'\n","# dfAll = pd.read_excel(input_dir)\n","# dfAll.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5mfjVVbJXZ2B"},"source":["# Selecting and Sampling"]},{"cell_type":"code","metadata":{"id":"mToRCE_3ACK8"},"source":["data = pd.read_csv('/content/drive/MyDrive/Thesis/pos/data.csv')\n","# dfc_ = pd.read_csv('/content/drive/MyDrive/Thesis/pos/dataS.csv')\n","dfPS = pd.read_csv('/content/drive/MyDrive/Thesis/pos/PSdata.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"oi68VREkUhxU","executionInfo":{"status":"ok","timestamp":1628997088403,"user_tz":-60,"elapsed":1313,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"f92fa100-0cf5-4e4e-c56f-371a07ef7b8c"},"source":["cv = cv2.imread(dfPS['mask'][0])\n","plt.imshow(cv)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fd279251590>"]},"metadata":{"tags":[]},"execution_count":12},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"ChyVx_LCwEBg"},"source":["dfSCM2 = pd.DataFrame(columns=['image', 'mask', 'label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"5DEVOhArNWlY"},"source":["#@title check_path\n","\n","def check_path(path):\n","  path = path.replace('\\\\', '/')\n","  try:\n","    # mask_ = imread(path,0)    \n","    im1 = Image.open(path)\n","    #rgb_im = mask_.convert('RGB')\n","    im1.save(path.replace('.png', '.jpg'))\n","\n","    return path.replace('.png', '.jpg')\n","  except :\n","  \n","    try:\n","      # mask_ = imread(path,0) \n","      path = path.replace('MASK', 'Mask')\n","      path = path.replace('.png', '.jpg')   \n","      im1 = Image.open(path)\n","\n","      return path\n","    except :\n","      try:\n","        # mask_ = imread(path,0) \n","        path = path.replace('Mask', 'MASK')\n","        path = path.replace('.png', '.jpg')   \n","        im1 = Image.open(path)\n","\n","        return path\n","      except :\n","        try:\n","          path = path.replace('.png', '.jpg')\n","          return path\n","        except:\n","          pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQsRNXnLnRR5","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"error","timestamp":1628997129337,"user_tz":-60,"elapsed":40941,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"cabeb8a5-7c51-4d01-f583-241907d9c08b"},"source":["#@title Default title text\n","# for i in range(0, len(data)): \n","i =0 \n","for i in range(2538, len(data)):\n","    path = root + data[\"fullPath\"][i]#2499 2\n","    path = path.replace('\\\\', '/')\n","    path = path.replace('.png', '.jpg')\n","    img = cv2.imread(path)\n","    Main_mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.uint8)\n","    masks = []\n","    if data['Tumour_Contour'][i] != '-':\n","      masks.append(check_path(root + data['Tumour_Contour'][i]))\n","    if data['Tumour_Contour2'][i] != '-':\n","      masks.append(check_path(root + data['Tumour_Contour2'][i]))\n","    if pd.isnull(data[\"Tumour_Contour3\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour3'][i]))\n","    if pd.isnull(data[\"Tumour_Contour4\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour4'][i]))\n","    if pd.isnull(data[\"Tumour_Contour5\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour5'][i]))\n","    if pd.isnull(data[\"Tumour_Contour6\"][i]) == False:\n","      masks.append(check_path(root + data['Tumour_Contour6'][i]))\n","    for mask in masks:\n","      ini_img = img.copy()\n","      mask_ = imread(mask)\n","      mask_ = cv2.resize(mask_, (ini_img.shape[0], ini_img.shape[1]))\n","      mask_[np.where(mask_ !=0)] = 255\n","      ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","      contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE) \n","      for cnt in contours:      \n","        (x,y,w,h) = cv2.boundingRect(cnt)\n","        cv2.drawContours(mask_, [cnt], 0,(255, 0, 0), 2)\n","      mask_ = np.expand_dims(resize(mask_, (img.shape[0], img.shape[1]), mode='constant',  \n","                                  preserve_range=True), axis=-1)\n","      Main_mask = np.maximum(Main_mask, mask_)\n","      Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 255\n","    if data['Status'][i] == 'Cancer':\n","      Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 2\n","    if data['Status'][i] == 'Benign':\n","      Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 1\n","\n","    # plt.imshow(np.squeeze(Main_mask))\n","    cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i) + \".jpg\", np.squeeze(Main_mask))\n","    dfSCM2.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i) + \".jpg\", data['Status'][i]]\n","    # i = i + 1"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-c04fd9bf6c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       mask_ = np.expand_dims(resize(mask_, (img.shape[0], img.shape[1]), mode='constant',  \n\u001b[0;32m---> 34\u001b[0;31m                                   preserve_range=True), axis=-1)\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0mMain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mMain_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMain_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    164\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m    165\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                    preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# n-dimensional interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    854\u001b[0m                 warped = _warp_fast[ctype](image, matrix,\n\u001b[1;32m    855\u001b[0m                                            \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                                            order=order, mode=mode, cval=cval)\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mskimage/transform/_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"Convert the input to an array.\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ZisEQhPyDJNd"},"source":["dfSCM2 = pd.read_csv('/content/drive/MyDrive/Thesis/pos/SCM2data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYdEH080HFEp"},"source":["dfSCML3 = pd.DataFrame(columns=['image', 'mask', 'label'])\n","dfSCML2 = pd.DataFrame(columns=['image', 'mask', 'label'])\n","dfSML2 = pd.DataFrame(columns=['image', 'mask', 'label'])\n","dfCA = pd.DataFrame(columns=['image', 'label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6CNbMBE0Vr8"},"source":["dfPS['mask'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZQGznZuI2tP"},"source":["from tensorflow.keras.utils import to_categorical\n","for i in range(0, len(dfSCM2)):\n","\n","    # mask = cv2.imread(dfSCM2['mask'][i])\n","    mask_ =cv2.imread('/content/drive/MyDrive/Thesis/pos/yS/'+str(i)+'.jpg')\n","\n","    # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n","    mask_ = cv2.cvtColor(mask_, cv2.COLOR_BGR2GRAY)\n","\n","    img = cv2.imread(dfSCM2['image'][i])\n","    ini_img = img.copy()\n","\n","    # imgc3 = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","    # imgc3[:,:,0][np.where(mask==0)] = 1\n","    # imgc3[:,:,1][np.where(mask==1)] = 1 \n","    # imgc3[:,:,2][np.where(mask==2)] = 1  \n","\n","    # imgc2 = np.zeros((mask.shape[0], mask.shape[1], 2), dtype=np.uint8)\n","    # imgc2[:,:,0][np.where(mask==0)] = 1\n","    # imgc2[:,:,1][np.where(mask!=0)] = 1 \n","    # # img3[:,:,2][np.where(mask==2)] = 1 \n","\n","    # img2 = np.zeros((mask_.shape[0], mask_.shape[1], 2), dtype=np.uint8)\n","    # img2[:,:,0][np.where(mask_==0)] = 1\n","    # img2[:,:,1][np.where(mask_!=0)] = 1 \n","    # # img2[:,:,2][np.where(mask==2)] = 1  \n","\n","    # mask_ = cv2.imread(dfS['mask'][i], 0)\n","    mask_ = cv2.resize(mask_, (ini_img.shape[0], ini_img.shape[1]))\n","    ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","    contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n","\n","    if dfSCM2[\"label\"][i] != 'Normal':\n","      for cnt in contours:      \n","        (x,y,w,h) = cv2.boundingRect(cnt)\n","        # cv2.drawContours(mask_, [cnt], 0,(255, 0, 0),-1)\n","      # ini_img[np.where(mask_==0)] = 0\n","      cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n","      try:\n","        cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xCA/\" + str(len(dfCA)) + \".jpg\", ini_img[y:y+h,x:x+w])\n","        dfCA.loc[len(dfCA)] = [\"/content/drive/MyDrive/Thesis/pos/xCA/\" + str(len(dfCA)) + \".jpg\", dfSCM2['label'][i]]\n","      except:\n","        pass\n","        # gray = np.zeros((mask_.shape[0], mask_.shape[1], 1), dtype=np.uint8)\n","        # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xCA/\" + str(len(dfCA)) + \".jpg\", gray)\n","     \n","\n","    # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/cm3/\" + str(i) + \".jpg\", imgc3)\n","    # dfSCML3.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/cm3/\" + str(i) + \".jpg\", dfSCM2['label'][i]]\n","\n","    # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/cm2/\" + str(i) + \".jpg\", imgc2)\n","    # dfSCML2.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/cm2/\" + str(i) + \".jpg\", dfSCM2['label'][i]]\n","\n","    # cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/m2/\" + str(i) + \".jpg\", img2)\n","    # dfSML2.loc[i] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/m2/\" + str(i) + \".jpg\", dfSCM2['label'][i]]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kAuxt6-Fsrw"},"source":["dfc = pd.read_csv('/content/drive/MyDrive/Thesis/pos/Cdata.csv')\n","dfc_ = pd.read_csv('/content/drive/MyDrive/Thesis/pos/dataC.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nY3RdD-j1TIq"},"source":["# dfSCML3.to_csv('/content/drive/MyDrive/Thesis/pos/SCML3data.csv', index = False)\n","# dfSCML2.to_csv('/content/drive/MyDrive/Thesis/pos/SCML2data.csv', index = False)\n","# dfSML2.to_csv('/content/drive/MyDrive/Thesis/pos/SML2data.csv', index = False)\n","dfCA.to_csv('/content/drive/MyDrive/Thesis/pos/CAdata.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axMpjivOB5HG"},"source":["dfUno = pd.read_csv('/content/drive/MyDrive/Thesis/pos/UData.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVDQRt-b_WQE"},"source":["from os import listdir\n","from PIL import Image as PImage\n","\n","imagesList = listdir(path)\n","loadedImages = []\n","for image in imagesList:\n","    img = PImage.open(path + image)\n","    loadedImages.append(img)\n","\n","\n","\n","path = \"/path/to/your/images/\"\n","\n","# your images in an array\n","imgs = loadImages(path)\n","\n","for img in imgs:\n","    # you can show every image\n","    img.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZ0gNq_M_rQx"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOt5pCcpPyJE"},"source":["im = cv2.imread(dfCA['image'][6])\n","plt.imshow(im)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5M_RSfZiwUG"},"source":["len(dfc['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyIg40pn4Fno"},"source":["img = cv2.imread(dfSCM2['image'][2538])\n","plt.imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YKERuF4d2kOm"},"source":["dfSCM2.to_csv('/content/drive/MyDrive/Thesis/pos/SCML3data.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDwYI0chx14-"},"source":["unique(np.squeeze(Main_mask))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2n-gVjJwhoZg"},"source":["im = cv2.imread(dfc_['image'][0])\n","plt.imshow(im)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2nMbS40gHlM"},"source":["# EfficientUnet"]},{"cell_type":"markdown","metadata":{"id":"nIJUmq6zgXPz"},"source":["## Efficientnet"]},{"cell_type":"code","metadata":{"cellView":"form","id":"q8dhNhDdgW58"},"source":["#@title Efficientnet\n","from keras import models, layers\n","# import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from tensorflow.keras.utils import get_file\n","# from utils import *\n","\n","__all__ = ['get_model_by_name', 'get_efficientnet_b0_encoder', 'get_efficientnet_b1_encoder',\n","           'get_efficientnet_b2_encoder', 'get_efficientnet_b3_encoder', 'get_efficientnet_b4_encoder',\n","           'get_efficientnet_b5_encoder', 'get_efficientnet_b6_encoder', 'get_efficientnet_b7_encoder']\n","\n","\n","def _efficientnet(input_shape, blocks_args_list, global_params):\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","\n","    # Stem part\n","    model_input = layers.Input(shape=input_shape)\n","    x = layers.Conv2D(\n","        filters=round_filters(32, global_params),\n","        kernel_size=[3, 3],\n","        strides=[2, 2],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='stem_conv2d'\n","    )(model_input)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='stem_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='stem_swish')(x)\n","\n","    # Blocks part\n","    idx = 0\n","    drop_rate = global_params.drop_connect_rate\n","    n_blocks = sum([blocks_args.num_repeat for blocks_args in blocks_args_list])\n","    drop_rate_dx = drop_rate / n_blocks\n","\n","    for blocks_args in blocks_args_list:\n","        assert blocks_args.num_repeat > 0\n","        # Update block input and output filters based on depth multiplier.\n","        blocks_args = blocks_args._replace(\n","            input_filters=round_filters(blocks_args.input_filters, global_params),\n","            output_filters=round_filters(blocks_args.output_filters, global_params),\n","            num_repeat=round_repeats(blocks_args.num_repeat, global_params)\n","        )\n","\n","        # The first block needs to take care of stride and filter size increase.\n","        x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","        idx += 1\n","\n","        if blocks_args.num_repeat > 1:\n","            blocks_args = blocks_args._replace(input_filters=blocks_args.output_filters, strides=[1, 1])\n","\n","        for _ in range(blocks_args.num_repeat - 1):\n","            x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","            idx += 1\n","\n","    # Head part\n","    x = layers.Conv2D(\n","        filters=round_filters(1280, global_params),\n","        kernel_size=[1, 1],\n","        strides=[1, 1],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='head_conv2d'\n","    )(x)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='head_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='head_swish')(x)\n","\n","    x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n","\n","    if global_params.dropout_rate > 0:\n","        x = layers.Dropout(global_params.dropout_rate)(x)\n","\n","    x = layers.Dense(\n","        global_params.num_classes,\n","        kernel_initializer=dense_kernel_initializer,\n","        activation='softmax',\n","        name='head_dense'\n","    )(x)\n","\n","    model = models.Model(model_input, x)\n","\n","    return model\n","\n","\n","def get_model_by_name(model_name, input_shape, classes=3, pretrained=False):\n","    \"\"\"Get an EfficientNet model by its name.\n","    \"\"\"\n","    blocks_args, global_params = get_efficientnet_params(model_name, override_params={'num_classes': classes})\n","    model = _efficientnet(input_shape, blocks_args, global_params)\n","\n","    try:\n","        if pretrained:\n","            weights = IMAGENET_WEIGHTS[model_name]\n","            weights_path = get_file(\n","                weights['name'],\n","                weights['url'],\n","                cache_subdir='models',\n","                md5_hash=weights['md5'],\n","            )\n","            model.load_weights(weights_path)\n","    except KeyError as e:\n","        print(\"NOTE: Currently model {} doesn't have pretrained weights, therefore a model with randomly initialized\"\n","              \" weights is returned.\".format(e))\n","\n","    return model\n","\n","\n","def _get_efficientnet_encoder(model_name, input_shape, pretrained=False):\n","    model = get_model_by_name(model_name, input_shape, pretrained=pretrained)\n","    encoder = models.Model(model.input, model.get_layer('global_average_pooling2d').output)\n","    encoder.layers.pop()  # remove GAP layer\n","    return encoder\n","\n","\n","def get_efficientnet_b0_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b0', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b1_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b1', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b2_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b2', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b3_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b3', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b4_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b4', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b5_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b5', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b6_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b6', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b7_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b7', input_shape, pretrained=pretrained)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iz1mgr0_goYL"},"source":["## Utils"]},{"cell_type":"code","metadata":{"cellView":"form","id":"qje92-TUkfjU"},"source":["#@title number of classes\n","\n","n_classes=5 #@param {type:\"integer\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PR4BA07zgNwD","cellView":"form"},"source":["#@title Utils\n","import re\n","from collections import namedtuple\n","from keras import layers\n","import keras.backend as K\n","import tensorflow as tf\n","import math\n","import numpy as np\n","\n","GlobalParams = namedtuple('GlobalParams', ['batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'num_classes',\n","                                           'width_coefficient', 'depth_coefficient', 'depth_divisor', 'min_depth',\n","                                           'drop_connect_rate'])\n","global_params = None\n","GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n","\n","BlockArgs = namedtuple('BlockArgs', ['kernel_size', 'num_repeat', 'input_filters', 'output_filters', 'expand_ratio',\n","                                     'id_skip', 'strides', 'se_ratio'])\n","BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n","\n","IMAGENET_WEIGHTS = {\n","\n","    'efficientnet-b0': {\n","        'name': 'efficientnet-b0_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000.h5',\n","        'md5': 'bca04d16b1b8a7c607b1152fe9261af7',\n","    },\n","\n","    'efficientnet-b1': {\n","        'name': 'efficientnet-b1_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000.h5',\n","        'md5': 'bd4a2b82f6f6bada74fc754553c464fc',\n","    },\n","\n","    'efficientnet-b2': {\n","        'name': 'efficientnet-b2_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000.h5',\n","        'md5': '45b28b26f15958bac270ab527a376999',\n","    },\n","\n","    'efficientnet-b3': {\n","        'name': 'efficientnet-b3_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000.h5',\n","        'md5': 'decd2c8a23971734f9d3f6b4053bf424',\n","    },\n","\n","    'efficientnet-b4': {\n","        'name': 'efficientnet-b4_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000.h5',\n","        'md5': '01df77157a86609530aeb4f1f9527949',\n","    },\n","\n","    'efficientnet-b5': {\n","        'name': 'efficientnet-b5_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000.h5',\n","        'md5': 'c31311a1a38b5111e14457145fccdf32',\n","    }\n","\n","}\n","\n","\n","def round_filters(filters, global_params):\n","    \"\"\"Round number of filters.\"\"\"\n","    multiplier = global_params.width_coefficient\n","    divisor = global_params.depth_divisor\n","    min_depth = global_params.min_depth\n","    if not multiplier:\n","        return filters\n","\n","    filters *= multiplier\n","    min_depth = min_depth or divisor\n","    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_filters < 0.9 * filters:\n","        new_filters += divisor\n","    return int(new_filters)\n","\n","\n","def round_repeats(repeats, global_params):\n","    \"\"\"Round number of repeats.\"\"\"\n","    multiplier = global_params.depth_coefficient\n","    if not multiplier:\n","        return repeats\n","    return int(math.ceil(multiplier * repeats))\n","\n","\n","def get_efficientnet_params(model_name, override_params=None):\n","    \"\"\"Get efficientnet params based on model name.\"\"\"\n","    params_dict = {\n","        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n","        # Note: the resolution here is just for reference, its values won't be used.\n","        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n","        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n","        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n","        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n","        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n","        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n","        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n","        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n","    }\n","    if model_name not in params_dict.keys():\n","        raise KeyError('There is no model named {}.'.format(model_name))\n","\n","    width_coefficient, depth_coefficient, _, dropout_rate = params_dict[model_name]\n","\n","    blocks_args = [\n","        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n","        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n","        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n","        'r1_k3_s11_e6_i192_o320_se0.25',\n","    ]\n","    global_params = GlobalParams(\n","        batch_norm_momentum=0.99,\n","        batch_norm_epsilon=1e-3,\n","        dropout_rate=dropout_rate,\n","        drop_connect_rate=0.2,\n","        num_classes=n_classes,\n","        width_coefficient=width_coefficient,\n","        depth_coefficient=depth_coefficient,\n","        depth_divisor=8,\n","        min_depth=None)\n","\n","    if override_params:\n","        global_params = global_params._replace(**override_params)\n","\n","    decoder = BlockDecoder()\n","    return decoder.decode(blocks_args), global_params\n","\n","\n","class BlockDecoder(object):\n","    \"\"\"Block Decoder for readability.\"\"\"\n","\n","    @staticmethod\n","    def _decode_block_string(block_string):\n","        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n","        assert isinstance(block_string, str)\n","        ops = block_string.split('_')\n","        options = {}\n","        for op in ops:\n","            splits = re.split(r'(\\d.*)', op)\n","            if len(splits) >= 2:\n","                key, value = splits[:2]\n","                options[key] = value\n","\n","        if 's' not in options or len(options['s']) != 2:\n","            raise ValueError('Strides options should be a pair of integers.')\n","\n","        return BlockArgs(\n","            kernel_size=int(options['k']),\n","            num_repeat=int(options['r']),\n","            input_filters=int(options['i']),\n","            output_filters=int(options['o']),\n","            expand_ratio=int(options['e']),\n","            id_skip=('noskip' not in block_string),\n","            se_ratio=float(options['se']) if 'se' in options else None,\n","            strides=[int(options['s'][0]), int(options['s'][1])]\n","        )\n","\n","    @staticmethod\n","    def _encode_block_string(block):\n","        \"\"\"Encodes a block to a string.\"\"\"\n","        args = [\n","            'r%d' % block.num_repeat,\n","            'k%d' % block.kernel_size,\n","            's%d%d' % (block.strides[0], block.strides[1]),\n","            'e%s' % block.expand_ratio,\n","            'i%d' % block.input_filters,\n","            'o%d' % block.output_filters\n","        ]\n","        if 0 < block.se_ratio <= 1:\n","            args.append('se%s' % block.se_ratio)\n","        if block.id_skip is False:\n","            args.append('noskip')\n","        return '_'.join(args)\n","\n","    def decode(self, string_list):\n","        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n","        Args:\n","          string_list: a list of strings, each string is a notation of block.\n","        Returns:\n","          A list of namedtuples to represent blocks arguments.\n","        \"\"\"\n","        assert isinstance(string_list, list)\n","        blocks_args = []\n","        for block_string in string_list:\n","            blocks_args.append(self._decode_block_string(block_string))\n","        return blocks_args\n","\n","    def encode(self, blocks_args):\n","        \"\"\"Encodes a list of Blocks to a list of strings.\n","        Args:\n","          blocks_args: A list of namedtuples to represent blocks arguments.\n","        Returns:\n","          a list of strings, each string is a notation of block.\n","        \"\"\"\n","        block_strings = []\n","        for block in blocks_args:\n","            block_strings.append(self._encode_block_string(block))\n","        return block_strings\n","\n","\n","class Swish(layers.Layer):\n","    def __init__(self, name=None, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","    def call(self, inputs, **kwargs):\n","        return tf.nn.silu(inputs)#tf.nn.swish I have changed this why I don't know yet\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['name'] = self.name\n","        return config\n","\n","\n","def SEBlock(block_args, **kwargs):\n","    num_reduced_filters = max(\n","        1, int(block_args.input_filters * block_args.se_ratio))\n","    filters = block_args.input_filters * block_args.expand_ratio\n","\n","    spatial_dims = [1, 2]\n","\n","    try:\n","        block_name = kwargs['block_name']\n","    except KeyError:\n","        block_name = ''\n","\n","    def block(inputs):\n","        x = inputs\n","        x = layers.Lambda(lambda a: K.mean(a, axis=spatial_dims, keepdims=True))(x)\n","        x = layers.Conv2D(\n","            num_reduced_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_reduce_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = Swish(name=block_name + 'se_swish')(x)\n","\n","        x = layers.Conv2D(\n","            filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_expand_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = layers.Activation('sigmoid')(x)\n","        out = layers.Multiply()([x, inputs])\n","        return out\n","\n","    return block\n","\n","\n","class DropConnect(layers.Layer):\n","\n","    def __init__(self, drop_connect_rate, **kwargs):\n","        super().__init__(**kwargs)\n","        self.drop_connect_rate = drop_connect_rate\n","\n","    def call(self, inputs, **kwargs):\n","        def drop_connect():\n","            keep_prob = 1.0 - self.drop_connect_rate\n","\n","            # Compute drop_connect tensor\n","            batch_size = tf.shape(inputs)[0]\n","            random_tensor = keep_prob\n","            random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n","            binary_tensor = tf.floor(random_tensor)\n","            output = tf.math.divide(inputs, keep_prob) * binary_tensor\n","            return output\n","\n","        return K.in_train_phase(drop_connect(), inputs, training=None)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['drop_connect_rate'] = self.drop_connect_rate\n","        return config\n","\n","\n","def conv_kernel_initializer(shape, dtype=K.floatx()):\n","    \"\"\"Initialization for convolutional kernels.\n","    The main difference with tf.variance_scaling_initializer is that\n","    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n","    standard deviation, whereas here we use a normal distribution. Similarly,\n","    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n","    a corrected standard deviation.\n","    Args:\n","        shape: shape of variable\n","        dtype: dtype of variable\n","    Returns:\n","        an initialization for the variable\n","    \"\"\"\n","    kernel_height, kernel_width, _, out_filters = shape\n","    fan_out = int(kernel_height * kernel_width * out_filters)\n","    return tf.random.normal(\n","        shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n","\n","\n","def dense_kernel_initializer(shape, dtype=K.floatx()):\n","    init_range = 1.0 / np.sqrt(shape[1])\n","    return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)\n","\n","\n","def MBConvBlock(block_args, global_params, idx, drop_connect_rate=None):\n","    filters = block_args.input_filters * block_args.expand_ratio\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","    has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n","\n","    block_name = 'blocks_' + str(idx) + '_'\n","\n","    def block(inputs):\n","        x = inputs\n","\n","        # Expansion phase\n","        if block_args.expand_ratio != 1:\n","            expand_conv = layers.Conv2D(filters,\n","                                        kernel_size=[1, 1],\n","                                        strides=[1, 1],\n","                                        kernel_initializer=conv_kernel_initializer,\n","                                        padding='same',\n","                                        use_bias=False,\n","                                        name=block_name + 'expansion_conv2d'\n","                                        )(x)\n","            bn0 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                            epsilon=batch_norm_epsilon,\n","                                            name=block_name + 'expansion_batch_norm')(expand_conv)\n","\n","            x = Swish(name=block_name + 'expansion_swish')(bn0)\n","\n","        # Depth-wise convolution phase\n","        kernel_size = block_args.kernel_size\n","        depthwise_conv = layers.DepthwiseConv2D(\n","            [kernel_size, kernel_size],\n","            strides=block_args.strides,\n","            depthwise_initializer=conv_kernel_initializer,\n","            padding='same',\n","            use_bias=False,\n","            name=block_name + 'depthwise_conv2d'\n","        )(x)\n","        bn1 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                        epsilon=batch_norm_epsilon,\n","                                        name=block_name + 'depthwise_batch_norm'\n","                                        )(depthwise_conv)\n","        x = Swish(name=block_name + 'depthwise_swish')(bn1)\n","\n","        if has_se:\n","            x = SEBlock(block_args, block_name=block_name)(x)\n","\n","        # Output phase\n","        project_conv = layers.Conv2D(\n","            block_args.output_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'output_conv2d',\n","            use_bias=False)(x)\n","        x = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                      epsilon=batch_norm_epsilon,\n","                                      name=block_name + 'output_batch_norm'\n","                                      )(project_conv)\n","        if block_args.id_skip:\n","            if all(\n","                    s == 1 for s in block_args.strides\n","            ) and block_args.input_filters == block_args.output_filters:\n","                # only apply drop_connect if skip presents.\n","                if drop_connect_rate:\n","                    x = DropConnect(drop_connect_rate)(x)\n","                x = layers.add([x, inputs])\n","\n","        return x\n","\n","    return block\n","\n","\n","def freeze_efficientunet_first_n_blocks(model, n):\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            model.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","\n","    all_block_names = ['blocks_{}_output_batch_norm'.format(i) for i in range(mbblock_nr)]\n","    all_block_index = []\n","    for idx, layer in enumerate(model.layers):\n","        if layer.name == all_block_names[0]:\n","            all_block_index.append(idx)\n","            all_block_names.pop(0)\n","            if len(all_block_names) == 0:\n","                break\n","    n_blocks = len(all_block_index)\n","\n","    if n <= 0:\n","        print('n is less than or equal to 0, therefore no layer will be frozen.')\n","        return\n","    if n > n_blocks:\n","        raise ValueError(\"There are {} blocks in total, n cannot be greater than {}.\".format(n_blocks, n_blocks))\n","\n","    idx_of_last_block_to_be_frozen = all_block_index[n - 1]\n","    for layer in model.layers[:idx_of_last_block_to_be_frozen + 1]:\n","        layer.trainable = False\n","\n","\n","def unfreeze_efficientunet(model):\n","    for layer in model.layers:\n","        layer.trainable = True\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZ29zXFqh5sm"},"source":["## *Efficientunet*"]},{"cell_type":"code","metadata":{"id":"TGg2DkR-g-2L"},"source":["#@markdown Efficientnet-unet\n","import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from keras.layers import *\n","from keras import models\n","# from efficientnet import *\n","# from utils import conv_kernel_initializer\n","\n","\n","__all__ = ['get_efficient_unet_b0', 'get_efficient_unet_b1', 'get_efficient_unet_b2', 'get_efficient_unet_b3',\n","           'get_efficient_unet_b4', 'get_efficient_unet_b5', 'get_efficient_unet_b6', 'get_efficient_unet_b7',\n","           'get_blocknr_of_skip_candidates']\n","\n","\n","def get_blocknr_of_skip_candidates(encoder, verbose=False):\n","    \"\"\"\n","    Get block numbers of the blocks which will be used for concatenation in the Unet.\n","    :param encoder: the encoder\n","    :param verbose: if set to True, the shape information of all blocks will be printed in the console\n","    :return: a list of block numbers\n","    \"\"\"\n","    shapes = []\n","    candidates = []\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","            shape = int(mbblock.shape[1]), int(mbblock.shape[2])\n","            if shape not in shapes:\n","                shapes.append(shape)\n","                candidates.append(mbblock_nr)\n","            if verbose:\n","                print('blocks_{}_output_shape: {}'.format(mbblock_nr, shape))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","    return candidates\n","\n","\n","def DoubleConv(filters, kernel_size, initializer='glorot_uniform'):\n","\n","    def layer(x):\n","\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","def UpSampling2D_block(filters, kernel_size=(3, 3), upsample_rate=(2, 2), interpolation='bilinear',\n","                       initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = UpSampling2D(size=upsample_rate, interpolation=interpolation)(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","    return layer\n","\n","\n","def Conv2DTranspose_block(filters, kernel_size=(3, 3), transpose_kernel_size=(2, 2), upsample_rate=(2, 2),\n","                          initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate, padding='same')(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","# noinspection PyTypeChecker\n","def _get_efficient_unet(encoder, out_channels=2, block_type='upsampling', concat_input=True):\n","    MBConvBlocks = []\n","\n","    skip_candidates = get_blocknr_of_skip_candidates(encoder)\n","\n","    for mbblock_nr in skip_candidates:\n","        mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","        MBConvBlocks.append(mbblock)\n","\n","    # delete the last block since it won't be used in the process of concatenation\n","    MBConvBlocks.pop()\n","\n","    input_ = encoder.input\n","    head = encoder.get_layer('head_swish').output\n","    blocks = [input_] + MBConvBlocks + [head]\n","\n","    if block_type == 'upsampling':\n","        UpBlock = UpSampling2D_block\n","    else:\n","        UpBlock = Conv2DTranspose_block\n","\n","    o = blocks.pop()\n","    o = UpBlock(512, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(256, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(128, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(64, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    if concat_input:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    else:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=None)(o)\n","    o = Conv2D(out_channels, (1, 1), padding='same', kernel_initializer=conv_kernel_initializer)(o)\n","\n","    model = models.Model(encoder.input, o)\n","\n","    return model\n","\n","\n","def get_efficient_unet_b0(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B0 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B0 model\n","    \"\"\"\n","    encoder = get_efficientnet_b0_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b1(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B1 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B1 model\n","    \"\"\"\n","    encoder = get_efficientnet_b1_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b2(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B2 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B2 model\n","    \"\"\"\n","    encoder = get_efficientnet_b2_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b3(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B3 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B3 model\n","    \"\"\"\n","    encoder = get_efficientnet_b3_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b4(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B4 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B4 model\n","    \"\"\"\n","    encoder = get_efficientnet_b4_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b5(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B5 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B5 model\n","    \"\"\"\n","    encoder = get_efficientnet_b5_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b6(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B6 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B6 model\n","    \"\"\"\n","    encoder = get_efficientnet_b6_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b7(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B7 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B7 model\n","    \"\"\"\n","    encoder = get_efficientnet_b7_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWn7h2XETHDn"},"source":["# C model"]},{"cell_type":"code","metadata":{"id":"QQAwN0zNl4Jf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSbvK6VVTlsl"},"source":["Channels =  3#@param {type:\"integer\"}\n","Img_size = 180#@param {type:\"integer\"}\n","input_shape = (Img_size, Img_size, Channels) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fkid-gMTElW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628997159125,"user_tz":-60,"elapsed":3492,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"1d42517b-b67d-4178-d7aa-479a15117007"},"source":["classifier =  get_efficientnet_b0_encoder(input_shape)\n","modelS = models.Sequential()\n","modelS.add(classifier)\n","modelS.add(layers.Dense(n_classes))\n","modelS.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_1 (Functional)         (None, 1280)              4049564   \n","_________________________________________________________________\n","dense (Dense)                (None, 5)                 6405      \n","=================================================================\n","Total params: 4,055,969\n","Trainable params: 4,013,953\n","Non-trainable params: 42,016\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2BHY357Hfy5","executionInfo":{"status":"ok","timestamp":1628997161864,"user_tz":-60,"elapsed":2745,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"6070f44e-d310-4610-8282-6ae7c2261e87"},"source":["classifierT =  get_efficientnet_b0_encoder(input_shape)\n","modelT = models.Sequential()\n","modelT.add(classifierT)\n","modelT.add(layers.Dense(n_classes))\n","modelT.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_3 (Functional)         (None, 1280)              4049564   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 5)                 6405      \n","=================================================================\n","Total params: 4,055,969\n","Trainable params: 4,013,953\n","Non-trainable params: 42,016\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7ld0iL6l7sd7"},"source":["# Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNmm_kkr5RcZ","executionInfo":{"status":"ok","timestamp":1628997161865,"user_tz":-60,"elapsed":32,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"fd24ac2b-5370-49c3-d4e2-8f41e783d22c"},"source":["from tensorflow.python.client import device_lib\n","\n","def get_available_gpus():\n","    local_device_protos = device_lib.list_local_devices()\n","    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n","get_available_gpus()    "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"1iJF97nG5Rcb"},"source":["#%cd /content/drive/MyDrive/Thesis/MPL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3Igabyt7sd9","cellView":"form"},"source":["#@title MPL config\n","import tensorflow as tf\n","\n","\n","\n","\n","# about dataset\n","IMG_SIZE = 180#@param {type:\"integer\"}\n","BATCH_SIZE = 8#@param {type:\"integer\"}\n","# LABEL_FILE_PATH = '/content/cifar/label4000.csv' # google\n","# UNLABEL_FILE_PATH = '/content/cifar/train.csv'\n","\n","_MAX_LEVEL = 10\n","CUTOUT_CONST = 40.\n","TRANSLATE_CONST = 100.\n","REPLACE_COLOR = [128, 128, 128]\n","\n","\n","# LABEL_FILE_PATH = '../input/cifar10/cifar/label4000.csv'  # kaggle\n","# UNLABEL_FILE_PATH = '../input/cifar10/cifar/train.csv'\n","\n","\n","AUGMENT_MAGNITUDE = 8\n","SHUFFLE_SIZE = BATCH_SIZE * 16\n","DATA_LEN = 400  # 数据集的总长度\n","\n","# about model\n","NUM_XLA_SHARDS = -1\n","BATCH_NORM_EPSILON = 1e-3\n","BATCH_NORM_DECAY = 0.999\n","DROPOUT_RATE = 0.\n","DROPOUT = 0.2\n","NUM_CLASSES = 5#@param {type:\"integer\"}\n","NUM_CLASS = 5#@param {type:\"integer\"}\n","\n","# about training\n","LOG_EVERY = 20\n","SAVE_EVERY = 5\n","TEA_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/b4PST3'\n","STD_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/b4PSS3'\n","\n","MAX_EPOCHS = 1920\n","MAX_STEPS = MAX_EPOCHS * (int(DATA_LEN / BATCH_SIZE)-1)\n","UDA_WEIGHT = 8  # uda的权重\n","UDA_STEPS = 2000\n","TEST_EVERY = 2\n","GRAD_BOUND = 1e9\n","EMA = 0.995\n","\n","\n","# continue train\n","TEA_CONTINUE = False\n","STD_CONTINUE = False\n","TEA_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/b4PST3'\n","STD_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/b4PSS3'\n","CONTINUE_EPOCH = 885\n","\n","\n","# about testing\n","# TEST_FILE_PATH = '/content/cifar/test.csv'\n","# TEST_FILE_PATH = '../input/cifar10/cifar/test.csv'\n","TEST_MODEL_PATH = '/content/drive/MyDrive/Thesis/weights/PS'\n","\n","# about UdaCrossEntroy\n","UDA_DATA = 4\n","LABEL_SMOOTHING = 0.15\n","UDA_TEMP = 0.7\n","UDA_THRESHOLD = 0.6\n","\n","# about learning rate\n","STUDENT_LR = 0.0005  # student\n","STUDENT_LR_WARMUP_STEPS = 4000\n","STUDENT_LR_WAIT_STEPS = 2000\n","TEACHER_LR = 0.0005  # teacher\n","TEACHER_LR_WARMUP_STEPS = 1000\n","TEACHER_NUM_WAIT_STEPS = 0\n","\n","LR_DECAY_TYPE = 'cosine'  # constant, exponential, cosine\n","NUM_DECAY_STEPS = 300\n","LR_DECAY_RATE = 0.97\n","\n","# about optimizer\n","OPTIM_TYPE = 'sgd'  # sgd, momentum, rmsprop\n","WEIGHT_DECAY = 5e-4\n","\n","\n","# dtype\n","DTYPE = tf.float32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XgCSbvH7sd-","cellView":"form"},"source":["#@title Self_aug_func\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import math\n","import tensorflow_addons.image as image_ops\n","\n","# import \n","\n","\n","def autocontrast(image):\n","    lo = tf.cast(tf.reduce_min(image, axis=[0, 1]), tf.float32)\n","    hi = tf.cast(tf.reduce_max(image, axis=[0, 1]), tf.float32)\n","    scale = tf.math.divide(255.0, (hi - lo))\n","    offset = tf.math.multiply(-lo, scale)\n","    image = tf.math.add(\n","        tf.math.multiply(tf.cast(image, tf.float32), scale),\n","        offset\n","    )\n","    image = tf.clip_by_value(image, 0.0, 255.0)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def equalize(image):\n","    # image = tf.cast(image, tf.int32)\n","    # channel = tf.shape(image)[-1]\n","    # for i in range(channel):\n","    #     im = tf.cast(image[:, :, i], tf.int32)\n","    #     histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","    #     nonzero = tf.where(tf.not_equal(histo, 0))\n","    #     nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","    #     step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","    #     print(step)\n","    #     if step == 0:\n","    #         pass\n","    #     else:\n","    #         lut = (tf.cumsum(histo) + (step // 2)) // step\n","    #         lut = tf.concat([[0], lut[:-1]], 0)\n","    #         lut = tf.clip_by_value(lut, 0, 255)\n","    #         # print(lut)\n","    #         image[:, :, i] = tf.gather(lut, image[:, :, i])\n","    #         # image[:, :, i] = im\n","    #     # image[:, :, i] = im\n","\n","    def scale_channel(im, c=0):\n","        \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n","        im = tf.cast(im[:, :, 0], tf.int32)\n","        # Compute the histogram of the image channel.\n","        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","\n","        # For the purposes of computing the step, filter out the nonzeros.\n","        nonzero = tf.where(tf.not_equal(histo, 0))\n","        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","\n","        def build_lut(histo, step):\n","            # Compute the cumulative sum, shifting by step // 2\n","            # and then normalization by step.\n","            lut = (tf.cumsum(histo) + (step // 2)) // step\n","            # Shift lut, prepending with 0.\n","            lut = tf.concat([[0], lut[:-1]], 0)\n","            # Clip the counts to be in range.  This is done\n","            # in the C code for image.point.\n","            return tf.clip_by_value(lut, 0, 255)\n","\n","        # If step is zero, return the original image.  Otherwise, build\n","        # lut from the full histogram and step and then index from it.\n","        result = tf.cond(tf.equal(step, 0),\n","                         lambda: im,\n","                         lambda: tf.gather(build_lut(histo, step), im))\n","        return tf.cast(result, tf.uint8)\n","\n","    s1 = scale_channel(image, 0)\n","    s2 = scale_channel(image, 1)\n","    s3 = scale_channel(image, 2)\n","    image = tf.stack([s1, s2, s3], 2)\n","\n","    return image\n","\n","\n","def invert(image):\n","    image = 255 - image\n","    return image\n","\n","\n","def rotate(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    degree = tf.cond(should_filp, lambda: level, lambda: -level)\n","    degree_to_radians = tf.convert_to_tensor(math.pi / 180., tf.float32)\n","    radians = tf.math.multiply(degree, degree_to_radians)\n","    new_imgsize = tf.cast(tf.math.abs(tf.divide(IMG_SIZE, radians)), tf.int32)\n","    image = tf.image.resize(image, (new_imgsize, new_imgsize))\n","    image = image_ops.rotate(image, radians, fill_mode='constant')\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def posterize(image):\n","    bit = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 4, tf.float32)\n","    shift = tf.cast(8 - bit, image.dtype)\n","    image = tf.bitwise.right_shift(image, shift)\n","    image = tf.bitwise.left_shift(image, shift)\n","    return image\n","\n","\n","def solarize_arg(image):\n","    threahold = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 22, tf.float32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def solarize_add(image, threahold=128):\n","    addition = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 2, tf.int32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.add(tf.cast(image, tf.int32), addition)\n","    image = tf.cast(tf.clip_by_value(image, 0, 255), tf.uint8)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def color(image, degenetate=None):\n","    if degenetate is None:\n","        degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.8 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def contrast(image):\n","    degenerate = tf.image.rgb_to_grayscale(image)\n","    degenerate = tf.cast(degenerate, tf.int32)\n","\n","    hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n","    mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.\n","    degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n","    degenerate = tf.clip_by_value(degenerate, 0., 255.)\n","    degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.6 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def brightness(image):\n","    image = tf.image.adjust_brightness(image, 0.25)\n","    return image\n","\n","\n","def sharpness(image):\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.6 + 0.1, tf.float32)\n","    image = tf.cast(image, tf.float32)\n","    image = image_ops.sharpness(image, factor)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_x(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.2, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_x(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_y(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.1, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_y(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def translate_x(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [-pixels, 0])\n","    return image\n","\n","\n","def translate_y(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [0, -pixels])\n","    return image\n","\n","\n","def cutout(image):\n","    pad_size = tf.cast(\n","        tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * CUTOUT_CONST,\n","        tf.int32\n","    )\n","    image_height = tf.shape(image)[0]\n","    image_width = tf.shape(image)[1]\n","\n","    # Samples the center location in the image where the zero mask is applied.\n","    cutout_center_height = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_height,\n","        dtype=tf.int32)\n","\n","    cutout_center_width = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_width,\n","        dtype=tf.int32)\n","\n","    lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n","    upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n","    left_pad = tf.maximum(0, cutout_center_width - pad_size)\n","    right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n","\n","    cutout_shape = [image_height - (lower_pad + upper_pad),\n","                    image_width - (left_pad + right_pad)]\n","    padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n","    mask = tf.pad(\n","        tf.zeros(cutout_shape, dtype=image.dtype),\n","        padding_dims, constant_values=1)\n","    mask = tf.expand_dims(mask, -1)\n","    mask = tf.tile(mask, [1, 1, 3])\n","    image = tf.where(\n","        tf.equal(mask, 0),\n","        tf.ones_like(image, dtype=image.dtype) * REPLACE_COLOR,\n","        image)\n","    return image\n","\n","\n","def identity(image):\n","    return tf.identity(image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qxxtHeZ7seC","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1628997162773,"user_tz":-60,"elapsed":14,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"b8ee3f6c-7335-4076-91f8-6c6472c47598"},"source":["#@title Self_aug_util\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import tensorflow as tf\n","\n","# from self_aug_func import *\n","\n","_MAX_LEVEL = 10\n","\n","\n","\n","def _enhance_level_to_arg(level):\n","    return (tf.cast((level / _MAX_LEVEL) * 1.8 + 0.1, tf.float32),)\n","\n","\n","def _translate_level_to_arg(level, translate_const):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * float(translate_const), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    final_tensor = tf.cond(should_flip, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _rotate_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _shear_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 0.3, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def level_to_arg(cutout_const, translate_const):\n","    '''\n","    将对image做变化的函数所用到的参数整理成字典形式\n","    :param cutout_const:\n","    :param translate_const:\n","    :return: type:dict\n","    '''\n","    no_arg = lambda level: ()\n","    posterize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 4,\n","        tf.float32\n","    )\n","    solarize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 256,\n","        tf.float32\n","    )\n","    solarize_add_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 110,\n","        tf.float32\n","    )\n","    cutout_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * cutout_const,\n","        tf.float32\n","    )\n","    translate_arg = lambda level: _translate_level_to_arg(level, translate_const)\n","\n","    args = {\n","        'Identity': no_arg,\n","        'AutoContrast': no_arg,\n","        'Equalize': no_arg,\n","        'Invert': no_arg,\n","        'Rotate': _rotate_level_to_arg,\n","        'Posterize': posterize_arg,\n","        'Solarize': solarize_arg,\n","        'SplarizeAdd': solarize_add_arg,\n","        'Color': _enhance_level_to_arg,\n","        'Contrast': _enhance_level_to_arg,\n","        'Brightness': _enhance_level_to_arg,\n","        'Sharpness': _enhance_level_to_arg,\n","        'ShearX': _shear_level_to_arg,\n","        'ShearY': _shear_level_to_arg,\n","        'Cutout': cutout_arg,\n","        'TranslateX': translate_arg,\n","        'TranslateY': translate_arg,\n","    }\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    NAME_TO_FUNC = {\n","        'AutoContrast': autocontrast,\n","        'Equalize': equalize,\n","        'Invert': invert,\n","        'Rotate': rotate,\n","        'Posterize': posterize,\n","        'Solarize': solarize_arg,\n","        'SolarizeAdd': solarize_add,\n","        'Color': color,\n","        'Contrast': contrast,\n","        'Brightness': brightness,\n","        'Sharpness': sharpness,\n","        'ShearX': shear_x,\n","        'ShearY': shear_y,\n","        'TranslateX': translate_x,\n","        'TranslateY': translate_y,\n","        'Cutout': cutout,\n","        'Identity': identity,\n","    }\n","\n","    available_ops = [\n","        'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","        'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","    ]\n","\n","    for (i, op_name) in enumerate(available_ops):\n","        func = NAME_TO_FUNC[op_name]\n","        args = level_to_arg(4, 4)[op_name](16)\n","        print(args)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["()\n","()\n","()\n","tf.Tensor(-48.0, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(409.6, shape=(), dtype=float32)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","tf.Tensor(0.48, shape=(), dtype=float32)\n","tf.Tensor(-0.48, shape=(), dtype=float32)\n","tf.Tensor(-6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wKiP8pWx7seE","cellView":"form"},"source":["#@title Self_augment\n","'''\n","reference:\n","https://github.com/google-research/google-research/tree/1f1741a985a0f2e6264adae985bde664a7993bd2/flax_models/cifar/datasets\n","'''\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","\n","'''\n","可能augment.py中的内容有问题 涉及文件augment.py的line 53，54\n","引用的库不一样，因为tensorflow.contrib已经停用，\n","使用的第三方：pip install tensorflow-addons\n","'''\n","import os\n","\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","\n","# from self_aug_func import *\n","\n","# 将对图片做augment的函数变成一个字典\n","NAME_TO_FUNC = {\n","    'AutoContrast': autocontrast,\n","    'Equalize': equalize,\n","    'Invert': invert,\n","    'Rotate': rotate,\n","    'Posterize': posterize,\n","    'Solarize': solarize_arg,\n","    'SolarizeAdd': solarize_add,\n","    'Color': color,\n","    'Contrast': contrast,\n","    'Brightness': brightness,\n","    'Sharpness': sharpness,\n","    'ShearX': shear_x,\n","    'ShearY': shear_y,\n","    'TranslateX': translate_x,\n","    'TranslateY': translate_y,\n","    'Cutout': cutout,\n","    'Identity': identity,\n","}\n","# 在某些函数中有一些需要一个替换的值，比如旋转中有一些位置的像素值需要补充\n","REPLACE_FUNCS = frozenset({\n","    'Rotate',\n","    'TranslateX',\n","    'ShearX',\n","    'SHearY',\n","    'TranslateY',\n","    'Cutout',\n","})\n","\n","\n","class RandAugment(object):\n","    def __init__(self, num_layers=2, magnitude=None, cutout_const=40, translate_const=100., available_ops=None):\n","        '''\n","        reference: https://arxiv.org/abs/1909.13719\n","        :param num_layers:\n","        :param magnitude:\n","        :param cutout_const:\n","        :param translate_const:\n","        :param avalilable_ops:\n","        '''\n","        super(RandAugment, self).__init__()\n","        self.num_layers = num_layers\n","        self.cutout_const = float(cutout_const)\n","        self.translate_const = float(translate_const)\n","        if available_ops is None:\n","            available_ops = [\n","                'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","                'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","                'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","            ]\n","        self.available_ops = available_ops\n","        self.magnitude = magnitude\n","\n","    def distort(self, image):\n","        '''\n","\n","        :param image:  shape:[HWC] C=3\n","        :return: 返回一个经过变化后的图片\n","        '''\n","        input_image_type = image.dtype\n","        image = tf.clip_by_value(image, tf.cast(0, input_image_type), tf.cast(255, input_image_type))\n","        image = tf.cast(image, tf.uint8)\n","\n","        prob = tf.random.uniform([], 0.2, 0.8, tf.float32)\n","\n","        for _ in range(self.num_layers):\n","            op_to_select = tf.random.uniform([], minval=0, maxval=len(self.available_ops), dtype=tf.int32)\n","            for (i, op_name) in enumerate(self.available_ops):\n","                func = NAME_TO_FUNC[op_name]  # 得到函数名称\n","                if i == op_to_select:\n","                    flag = tf.random.uniform([], 0., 1., prob.dtype)\n","                    if tf.math.greater_equal(prob, flag):\n","                        image = func(image)\n","\n","        image = tf.cast(image, dtype=input_image_type)\n","        return image\n","\n","\n","def unlabel_image(img_file, label):\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","\n","    aug_image, some_info = aug.distort(img)\n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, tf.float32) / 255.0\n","    ori_image = tf.cast(ori_image, tf.float32) / 255.0\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pesc1AwM7seG","cellView":"form"},"source":["#@title UDa\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","\n","# import config\n","# from Model import Wrn28k\n","\n","\n","def UdaCrossEntroy(all_logits, l_labels, global_step):\n","    batch_size = BATCH_SIZE\n","    uda_data = UDA_DATA\n","    logits = {}\n","    labels = {}\n","    cross_entroy = {}\n","    masks = {}\n","    # 将网络的输出结果区分成 label ori aug 三个部分\n","    logits['l'], logits['ori'], logits['aug'] = tf.split(\n","        all_logits,\n","        [batch_size, batch_size * uda_data, batch_size * uda_data],\n","        axis=0,\n","    )\n","    # 对标签进行处理\n","    labels['l'] = l_labels\n","\n","    # ------------loss的计算---------\n","    # part1：有监督部分\n","    cross_entroy['l'] = tf.losses.CategoricalCrossentropy(\n","        from_logits=True,\n","        label_smoothing=LABEL_SMOOTHING,\n","        reduction=keras.losses.Reduction.NONE,)(labels['l'], logits['l'])\n","    '''\n","    probs = tf.nn.softmax(logits['l'], axis=-1)  # 将每张图片对应10个类别的输出转化为概率的形式\n","    correct_probs = tf.reduce_sum(labels['l'] * probs, axis=-1)  # 根据图片对应的label和概率计算出 预测正确类别的概率\n","    # 计算一个阈值l_threshold\n","    r = tf.cast(global_step, tf.float32) / tf.convert_to_tensor(MAX_STEPS, dtype=tf.float32)\n","    num_classes = tf.convert_to_tensor(NUM_CLASSES, tf.float32)\n","    l_threshold = r * (1. - 1. / num_classes) + 1. / num_classes\n","    masks['l'] = tf.math.less_equal(correct_probs, l_threshold)\n","    masks['l'] = tf.cast(masks['l'], tf.float32)\n","    masks['l'] = tf.stop_gradient(masks['l'])  # 如果对某图片预测的概率小于l_threahold,输出1，否则是0\n","    '''\n","    cross_entroy['l'] = tf.reduce_sum(cross_entroy['l']) / float(batch_size)\n","\n","    # part2: 无监督部分\n","    labels['ori'] = tf.nn.softmax(logits['ori'] / tf.convert_to_tensor(UDA_TEMP), axis=-1)\n","    labels['ori'] = tf.stop_gradient(labels['ori'])\n","    # tf.nn.log_softmax: 设一张图片对应3个类别的输出为o1，o2，o3 ==>\n","    # b = log(sum(exp(o1) + exp(o2) + exp(o3)))  new_o1=o1-b, new_o2=o2-b ... 恒负，大小关系不变\n","    cross_entroy['u'] = (\n","            labels['ori'] * tf.nn.log_softmax(logits['aug'], axis=-1)\n","    )\n","\n","    largest_probs = tf.reduce_max(labels['ori'], axis=-1, keepdims=True)\n","\n","    masks['u'] = tf.math.greater_equal(largest_probs, tf.constant(UDA_THRESHOLD))  # 判断最大概率是否大于阈值\n","    masks['u'] = tf.cast(masks['u'], DTYPE)\n","    masks['u'] = tf.stop_gradient(masks['u'])\n","    # 极端情况，当ori的预测完全准确，即class i = 1, 其他类别为0时，\n","    # aug的class i最大，即最大的负数，两者相乘再取负，就是一个非常接近于0的数字\n","    cross_entroy['u'] = tf.reduce_sum(-cross_entroy['u'] * masks['u']) / \\\n","                        tf.convert_to_tensor((batch_size * uda_data), dtype=DTYPE)\n","\n","    return logits, labels, masks, cross_entroy\n","\n","\n","# if __name__ == '__main__':\n","#     # 制作数据\n","#     l_images = np.random.random((1, 32, 32, 3))\n","#     l_images = tf.convert_to_tensor(l_images, dtype=DTYPE)\n","#     ori_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     ori_images = tf.convert_to_tensor(ori_images, dtype=DTYPE)\n","#     aug_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     aug_images = tf.convert_to_tensor(aug_images, dtype=DTYPE)\n","#     all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [3, 32, 32, 3]\n","\n","#     l_labels = np.array([2])\n","#     l_labels = tf.convert_to_tensor(l_labels, dtype=tf.int32)\n","#     l_labels = tf.raw_ops.OneHot(indices=l_labels, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","#     l_labels = tf.cast(l_labels, DTYPE)\n","\n","#     # 构建teacher模型，产生输出\n","#     teacher = Wrn28k(num_inp_filters=3, k=2)\n","#     output = teacher(x=all_images)  # shape=[15, 10]\n","\n","#     logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, 1)\n","#     print('logits: ', logits.keys())\n","#     print('labels: ', labels.keys())\n","#     print('masks: ', masks.keys())\n","#     # print('cross entroy: ', cross_entroy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4z4XaIqi7seI","cellView":"form"},"source":["#@title Test\n","import os\n","\n","# from WideResnet import WideResnet\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import pandas as pd\n","\n","# import config\n","\n","\n","def test(student, file_paths, labels):\n","    student.training = False\n","    # 准备数据\n","    # df_label = pd.read_csv(TEST_FILE_PATH)\n","    # file_paths = df_label['file_name'].values\n","    # labels = df_label['label'].values\n","\n","    # testing\n","    total_num = int(len(labels)/2)\n","    corrent_num = 0\n","    for i in range(total_num):\n","        img_file = file_paths[i]\n","        label = int(labels[i])\n","\n","        # 对图片的处理\n","        img = tf.io.read_file(img_file)\n","        img = tf.image.decode_jpeg(img, channels=3)\n","        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","        img = tf.cast(img, dtype=DTYPE) / 255.0\n","        img = tf.expand_dims(img, axis=0)\n","        mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","        std = tf.expand_dims(tf.convert_to_tensor([0.0737, 0.0737, 0.0737], dtype=DTYPE), axis=0)\n","        img = (img - mean) / std\n","\n","        # 网络\n","        output = student(img)\n","        output = tf.nn.softmax(output)\n","        class_index = tf.squeeze(tf.math.argmax(output, axis=1))\n","\n","        if class_index == label:\n","            corrent_num += 1\n","    accuracy = float(corrent_num) / float(total_num) * 100.\n","    student.training = True\n","    return accuracy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLLiFSNQ7seJ","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1628997163144,"user_tz":-60,"elapsed":13,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"891d2e02-349f-4a77-c8d2-e659bf98561d"},"source":["#@title learning rate\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","# import config\n","\n","\n","class LearningRate(object):\n","    def __init__(self, initial_lr, num_warmup_steps, num_wait_steps=None):\n","        if initial_lr is None:\n","            raise ValueError(f'initial_lr is error in learningRate file')\n","        if num_warmup_steps is None:\n","            raise ValueError(f'num_warmup_steps is error in learningRate file')\n","        if num_wait_steps is None:\n","            raise ValueError(f'num_wait_steps is error in learningRate file')\n","\n","        # initial_lr = initial_lr * BATCH_SIZE / 256\n","        self.initial_lr = initial_lr\n","        self.num_warmup_steps = num_warmup_steps\n","        self.num_wait_steps = num_wait_steps\n","\n","        if LR_DECAY_TYPE == 'constant':\n","            self.lr = tf.constant(self.initial_lr, dtype=tf.float32)\n","\n","        elif LR_DECAY_TYPE == 'exponential':\n","            self.lr = keras.optimizers.schedules.ExponentialDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=NUM_DECAY_STEPS,\n","                decay_rate=LR_DECAY_RATE,\n","            )\n","\n","        elif LR_DECAY_TYPE == 'cosine':\n","            self.lr = keras.experimental.CosineDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=MAX_STEPS - self.num_wait_steps - self.num_warmup_steps,\n","                alpha=0.0\n","            )\n","        else:\n","            raise ValueError(f'unknown lr_decay_type in py')\n","\n","    def __call__(self, global_step):\n","        global_step = global_step - self.num_wait_steps\n","        if LR_DECAY_TYPE == 'constant':\n","            learn_rate = self.lr\n","        else:\n","            learn_rate = self.lr.__call__(global_step)\n","\n","        r = tf.constant((global_step + 1), tf.float32) / tf.constant(self.num_warmup_steps, tf.float32)\n","        warmup_lr = self.initial_lr * r\n","        lr = tf.cond(\n","            tf.cast(global_step, tf.int32) < tf.cast(self.num_warmup_steps, tf.int32),\n","            lambda: warmup_lr,\n","            lambda: learn_rate,\n","        )\n","        lr = tf.cond(global_step < 0, lambda: tf.constant(0., tf.float32), lambda: lr)\n","        return lr\n","\n","\n","'''\n","def LearningRate(initial_lr, num_warmup_steps, num_wait_steps):\n","    if initial_lr is None:\n","        raise ValueError(f'initial_lr is error in learningRate file')\n","    if num_warmup_steps is None:\n","        raise ValueError(f'num_warmup_steps is error in learningRate file')\n","    if num_wait_steps is None:\n","        raise ValueError(f'num_wait_steps is error in learningRate file')\n","    initial_lr = initial_lr * BATCH_SIZE / 256\n","    if LR_DECAY_TYPE == 'constant':\n","        lr = tf.constant(initial_lr, dtype=tf.float32)\n","    elif LR_DECAY_TYPE == 'exponential':\n","        lr = keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=NUM_DECAY_STEPS,\n","            decay_rate=LR_DECAY_RATE,\n","        )\n","    elif LR_DECAY_TYPE == 'cosine':\n","        lr = keras.experimental.CosineDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=MAX_STEPS - num_wait_steps - num_warmup_steps,\n","            alpha=0.0\n","        )\n","    else:\n","        raise ValueError(f'unknown lr_decay_type in py')\n","    return lr\n","'''\n","\n","import math\n","def lr_lambda(current_step):\n","    if current_step < 0:\n","        return float(current_step) / float(max(1, 0))\n","\n","    progress = float(current_step - 0) / \\\n","               float(max(1, 10 - 0))\n","    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress)))\n","\n","\n","if __name__ == '__main__':\n","    for i in range(10):\n","        print(lr_lambda(i))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.0\n","0.9755282581475768\n","0.9045084971874737\n","0.7938926261462366\n","0.6545084971874737\n","0.5\n","0.34549150281252633\n","0.2061073738537635\n","0.09549150281252633\n","0.024471741852423234\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Bd-iazA7seK","cellView":"form"},"source":["#@title C Dataset MLP\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# import config\n","import sys  \n","sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL')\n","\n","# from Self_augment import RandAugment\n","\n","\n","def normalize_image(img, label):\n","    '''\n","    图片的归一化\n","    :param img:\n","    :param label:\n","    :return:\n","    '''\n","    return tf.cast(img, tf.float32) / 255.0, label\n","\n","\n","# 制作有标签的数据集\n","def label_image(img_file, label):\n","    '''\n","    获取图片，对图片做水平翻转 随机剪裁等， label变为onehot\n","    :param img_file:\n","    :param label:\n","    :return:\n","    '''\n","    # 对图片的处理\n","    img = tf.io.read_file(img_file)\n","    # img = tf.image.grayscale_to_rgb(img, name=None)\n","    # img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE, 3] )\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.random_flip_left_right(img)\n","    img = tf.image.rot90(img) \n","    img = tf.image.random_flip_up_down(img)\n","    img = tf.image.resize(img, (IMG_SIZE + 5, IMG_SIZE + 5))\n","    img = tf.image.random_crop(img, (IMG_SIZE, IMG_SIZE, 3))\n","    img = tf.cast(img, DTYPE) / 255.0\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.0740, 0.0740, 0.0740], dtype=DTYPE), axis=0)\n","    img = (img-mean)/std\n","\n","\n","    # 对标签的处理\n","    label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","    label = tf.cast(label, dtype=DTYPE)\n","\n","    return {'images': img, 'labels': label}\n","\n","\n","# 制作无标签的数据集\n","def unlabel_image(img_file, label):\n","    '''\n","    处理无标签数据\n","    :param img_file:\n","    :param label:\n","    :return: 两张图片，一张经过轻微变换后的图片称为ori_image 一张经过较为剧烈变化后的图片，称为aug_images\n","    '''\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","    # aug_image = mask_label(img)\n","    aug_image = aug.distort(img)\n","    \n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, DTYPE) / 255.0\n","    ori_image = tf.cast(ori_image, DTYPE) / 255.0\n","\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.2153,0.2153,0.2153], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.2196, 0.2196, 0.2196], dtype=DTYPE), axis=0)\n","\n","    aug_image = (aug_image-mean)/std\n","    ori_image = (ori_image-mean)/std\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n","def merge_dataset(label_data, unlabel_data):\n","    return label_data['images'], label_data['labels'], unlabel_data['ori_images'], unlabel_data['aug_images']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZIQ-DgO_c1b"},"source":["# dfc_['label'].loc[(dfc_.label != 'Normal') ] = \"Tumor\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k0AUC5LDAt1-"},"source":["file_paths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEB-BzfizIZI"},"source":["from google.colab import files\n","files.download('/content/drive/MyDrive/Thesis') \n","files.download('/content/drive/MyDrive/Colab Notebooks') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Fh_q8bqefz7"},"source":["import pathlib\n","dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n","data_dir = pathlib.Path(data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wgw03pkP252","executionInfo":{"status":"ok","timestamp":1629994585132,"user_tz":-60,"elapsed":299,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"7bc4347f-42d9-4c77-f239-32219da7c304"},"source":["data_dir"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('/root/.keras/datasets/flower_photos.tgz')"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EozRWcdqO9u1","executionInfo":{"status":"ok","timestamp":1629994835415,"user_tz":-60,"elapsed":25139,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"0ccb521f-a2fa-4ade-bfd2-ca9f5ac14617"},"source":["import pathlib\n","dataset_url = 'https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz'\n","data_dir = tf.keras.utils.get_file('oxford-102-flowers', origin=dataset_url, untar=True)\n","data_dir = pathlib.Path(data_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz\n","345243648/345236087 [==============================] - 24s 0us/step\n","345251840/345236087 [==============================] - 24s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fo82VxQXS7Iu","executionInfo":{"status":"ok","timestamp":1629995505797,"user_tz":-60,"elapsed":1473,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"0c0a3252-6bd1-40be-cbe6-0cd111646d3d"},"source":["!curl -s https://course.fast.ai/setup/colab | bash"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bash: line 1: syntax error near unexpected token `newline'\n","bash: line 1: `<!DOCTYPE html>'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"IdJdCX_0Qpu7","executionInfo":{"status":"error","timestamp":1629995636913,"user_tz":-60,"elapsed":348,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"5c5e8360-9480-4ecf-ac37-f98a895552d7"},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","from fastai import *\n","from fastai.basics import *\n","from fastai.vision import *\n","from fastai.metrics import *\n","from fastai.callbacks.hooks import *\n","from fastai.utils.mem import *\n","%matplotlib inline\n","path = untar_data(dataset_url)\n","path.ls()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-6f1050842184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastai.utils.mem'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"x9pB2wwLfx7W"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ukVwx0aiZX6"},"source":["# x_train = \n","# for i in range(-,lrn(list(data_dir.glob('*/*'))):\n","# (list(data_dir.glob('*/*')))\n","dataF =list(data_dir.glob('*/*'))\n","random.shuffle(dataF)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"ZIUO3rpnRHcn","executionInfo":{"status":"error","timestamp":1629994975978,"user_tz":-60,"elapsed":420,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"bfc8c372-c3a7-41eb-f156-f9a61d6fc38b"},"source":["data_dir.ls()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-90a92481ab1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'ls'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"R7FvdkkluKXA","executionInfo":{"status":"ok","timestamp":1629994892701,"user_tz":-60,"elapsed":342,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"853657aa-20ea-4998-da33-ff76a90815b7"},"source":["\n","str(dataF[0]).split(\"/root/.keras/datasets/oxford-102-flowers/\")[1].split(\"/\")[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'jpg'"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"BdYXIzfWu-mx"},"source":["h = int(len(dataF)*0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7SBEKdOnXlC"},"source":["x = []\n","y = []\n","for i in range(0, len(dataF)):\n","  x.append(str(dataF[0]))\n","  y.append(str(dataF[0]).split(\"/root/.keras/datasets/flower_photos/\")[1].split(\"/\")[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bods-Fe8tvQn"},"source":["y.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-cXtw3Ciezim"},"source":["roses = list(data_dir.glob('roses/*'))\n","PIL.Image.open('/root/.keras/datasets/flower_photos/roses/14001990976_bd2da42dbc.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Mn1VQd5Q7seL","cellView":"form","executionInfo":{"status":"error","timestamp":1629008494125,"user_tz":-60,"elapsed":9488720,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"981003a6-2780-47f3-db26-fe51a7c2e14b"},"source":["#@title C MLPTrain\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import tensorflow_addons as tfa\n","# from WideResnet import WideResnet\n","from copy import deepcopy\n","import sklearn\n","from sklearn import preprocessing\n","\n","# import config\n","# from Model import Wrn28k\n","# from UdaCrossEntroy import UdaCrossEntroy\n","# from learningRate import LearningRate\n","# from Dataset import label_image\n","# from Dataset import unlabel_image\n","# from Dataset import merge_dataset\n","# from test import test\n","\n","\n","def my_update(model, model_):\n","    for i in range(len(model_)):\n","        model.weights[i] = model.weights[i].assign(\n","            model.weights[i]*(1-EMA)+model_[i]*EMA)\n","    model_ = deepcopy(model.weights)\n","    return model, model_\n","\n","\n","if __name__ == '__main__':\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","    # 有标签的数据集 batch_size=BATCH_SIZE\n","    # df_label = pd.read_csv(LABEL_FILE_PATH)\n","    le = preprocessing.LabelEncoder()\n","    y = le.fit_transform(y)\n","\n","    u_file_paths = dfUno['image'].values\n","    u_labels =  dfUno['image'].values\n","    # for i in range(0, len(dfUno)): \n","    #     u_file_paths.append(dfUno.loc[i])\n","    #     u_labels.append(dfUno.loc[i])\n","\n","    # for i in range(0, len(data)): \n","    #   path = root + data[\"fullPath\"][i]#2499\n","    #   path = path.replace('\\\\', '/')\n","    #   path = path.replace('.png', '.jpg')\n","    #   u_file_paths.append(path)\n","    #   if data[\"Status\"][i] ==\"Cancer\":\n","    #     u_labels.append(3)\n","    #   elif data[\"Status\"][i] == \"Normal\":\n","    #     u_labels.append(1)\n","    #   elif data[\"Status\"][i] == \"Benign\" :\n","    #     u_labels.append(2)\n","  \n","    # train_dfc_ = dfc_[:int(len(dfc_)*0.7)] \n","    # test_dfc_ = dfc_[-int(len(dfc_)*0.7):]\n","    # t_file_paths = test_dfc_['image'].values\n","    # t_labels = test_dfc_['label'].values\n","    # file_paths = train_dfc_['image'].values\n","    # labels = train_dfc_['label'].values\n","\n","    file_paths = x[:int(len(dfc_)*0.7)] \n","    labels = y[:int(len(dfc_)*0.7)] \n","    t_file_paths = x[-int(len(dfc_)*0.7):]\n","    t_labels = y[-int(len(dfc_)*0.7):]\n","\n","\n","    ds_label_train = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n","    ds_label_train = ds_label_train \\\n","        .map(label_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=500) \\\n","        .batch(BATCH_SIZE, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 无标签的数据集 batch_size=BATCH_SIZE*UDA_DATA\n","    # df_unlabel = pd.read_csv(UNLABEL_FILE_PATH)\n","    # file_paths = df_unlabel['name'].values\n","    # labels = df_unlabel['label'].values\n","    ds_unlabel_train = tf.data.Dataset.from_tensor_slices((u_file_paths, u_labels))\n","    ds_unlabel_train = ds_unlabel_train \\\n","        .map(unlabel_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=500) \\\n","        .batch(BATCH_SIZE * UDA_DATA, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 将有标签数据和无标签数据整合成最终的数据形式\n","    ds_train = tf.data.Dataset.zip((ds_label_train, ds_unlabel_train))\n","    ds_train = ds_train.map(merge_dataset)\n","\n","    # 构建teacher模型\n","    if TEA_CONTINUE:\n","        print('continue teacher training')\n","        teacher = modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","        teacher.load_weights(TEA_LOAD_PATH)\n","        teacher.training = True\n","    else:\n","        # teacher = Wrn28k(num_inp_filters=3, k=2)\n","        teacher =  modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","\n","    # 构建student模型\n","    if STD_CONTINUE:\n","        print('continue student training')\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape,  pretrained=False)\n","        student.load_weights(STD_LOAD_PATH)\n","        student.training = True\n","        student = tf.saved_model.load(STD_LOAD_PATH)\n","    else:\n","        # student = Wrn28k(num_inp_filters=3, k=2)\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","    student_ = student.weights\n","\n","    # 定义teacher的损失函数，损失函数之一为UdaCrossEntroy\n","    mpl_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","    # 定义student的损失函数， PS：teacher的损失函数为UdaCrossEntroy\n","    s_unlabel_loss = tf.losses.CategoricalCrossentropy(\n","        label_smoothing=LABEL_SMOOTHING,\n","        from_logits=True,\n","        reduction=tf.keras.losses.Reduction.NONE,\n","    )\n","\n","    s_label_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.keras.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","\n","    # 定义teacher的学习率\n","    Tea_lr_fun = LearningRate(\n","        TEACHER_LR,\n","        TEACHER_LR_WARMUP_STEPS,\n","        TEACHER_NUM_WAIT_STEPS\n","    )\n","    # 定义student的学习率\n","    Std_lr_fun = LearningRate(\n","        STUDENT_LR,\n","        STUDENT_LR_WARMUP_STEPS,\n","        STUDENT_LR_WAIT_STEPS\n","    )\n","\n","    global_step = 62*CONTINUE_EPOCH\n","    print(f'start training from global step {global_step}......')\n","    TBacc = 0.78\n","    Tacc = 0\n","    SBacc = 0.31\n","    Sacc = 0\n","    epochs = MAX_EPOCHS - CONTINUE_EPOCH\n","    for epoch in range(epochs):\n","        TLOSS = 0\n","        TLOSS_1 = 0\n","        TLOSS_2 = 0\n","        TLOSS_3 = 0\n","        SLOSS = 0\n","        for batch_idx, (l_images, l_labels, ori_images, aug_images) in enumerate(ds_train):\n","            global_step += 1\n","            all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [15, 32, 32, 3]\n","            u_aug_and_l_images = tf.concat([aug_images, l_images], axis=0)\n","            # step1：经过teacher，得到输出\n","            with tf.GradientTape() as t_tape:\n","                output = teacher(all_images)  # shape=[15, 10]\n","                logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, global_step)\n","            # step2：1st call student -----------------------------\n","            with tf.GradientTape() as s_tape:\n","                logits['s_on_aug_and_l'] = student(u_aug_and_l_images)  # shape=[8, 10]\n","                logits['s_on_u'], logits['s_on_l_old'] = tf.split(\n","                    logits['s_on_aug_and_l'],\n","                    [aug_images.shape[0], l_images.shape[0]],\n","                    axis=0\n","                )\n","                cross_entroy['s_on_u'] = s_unlabel_loss(\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], -1)),\n","                    y_pred=logits['s_on_u']\n","                )\n","                # 计算损失函数\n","                cross_entroy['s_on_u'] = tf.reduce_sum(cross_entroy['s_on_u']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=tf.float32)\n","                SLOSS += cross_entroy['s_on_u']\n","                # for taylor\n","                cross_entroy['s_on_l_old'] = s_label_loss(\n","                    y_true=labels['l'],\n","                    y_pred=logits['s_on_l_old']\n","                )\n","\n","                cross_entroy['s_on_l_old'] = tf.reduce_sum(cross_entroy['s_on_l_old']) / \\\n","                                             tf.convert_to_tensor(BATCH_SIZE, dtype=tf.float32)\n","            # 反向传播，更新student的参数-------\n","            StudentLR = Std_lr_fun.__call__(global_step=global_step)\n","            StdOptim = keras.optimizers.SGD(\n","                learning_rate=StudentLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # StdOptim = keras.optimizers.Adam(learning_rate=StudentLR)\n","            GStud_unlabel = s_tape.gradient(cross_entroy['s_on_u'], student.trainable_variables)\n","            GStud_unlabel, _ = tf.clip_by_global_norm(GStud_unlabel, GRAD_BOUND)\n","            StdOptim.apply_gradients(zip(GStud_unlabel, student.trainable_variables))\n","            # 如何更新参数\n","            student, student_ = my_update(student, student_)\n","\n","            # step3: 2nd call student ------------------------------\n","            logits['s_on_l_new'] = student(l_images)\n","            cross_entroy['s_on_l_new'] = s_label_loss(\n","                y_true=labels['l'],\n","                y_pred=logits['s_on_l_new']\n","            )\n","            cross_entroy['s_on_l_new'] = tf.reduce_sum(cross_entroy['s_on_l_new']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE, dtype=DTYPE)\n","            dot_product = cross_entroy['s_on_l_new'] - cross_entroy['s_on_l_old']\n","            limit = 3.0**(0.5)\n","            moving_dot_product = tf.random_uniform_initializer(minval=-limit, maxval=limit)(shape=dot_product.shape)\n","            moving_dot_product = tf.Variable(initial_value=moving_dot_product, trainable=False, dtype=DTYPE)\n","            moving_dot_product_update = moving_dot_product.assign_sub(0.01 * (moving_dot_product - dot_product))\n","            dot_product = dot_product - moving_dot_product\n","            dot_product = tf.stop_gradient(dot_product)\n","            # step4: 求teacher的损失函数\n","            with t_tape:\n","                # label = tf.math.argmax(tf.nn.softmax(logits['aug'], axis=-1), axis=-1)\n","                # label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","                cross_entroy['mpl'] = mpl_loss(\n","                    # y_true=tf.stop_gradient(label),\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], axis=-1)),\n","                    y_pred=logits['aug']\n","                )  # 恒正\n","                cross_entroy['mpl'] = tf.reduce_sum(cross_entroy['mpl']) / \\\n","                                      tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=DTYPE)\n","                uda_weight = UDA_WEIGHT * tf.math.minimum(\n","                    1., tf.cast(global_step, DTYPE) / float(UDA_STEPS)\n","                )\n","                # if StudentLR == 0:\n","                #     dot_product = 0\n","                teacher_loss = cross_entroy['u'] * uda_weight + \\\n","                               cross_entroy['l'] + \\\n","                               cross_entroy['mpl'] * dot_product\n","\n","                TLOSS += teacher_loss\n","                TLOSS_1 += (cross_entroy['u'] * uda_weight)\n","                TLOSS_2 += cross_entroy['l']\n","                TLOSS_3 += cross_entroy['mpl'] * dot_product\n","            # 反向传播，更新teacher的参数-------\n","            TeacherLR = Tea_lr_fun.__call__(global_step=global_step)\n","            TeaOptim = keras.optimizers.SGD(\n","                learning_rate=TeacherLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # TeaOptim = keras.optimizers.Adam(learning_rate=TeacherLR)\n","            GTea = t_tape.gradient(teacher_loss, teacher.trainable_variables)\n","            GTea, _ = tf.clip_by_global_norm(GTea, GRAD_BOUND)\n","            TeaOptim.apply_gradients(zip(GTea, teacher.trainable_variables))\n","\n","            if (batch_idx + 1) % LOG_EVERY == 0:\n","                TLOSS = TLOSS / LOG_EVERY\n","                TLOSS_1 = TLOSS_1 / LOG_EVERY\n","                TLOSS_2 = TLOSS_2 / LOG_EVERY\n","                TLOSS_3 = TLOSS_3 / LOG_EVERY\n","                SLOSS = SLOSS / LOG_EVERY\n","                print(f'global: %4d' % global_step + ',[epoch:%4d/' % (epoch+CONTINUE_EPOCH) + 'EPOCH: %4d] \\t' % epochs\n","                      + '[U:%.4f' % (TLOSS_1) + ', L:%.4f' % (TLOSS_2) + ', M:%.4f' % (\n","                          TLOSS_3) + ']' + '[TLoss: %.4f]' % TLOSS + '/[SLoss: %.4f]' % SLOSS\n","                      + '\\t[TLR: %.6f' % TeacherLR + ']/[SLR: %.6f]' % StudentLR)\n","                TLOSS = 0\n","                TLOSS_1 = 0\n","                TLOSS_2 = 0\n","                TLOSS_3 = 0\n","                SLOSS = 0\n","        # 测试teacher在test上的acc\n","        if epoch % 5 == 0:\n","            Tacc = test(teacher, t_file_paths, t_labels)\n","            print(f'testing teacher model ... acc: {Tacc}')\n","        # 测试student在test上的acc，当student开始训练的时候\n","        if (StudentLR > 0) and (epoch % 5 == 0):\n","            Sacc = test(student, t_file_paths, t_labels)\n","            print(f'testing ... acc: {Sacc}')\n","        # 保存weights\n","        if Tacc > TBacc:\n","            Tsave_path = TEA_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            teacher.save_weights(Tsave_path)\n","            # tf.saved_model.save(teacher, Tsave_path)\n","            TBacc = Tacc\n","            print(f'saving for TBacc {TBacc}, Tpath:{Tsave_path}')\n","        if Sacc > SBacc:\n","            Ssave_path = STD_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            student.save_weights(Ssave_path)\n","            SBacc = Sacc\n","            print(f'saving for SBacc {SBacc}, Spath:{Ssave_path}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["start training from global step 54870......\n","global: 54890,[epoch: 885/EPOCH: 1035] \t[U:0.0000, L:1.5668, M:0.2967][TLoss: 1.8635]/[SLoss: 1.6094]\t[TLR: 0.000180]/[SLR: 0.000172]\n","global: 54910,[epoch: 885/EPOCH: 1035] \t[U:0.0000, L:1.5592, M:-0.3291][TLoss: 1.2301]/[SLoss: 1.6094]\t[TLR: 0.000180]/[SLR: 0.000172]\n","global: 54930,[epoch: 885/EPOCH: 1035] \t[U:0.0000, L:1.5516, M:0.2249][TLoss: 1.7766]/[SLoss: 1.6094]\t[TLR: 0.000180]/[SLR: 0.000172]\n","global: 54950,[epoch: 885/EPOCH: 1035] \t[U:0.0000, L:1.5441, M:-0.2492][TLoss: 1.2950]/[SLoss: 1.6094]\t[TLR: 0.000180]/[SLR: 0.000172]\n","global: 54970,[epoch: 885/EPOCH: 1035] \t[U:0.0000, L:1.5367, M:-0.1622][TLoss: 1.3745]/[SLoss: 1.6094]\t[TLR: 0.000180]/[SLR: 0.000172]\n","testing teacher model ... acc: 100.0\n","testing ... acc: 0.0\n","saving for TBacc 100.0, Tpath:/content/drive/MyDrive/Thesis/weights/b4PST3\n","global: 54991,[epoch: 886/EPOCH: 1035] \t[U:0.0000, L:1.5289, M:0.2949][TLoss: 1.8239]/[SLoss: 1.6094]\t[TLR: 0.000180]/[SLR: 0.000172]\n","global: 55011,[epoch: 886/EPOCH: 1035] \t[U:0.0000, L:1.5215, M:0.2297][TLoss: 1.7513]/[SLoss: 1.6094]\t[TLR: 0.000179]/[SLR: 0.000171]\n","global: 55031,[epoch: 886/EPOCH: 1035] \t[U:0.0000, L:1.5142, M:-0.0726][TLoss: 1.4417]/[SLoss: 1.6094]\t[TLR: 0.000179]/[SLR: 0.000171]\n","global: 55051,[epoch: 886/EPOCH: 1035] \t[U:0.0000, L:1.5069, M:-0.0646][TLoss: 1.4423]/[SLoss: 1.6094]\t[TLR: 0.000179]/[SLR: 0.000171]\n","global: 55071,[epoch: 886/EPOCH: 1035] \t[U:0.0000, L:1.4997, M:0.5864][TLoss: 2.0861]/[SLoss: 1.6094]\t[TLR: 0.000179]/[SLR: 0.000171]\n","global: 55092,[epoch: 887/EPOCH: 1035] \t[U:0.0000, L:1.4921, M:-0.3567][TLoss: 1.1354]/[SLoss: 1.6094]\t[TLR: 0.000179]/[SLR: 0.000171]\n","global: 55112,[epoch: 887/EPOCH: 1035] \t[U:0.0000, L:1.4850, M:0.0932][TLoss: 1.5782]/[SLoss: 1.6094]\t[TLR: 0.000179]/[SLR: 0.000171]\n","global: 55132,[epoch: 887/EPOCH: 1035] \t[U:0.0000, L:1.4779, M:0.1754][TLoss: 1.6533]/[SLoss: 1.6094]\t[TLR: 0.000179]/[SLR: 0.000170]\n","global: 55152,[epoch: 887/EPOCH: 1035] \t[U:0.0000, L:1.4708, M:-0.3383][TLoss: 1.1324]/[SLoss: 1.6094]\t[TLR: 0.000178]/[SLR: 0.000170]\n","global: 55172,[epoch: 887/EPOCH: 1035] \t[U:0.0000, L:1.4637, M:0.1547][TLoss: 1.6184]/[SLoss: 1.6094]\t[TLR: 0.000178]/[SLR: 0.000170]\n","global: 55193,[epoch: 888/EPOCH: 1035] \t[U:0.0000, L:1.4564, M:0.7255][TLoss: 2.1819]/[SLoss: 1.6094]\t[TLR: 0.000178]/[SLR: 0.000170]\n","global: 55213,[epoch: 888/EPOCH: 1035] \t[U:0.0000, L:1.4494, M:-0.1557][TLoss: 1.2937]/[SLoss: 1.6094]\t[TLR: 0.000178]/[SLR: 0.000170]\n","global: 55233,[epoch: 888/EPOCH: 1035] \t[U:0.0000, L:1.4425, M:0.2519][TLoss: 1.6944]/[SLoss: 1.6094]\t[TLR: 0.000178]/[SLR: 0.000169]\n","global: 55253,[epoch: 888/EPOCH: 1035] \t[U:0.0000, L:1.4356, M:-0.0014][TLoss: 1.4342]/[SLoss: 1.6094]\t[TLR: 0.000178]/[SLR: 0.000169]\n","global: 55273,[epoch: 888/EPOCH: 1035] \t[U:0.0000, L:1.4288, M:-0.0123][TLoss: 1.4164]/[SLoss: 1.6094]\t[TLR: 0.000177]/[SLR: 0.000169]\n","global: 55294,[epoch: 889/EPOCH: 1035] \t[U:0.0000, L:1.4216, M:-0.3716][TLoss: 1.0500]/[SLoss: 1.6094]\t[TLR: 0.000177]/[SLR: 0.000169]\n","global: 55314,[epoch: 889/EPOCH: 1035] \t[U:0.0000, L:1.4148, M:-0.7894][TLoss: 0.6254]/[SLoss: 1.6094]\t[TLR: 0.000177]/[SLR: 0.000169]\n","global: 55334,[epoch: 889/EPOCH: 1035] \t[U:0.0000, L:1.4081, M:-0.1382][TLoss: 1.2699]/[SLoss: 1.6094]\t[TLR: 0.000177]/[SLR: 0.000169]\n","global: 55354,[epoch: 889/EPOCH: 1035] \t[U:0.0000, L:1.4014, M:-0.1386][TLoss: 1.2628]/[SLoss: 1.6094]\t[TLR: 0.000177]/[SLR: 0.000168]\n","global: 55374,[epoch: 889/EPOCH: 1035] \t[U:0.0000, L:1.3947, M:0.1918][TLoss: 1.5865]/[SLoss: 1.6094]\t[TLR: 0.000177]/[SLR: 0.000168]\n","global: 55395,[epoch: 890/EPOCH: 1035] \t[U:0.0000, L:1.3877, M:-0.1544][TLoss: 1.2333]/[SLoss: 1.6094]\t[TLR: 0.000176]/[SLR: 0.000168]\n","global: 55415,[epoch: 890/EPOCH: 1035] \t[U:0.0000, L:1.3811, M:-0.0233][TLoss: 1.3578]/[SLoss: 1.6094]\t[TLR: 0.000176]/[SLR: 0.000168]\n","global: 55435,[epoch: 890/EPOCH: 1035] \t[U:0.0000, L:1.3745, M:0.3822][TLoss: 1.7567]/[SLoss: 1.6094]\t[TLR: 0.000176]/[SLR: 0.000168]\n","global: 55455,[epoch: 890/EPOCH: 1035] \t[U:0.0000, L:1.3680, M:0.6002][TLoss: 1.9682]/[SLoss: 1.6094]\t[TLR: 0.000176]/[SLR: 0.000168]\n","global: 55475,[epoch: 890/EPOCH: 1035] \t[U:0.0000, L:1.3615, M:-0.0969][TLoss: 1.2646]/[SLoss: 1.6094]\t[TLR: 0.000176]/[SLR: 0.000167]\n","testing teacher model ... acc: 100.0\n","testing ... acc: 0.0\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-596014dc2a6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# step2：1st call student -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's_on_aug_and_l'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_aug_and_l_images\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape=[8, 10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 logits['s_on_u'], logits['s_on_l_old'] = tf.split(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's_on_aug_and_l'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1005\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1006\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \"\"\"\n\u001b[1;32m    415\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 416\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1005\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1006\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \"\"\"\n\u001b[1;32m    415\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 416\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1005\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1006\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2601\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2603\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2604\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2605\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292},"id":"8Nisobn6eV-b","executionInfo":{"status":"error","timestamp":1629008607491,"user_tz":-60,"elapsed":302,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"2a70b320-9579-41c5-e832-6c51bbee1d97"},"source":["labels['l']"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-0aed3b128a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'values'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTPYC376d9p8","executionInfo":{"status":"ok","timestamp":1629008795330,"user_tz":-60,"elapsed":109147,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"}},"outputId":"7888498c-2b6a-4365-ee9b-808c75137f45"},"source":["test(teacher, x[:int(len(dfc_)*0.7)] , y[:int(len(dfc_)*0.7)])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100.0"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"XcHP84E8rLR7"},"source":["# *`Refrence`*"]},{"cell_type":"markdown","metadata":{"id":"Gj40a-mBA23L"},"source":["https://github.com/YanYan0716/MPL"]},{"cell_type":"markdown","metadata":{"id":"hs2MGO3iBGja"},"source":["https://github.com/zhoudaxia233/EfficientUnet"]}]}
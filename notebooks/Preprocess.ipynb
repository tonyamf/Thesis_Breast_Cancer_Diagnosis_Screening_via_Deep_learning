{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Preprocess.ipynb","provenance":[{"file_id":"1yhEeWMmcKO-sOIAo3tTeAQiOdbPwQPgu","timestamp":1627719755769}],"collapsed_sections":["Bf0ES9ZmG0DP","iEg4Ykf4Vyd6","ACEvRQcgbbKN","CV0-LhNMBBSA","m2nMbS40gHlM","-j9x5-Py5RcX"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNbWMEwcXI-C","executionInfo":{"elapsed":4053,"status":"ok","timestamp":1628219421662,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"1a7a9f94-1c5f-4da9-da5b-002028a1414a"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.13.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"dExRVQI8aLbl"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"K_X6Tj9TaBOF","executionInfo":{"elapsed":2082,"status":"ok","timestamp":1628219575226,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"3963a234-fe9a-42cc-8ca6-854f0a3d4888"},"source":["#@title Libraries\n","%pylab inline\n","import imutils\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import pylab as pylab\n","import matplotlib.image as mpimg\n","from PIL import Image as im\n","# import segmentation_models_pytorch as smp\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from scipy import ndimage, misc\n","from skimage.filters import threshold_otsu\n","from skimage.segmentation import clear_border\n","from skimage.measure import label, regionprops\n","from skimage.morphology import closing, square\n","from skimage.color import label2rgb\n","import matplotlib.patches as mpatches\n","from scipy.misc import face\n","from scipy.signal.signaltools import wiener\n","import sys\n","import numpy as np\n","import skimage.color\n","import skimage.filters\n","import skimage.io\n","import skimage.viewer\n","from skimage import feature, io, color, filters\n","from skimage.transform import hough_line, hough_line_peaks\n","from skimage.feature import canny\n","from skimage.filters import sobel\n","from skimage.draw import polygon\n","from skimage import exposure\n","from skimage.transform import resize\n","from PIL import Image\n","import scipy.ndimage as snd\n","#from meta-pseudo-labels.\n","from random import seed\n","from random import random\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: UserWarning: Viewer requires Qt\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bf0ES9ZmG0DP"},"source":["# Mount file syste,"]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"u2yW4Ik2Gy22","executionInfo":{"elapsed":1722,"status":"error","timestamp":1628220550573,"user":{"displayName":"Antonio Franco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpZWpPk6ug7_9xt90pVX8z1iULSGekWi2cTqST=s64","userId":"09756366898940169615"},"user_tz":-60},"outputId":"5de773f9-41bc-46ba-af33-e2e0dd428a80"},"source":["#@title Driver mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b12896192220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Driver mount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"w7kfgMnUadk0"},"source":["# Import data"]},{"cell_type":"code","metadata":{"id":"X30cgZKOacpN"},"source":["\n","# import os\n","# input_dir = \"/content/drive/MyDrive/Thesis/MINI-DDSM-Complete-JPEG-8/Data.xlsx\"\n","root = '\\\\content\\\\drive\\\\MyDrive\\\\Thesis\\\\MINI-DDSM-Complete-JPEG-8\\\\'\n","# dfAll = pd.read_excel(input_dir)\n","# dfAll.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"mToRCE_3ACK8","executionInfo":{"elapsed":526,"status":"error","timestamp":1628219586509,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"96780497-1cdf-46e1-bc3d-18f9b187a445"},"source":["data = pd.read_csv('/content/drive/MyDrive/Thesis/pos/data.csv')"],"execution_count":null,"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-329df88fe703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Thesis/pos/data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Thesis/pos/data.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"iEg4Ykf4Vyd6"},"source":["# Enchecement"]},{"cell_type":"code","metadata":{"id":"1FTwjLg3ESUy"},"source":["#@title contrast_streching\n","def contrast_streching(img):\n","  img1 = img\n","  minmax_img = np.zeros((img1.shape[0],img1.shape[1]),dtype = 'uint8')\n","  for i in range(img1.shape[0]):\n","      for j in range(img1.shape[1]):\n","          minmax_img[i,j] = 255*(img1[i,j]-np.min(img1))/(np.max(img1)-np.min(img1))\n","  \n","  return minmax_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"tZJv2OGmV1pu"},"source":["#@title morphological_enhancement\n","def morphological_enhancement(imag):\n","  # imag = cv2.imread(path, 0)\n","  rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n","  tophat = cv2.morphologyEx(imag, cv2.MORPH_TOPHAT, rectKernel)\n","  blackhat = cv2.morphologyEx(imag, cv2.MORPH_BLACKHAT, rectKernel)\n","  imag = imag + tophat - blackhat\n","\n","\n","  return imag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"fAE_0Wg24lgc"},"source":["#@title clahe\n","def clahe(img, i):\n","\n","  clahe = cv2.createCLAHE(clipLimit=i, tileGridSize=( grid_l,  grid_w))\n","  cl1 = clahe.apply(img)\n","  return cl1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACEvRQcgbbKN"},"source":["# Image pre processing"]},{"cell_type":"code","metadata":{"cellView":"form","id":"qaZRfCZGou7i"},"source":["#@title Mask_label\n","def mask_label(image):\n","  #grid = int((image.shape[0]+image.shape[1])/1000)*3\n","  \n","  # img_clahe = clahe(image)\n","  # img_md_n = ndimage.median_filter(image, 3)\n","  # img_md = cv2.dilate(image, None, iterations=3)\n","\n","  img_md = cv2.medianBlur(image, 7)\n","  thresh = cv2.adaptiveThreshold(img_md, 255,\n","    cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 10)\n","  thresh = cv2.dilate(thresh, None, iterations=5)\n","  cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n","  cv2.CHAIN_APPROX_SIMPLE)[0]\n","  for cnt in cnts: \n","    cv2.drawContours(thresh, [cnt], 0, 255,-1)\n","\n","  cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n","  cv2.CHAIN_APPROX_SIMPLE)[0]\n","  for cnt in cnts: \n","      cv2.drawContours(image, [cnt], 0, 0,-1)\n","\n","  img_md = ndimage.median_filter(image, 7)\n","  # img_md = cv2.GaussianBlur(img,(5,5),0)\n","\n","  left_nonzero = cv2.countNonZero(image[:, 0:int(image.shape[1]/2)])\n","  right_nonzero = cv2.countNonZero(image[:, int(image.shape[1]/2):])\n","  # flip_n = left_nonzero\n","  n =  np.median(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > 0] )\n","  n_l = len(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > n] )\n","  \n","  if(left_nonzero < right_nonzero):\n","    # flip_n = right_nonzero\n","    n =  np.median(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","    n_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > n] )\n","  #     print('wtf')\n","  #     flip = True\n","  #     image = cv2.flip(image, 1)\n","  # img_md = cv2.dilate(img_md, None, iterations=7)\n","  # image_eq = cv2.equalizeHist(img_md)\n","  # ret,thresh1 = cv2.threshold(image_eq, 0,255, cv2.THRESH_OTSU)\n","  # np.mean(cv2.countNonZero(image[:, 0:int(image.shape[1]/2)]))\n","  # np.where(np.nonzero(image[:, 0:int(image.shape[1]/2)]\n","\n","  ret,thresh  = cv2.threshold(img_md,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n","  thresh = cv2.dilate(thresh, None, iterations=5)\n","  # contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n","  # contours2, hier2 = cv2.findContours(thresh2,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n","  # cnts = cv2.findContours(thresh1.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","  # cnts = imutils.grab_contours(cnts)\n","  # contours,hierarchy = cv2.findContours(thresh2, 1, 2)\n","\n","  # find contours in thresholded image, then grab the largest\n","  # one\n","  cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n","    cv2.CHAIN_APPROX_SIMPLE)\n","  cnts = imutils.grab_contours(cnts)\n","  c = max(cnts, key=cv2.contourArea)\n","  max2 = 0\n","  for cnt in cnts:\n","    if cv2.contourArea(cnt) > max2 and cv2.contourArea(cnt) < cv2.contourArea(c):\n","      max2 = cv2.contourArea(cnt)\n","  # # x_m = image.shape[0] - image.shape[0]/5\n","  # # x_n = image.shape[0]/5\n","  # # y_m =  image.shape[1] - image.shape[1]/4.2\n","  # d_image = np.sqrt( (image.shape[0]**2) + (image.shape[1]**2) )\n","  # max1 = 0\n","  # # max2 = 0\n","  # for cnt in contours:\n","  #   if cv2.contourArea(cnt) > max1:\n","  #     max1 = cv2.contourArea(cnt)\n","  # left_nonzero = cv2.countNonZero(image[:, 0:int(image.shape[1]/2)])\n","  # right_nonzero = cv2.countNonZero(image[:, int(image.shape[1]/2):])\n","  # # flip_n = left_nonzero\n","  # n =  np.mean(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > 0] )\n","  # n_l = len(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > n] )\n","  # # s =  np.mean(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > 0] )\n","  # # s_l = len(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > s] )\n","  # # o =  np.mean(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","  # # o_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > o] )\n","  # if(left_nonzero < right_nonzero):\n","  #   # flip_n = right_nonzero\n","  #   n =  np.mean(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","  #   n_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > n] )\n","    # o =  np.mean(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > 0] )\n","    # o_l = len(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > o] )\n","    # s =  np.mean(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","    # s_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > s] )\n","\n","\n","\n","  # if cv2.countNonZero(thresh) < n_l:\n","  #   pass\n","    # for cnt in cnts: \n","    #   if cv2.contourArea(cnt) < cv2.contourArea(c):\n","    #   #   hausdorff_sd = cv2.createHausdorffDistanceExtractor()\n","    #     (x,y,w,h) = cv2.boundingRect(cnt)\n","    #     cv2.drawContours(img_md, [cnt], 0, 0,-1)\n","\n","  \n","  for cnt in cnts: \n","    if cv2.contourArea(cnt) < max2:\n","      cv2.drawContours(thresh, [cnt], 0, 0,-1)\n","  if cv2.countNonZero(thresh) < n_l:\n","    pass\n","  else:\n","    image[np.where(thresh == 0)] = 0 \n","\n","      # cv2.(img_md,(x,y),(x+w,y+h),0,-1)\n","      # mask = np.zeros((img_md.shape), dtype=np.uint8)\n","      # cv2.fillPoly(img_md, (x,y),(x+w,y+h),0,-1)\n","      # img_md[np.where(mask\n","    # # 4. Calculate the distance between contours\n","    #   if hausdorff_sd.computeDistance(c, cnt) > d_image/5:\n","  # hausdorff_sd = cv2.createHausdorffDistanceExtractor()  \n","  # for x in range(img_md.shape[0]):\n","  #   for y in range(img_md.shape[1]):\n","  #           # for the given pixel at w,h, lets check its value against the threshold\n","            \n","  #     if hausdorff_sd.computeDistance((x,y,1,1) , cnt) > d_image/5:\n","  #       img_md[x, y] = 0\n","        # M = cv2.moments(cnt)\n","        # if M[\"m00\"] < 1:\n","        #   M[\"m00\"] =1\n","        # cX = int(M[\"m10\"] / M[\"m00\"])\n","        # cY = int(M[\"m01\"] / M[\"m00\"])\n","        # (x,y,w,h) = cv2.boundingRect(cnt)\n","        # if cX > x_m or cX < x_n or cY > y_m:\n","      # cv2.rectangle(img_md,(x,y),(x+w,y+h),0,-1)\n","\n","\n","  # for cnt in contours:\n","  #   if cv2.contourArea(cnt) > max2 and cv2.contourArea(cnt) < max1:\n","  #     max2 = cv2.contourArea(cnt)\n","  # x_m = image.shape[0] - image.shape[0]/5\n","  # x_n = image.shape[0]/5\n","  # y_m =  image.shape[1] - image.shape[1]/4.2\n","  # for cnt in contours: \n","  #   M = cv2.moments(cnt)\n","  #   if M[\"m00\"] < 1:\n","  #     M[\"m00\"] =1\n","  #   cX = int(M[\"m10\"] / M[\"m00\"])\n","  #   cY = int(M[\"m01\"] / M[\"m00\"])\n","  #   (x,y,w,h) = cv2.boundingRect(cnt)\n","  #   if cX > x_m or cX < x_n or cY > y_m:\n","  #     print(x_m, x_n, y_m)\n","  #     print()\n","  #     print(cX, cY)\n","  #     cv2.rectangle(thresh2,(x,y),(x+w,y+h),0,-1)\n","\n","  # max1 = 0\n","  # max2 = 0\n","  # for cnt in contours2:\n","  #   if cv2.contourArea(cnt) > max1:\n","  #     max1 = cv2.contourArea(cnt)\n","\n","  # for cnt in contours2:\n","  #   if cv2.contourArea(cnt) > max2 and cv2.contourArea(cnt) < max1:\n","  #     max2 = cv2.contourArea(cnt)\n","\n","  # for cnt in contours2:      \n","  #     if cv2.contourArea(cnt) < max2 * (max2/max1):\n","  #         (x,y,w,h) = cv2.boundingRect(cnt)\n","  #         cv2.rectangle(thresh2,(x,y),(x+w,y+h),0,-1)\n","\n","  \n","  # lab_val = 255\n","  # kernel_size = 15\n","  # _, mammo_binary = cv2.threshold(img_md, 0, maxval=255, type=cv2.THRESH_BINARY)\n","  # n_labels, img_labeled, lab_stats, _ = cv2.connectedComponentsWithStats(\n","  #     thresh1, connectivity=8, ltype=cv2.CV_32S)\n","  # largest_obj_lab = np.argmax(lab_stats[1:, 4]) + 1\n","  # largest_mask = np.zeros(thresh1.shape, dtype=np.uint8)\n","  # largest_mask[img_labeled == largest_obj_lab] = lab_val\n","  \n","    \n","\n","  # fig, axes = plt.subplots(1, 2, figsize=(15,10))\n","  # fig.tight_layout(pad=3.0)\n","  # axes[0].set_title('Image')\n","  # axes[0].imshow(img_md, cmap=pylab.cm.gray)\n","  # axes[0].axis('on')\n","  # axes[1].set_title('Mask')\n","  # axes[1].imshow(thresh, cmap=pylab.cm.gray)\n","  # axes[1].axis('on')\n","  # plt.show()\n","  # if flip == True:\n","  #   image = cv2.flip(image, 1)\n","\n","  # plt.imshow(img, 'gray')\n","\n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"-f1kEIlmeiH0"},"source":["#@title right_orient_mammogram\n","\n","def right_orient_mammogram(image):\n","    flip = False\n","    left_nonzero = cv2.countNonZero(image[:, 0:int(image.shape[1]/2)])\n","    right_nonzero = cv2.countNonZero(image[:, int(image.shape[1]/2):])\n","    \n","    if(left_nonzero < right_nonzero):\n","        # print('wtf')\n","        flip = True\n","        image = cv2.flip(image, 1)\n","    # print(flip)    \n","    return [image, flip]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"vbh8_RuAzmMU"},"source":["#@title Apply_canny\n","def apply_canny(image):\n","    #img_eq = exposure.equalize_hist(image)\n","    canny_img = canny(image, 6)\n","    return sobel(canny_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"YC7QSQJf0MPQ"},"source":["#@title get_hough_lines\n","\n","def get_hough_lines(canny_img):\n","    h, theta, d = hough_line(canny_img)\n","    lines = list()\n","    # print('\\nAll hough lines')\n","    for _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n","        # print(\"Angle: {:.2f}, Dist: {:.2f}\".format(np.degrees(angle), dist))\n","        x1 = 0\n","        y1 = (dist - x1 * np.cos(angle)) / np.sin(angle)\n","        x2 = canny_img.shape[1]\n","        y2 = (dist - x2 * np.cos(angle)) / np.sin(angle)\n","        lines.append({\n","            'dist': dist,\n","            'angle': np.degrees(angle),\n","            'point1': [x1, y1],\n","            'point2': [x2, y2]\n","        })\n","    \n","    return lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"GKmaOOag0Vhx"},"source":["#@title shortlist_lines\n","def shortlist_lines(lines, max_dist):\n","    MIN_ANGLE = 1\n","    MAX_ANGLE = 30\n","    MIN_DIST  = max_dist/100\n","    MAX_DIST  = max_dist*1.2\n","    \n","    shortlisted_lines = [x for x in lines if \n","                          (x['dist']>=abs(MIN_DIST)) &\n","                          (x['dist']<=abs(MAX_DIST)) &\n","                          (x['angle']>=abs(MIN_ANGLE)) &\n","                          (x['angle']<=abs(MAX_ANGLE))\n","                        ]\n","    # print('\\nShorlisted lines')\n","    # for i in shortlisted_lines:\n","    #     print(\"Angle: {:.2f}, Dist: {:.2f}\".format(i['angle'], i['dist']))\n","        \n","    return shortlisted_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"ezTfIhIY1KyQ"},"source":["#@title remove_pectoral\n","def remove_pectoral(shortlisted_lines):\n","    shortlisted_lines.sort(key = lambda x: x['dist'])\n","    pectoral_line = shortlisted_lines[0].copy()\n","    # shortlisted_lines.pop(0)\n","    d = pectoral_line['dist']\n","    theta = np.radians(pectoral_line['angle'])\n","    \n","    x_intercept = d/np.cos(theta)\n","    y_intercept = d/np.sin(theta)\n","    \n","    return polygon([0, 0, y_intercept], [0, x_intercept, 0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sh6HC6skbk3_"},"source":["#@title breast_snip\n","def breast_snip(image, plotting=True):\n","    # img = io.imread(filename)\n","    # img = color.rgb2gray(img)\n","    # plt.imshow(img, 'gray')\n","    image = mask_label(image)\n","    flip_array = right_orient_mammogram(image)\n","    image = flip_array[0]\n","    # img = clahe(image.copy(), 2)\n","    img = image.copy()\n","    canny_img = apply_canny(img)\n","    lines = get_hough_lines(canny_img)\n","    shortlisted_lines = shortlist_lines(lines, img.shape[0]/1.5)\n","\n","    if plotting:\n","      fig, axes = plt.subplots(1, 4, figsize=(15,10))\n","      fig.tight_layout(pad=3.0)\n","      plt.xlim(0,img.shape[1])\n","      plt.ylim(img.shape[0])\n","      \n","      \n","      axes[0].set_title('Right-oriented mammogram')\n","      axes[0].imshow(img, cmap=pylab.cm.gray)\n","      axes[0].axis('on') \n","      \n","      axes[1].set_title('Hough Lines on Canny Edge img')\n","      axes[1].imshow(canny_img, cmap=pylab.cm.gray)\n","      axes[1].axis('on')\n","      axes[1].set_xlim(0,img.shape[1])\n","      axes[1].set_ylim(img.shape[0])\n","      for line in lines:\n","          axes[1].plot((line['point1'][0],line['point2'][0]), (line['point1'][1],line['point2'][1]), '-r')\n","          \n","      axes[2].set_title('Shortlisted Lines')\n","      axes[2].imshow(canny_img, cmap=pylab.cm.gray)\n","      axes[2].axis('on')\n","      axes[2].set_xlim(0,img.shape[1])\n","      axes[2].set_ylim(img.shape[0])\n","      for line in shortlisted_lines:\n","          axes[2].plot((line['point1'][0],line['point2'][0]), (line['point1'][1],line['point2'][1]), '-r')\n","    if size(shortlisted_lines) > 0:\n","      rr, cc = remove_pectoral(shortlisted_lines)\n","      try:\n","        image[rr, cc] = 0\n","      except:\n","        pass\n","    if plotting:\n","      axes[3].set_title('Pectoral muscle removed')\n","      axes[3].imshow(img, cmap=pylab.cm.gray)\n","      axes[3].axis('on')\n","      plt.show()\n","    if flip_array[1]:\n","      # print('yes')\n","      image = cv2.flip(image, 1)\n","    image = clahe(image, 2)\n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CV0-LhNMBBSA"},"source":["# Train data"]},{"cell_type":"code","metadata":{"cellView":"form","id":"5DEVOhArNWlY"},"source":["#@title check_path\n","\n","def check_path(path):\n","  path = path.replace('\\\\', '/')\n","  try:\n","    # mask_ = imread(path,0)    \n","    im1 = Image.open(path)\n","    #rgb_im = mask_.convert('RGB')\n","    im1.save(path.replace('.png', '.jpg'))\n","\n","    return path.replace('.png', '.jpg')\n","  except :\n","  \n","    try:\n","      # mask_ = imread(path,0) \n","      path = path.replace('MASK', 'Mask')\n","      path = path.replace('.png', '.jpg')   \n","      im1 = Image.open(path)\n","\n","      return path\n","    except :\n","      try:\n","        # mask_ = imread(path,0) \n","        path = path.replace('Mask', 'MASK')\n","        path = path.replace('.png', '.jpg')   \n","        im1 = Image.open(path)\n","\n","        return path\n","      except :\n","        try:\n","          path = path.replace('.png', '.jpg')\n","          return path\n","        except:\n","          pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"FroSgt4FJjn3"},"source":["#@title Variable for mask add\n","i_c = 0\n","i_s = 0\n","# last_mask = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9x0jd0jxvwX"},"source":["#@title add_image_and_mask 3 channels mask\n","def mask_add(img, data, index):\n","  i = index\n","  global i_c\n","  global i_s\n","  path = root + data[\"fullPath\"][i]#2499 2\n","  path = path.replace('\\\\', '/')\n","  path = path.replace('.png', '.jpg')\n","  Main_mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.uint8)\n","  masks = []\n","  if data['Tumour_Contour'][i] != '-':\n","    masks.append(check_path(root + data['Tumour_Contour'][i]))\n","  if data['Tumour_Contour2'][i] != '-':\n","    masks.append(check_path(root + data['Tumour_Contour2'][i]))\n","  if pd.isnull(data[\"Tumour_Contour3\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour3'][i]))\n","  if pd.isnull(data[\"Tumour_Contour4\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour4'][i]))\n","  if pd.isnull(data[\"Tumour_Contour5\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour5'][i]))\n","  if pd.isnull(data[\"Tumour_Contour6\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour6'][i]))\n","  for mask in masks:\n","    ini_img = img.copy()\n","    mask_ = imread(mask)\n","    mask_ = cv2.resize(mask_, (ini_img.shape[0], ini_img.shape[1]))\n","    mask_[np.where(mask_ !=0)] = 255\n","    ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","    contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE) \n","    for cnt in contours:      \n","      (x,y,w,h) = cv2.boundingRect(cnt)\n","      cv2.drawContours(mask_, [cnt], 0,(255, 0, 0),-1)\n","    mask_ = np.expand_dims(resize(mask_, (img.shape[0], img.shape[1]), mode='constant',  \n","                                preserve_range=True), axis=-1)\n","    Main_mask = np.maximum(Main_mask, mask_)\n","    Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 255\n","  cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i_s) + \".jpg\", img)\n","  cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i_s) + \".jpg\", np.squeeze(Main_mask))\n","  dfS.loc[i_s] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i_s) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i_s) + \".jpg\", data['Status'][i]]\n","  i_s = i_s + 1\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciedbWzIsjEw"},"source":["    try:\n","      ret2,thresh2 = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","      cnts = cv2.findContours(thresh2.copy(), cv2.RETR_EXTERNAL,\n","        cv2.CHAIN_APPROX_SIMPLE)\n","      cnts = imutils.grab_contours(cnts)\n","      c = max(cnts, key=cv2.contourArea)\n","      (x,y,w,h) = cv2.boundingRect(c)\n","      # print( np.where(np.nonzero(np.squeeze(Main_mask))))\n","      # ini_img = ini_img * mask_\n","      ini_img[np.where(mask_==0), 0] = 0\n","    \n","      gray = ini_img[y:y+h,x:x+w]\n","\n","\n","      img2 = np.zeros((gray.shape[0], gray.shape[1], 3), dtype=np.uint8)\n","      img2[:,:,0] = gray\n","      img2[:,:,1] = gray\n","      img2[:,:,2] = gray\n","\n","      cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", img2)\n","      dfC.loc[i_c] = [\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", data['Status'][i]]\n","      i_c = i_c+1\n","    except:\n","      pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4_C38jMsDOq"},"source":["  path = root + data[\"fullPath\"][364]#2499 2\n","  path = path.replace('\\\\', '/')\n","  path = path.replace('.png', '.jpg')\n","  img = cv2.imread('/content/drive/MyDrive/Thesis/MINI-DDSM-Complete-JPEG-8/Cancer/0044/C_0044_1.LEFT_CC.jpg')\n","  print(path)\n","  img.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wdPNw8UgAIa"},"source":["# Action pre processing"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Cn131CvgMGHw"},"source":["#@title Iniciate Dataframe\n","\n","dfC = pd.DataFrame(columns=['image', 'label'])\n","dfS = pd.DataFrame(columns=['image', 'mask', 'label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"IzwitLBI3X_8"},"source":["#@title Default title text\n","\n","for i in range(0, len(data)):\n","    path = root + data['fullPath'][i]  # 2499\n","    path = path.replace('\\\\', '/')\n","    path = path.replace('.png', '.jpg')\n","    img = cv2.imread(path, 0)\n","    ini_img = img.copy()\n","    grid_l = int(img.shape[0] * img.shape[0] / img.shape[1] / 600)\n","    grid_w = int(img.shape[1] * img.shape[1] / img.shape[0] / 200)\n","    if grid_l < 2:\n","        grid_l = 2\n","    if grid_w < 2:\n","        grid_w = 2\n","    thresh = clahe(ini_img, 2)\n","    # for r in range(0, 8):\n","    mask_add(thresh, data, i)\n","    #print(i)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"47lvrAtjM34W","executionInfo":{"elapsed":179,"status":"error","timestamp":1628219033587,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"27a20fa7-b782-4872-e039-5b49eb856fcf"},"source":["#@title Saving dataframe\n","# dfC.to_csv('/content/drive/MyDrive/Thesis/pos/dataC.csv', index = False)\n","dfS.to_csv('/content/drive/MyDrive/Thesis/pos/Sdata.csv', index = False)"],"execution_count":null,"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ad82cdafa270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Saving dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# dfC.to_csv('/content/drive/MyDrive/Thesis/pos/dataC.csv', index = False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Thesis/pos/Sdata.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dfS' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"0jZNus_PI3Y7","executionInfo":{"elapsed":188,"status":"ok","timestamp":1628214369517,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"4c9705e2-7088-4e1a-a178-ad854fc2a1d5"},"source":["dfS.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>mask</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/0.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/0.jpg</td>\n","      <td>Benign</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/1.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/1.jpg</td>\n","      <td>Cancer</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/2.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/2.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/3.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/3.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/4.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/4.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        image  ...   label\n","0  /content/drive/MyDrive/Thesis/pos/xS/0.jpg  ...  Benign\n","1  /content/drive/MyDrive/Thesis/pos/xS/1.jpg  ...  Cancer\n","2  /content/drive/MyDrive/Thesis/pos/xS/2.jpg  ...  Normal\n","3  /content/drive/MyDrive/Thesis/pos/xS/3.jpg  ...  Normal\n","4  /content/drive/MyDrive/Thesis/pos/xS/4.jpg  ...  Normal\n","\n","[5 rows x 3 columns]"]},"execution_count":122,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"ME-MDADsKVM0"},"source":["i_c = 0\n","for i in range(0, len(dfS)):\n","    img = cv2.imread(dfS['image'][i], 0)\n","    ini_img = img.copy()\n","\n","    mask_ = cv2.imread(dfS['mask'][i], 0)\n","    ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","    contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)                   \n","    if dfS[\"label\"][i] != 'Normal':\n","      for cnt in contours:      \n","        (x,y,w,h) = cv2.boundingRect(cnt)\n","        cv2.drawContours(mask_, [cnt], 0,(255, 0, 0),-1)\n","      ini_img[np.where(mask_==0)] = 0\n","      cv2.rectangle(ini_img,(x,y),(x+w,y+h),(255,0,0),2)\n","      cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", ini_img[y:y+h,x:x+w])\n","      dfC.loc[i_c] = [\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", dfS['label'][i]]\n","      i_c = i_c + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYc-HBGyMDko"},"source":["dfC.to_csv('/content/drive/MyDrive/Thesis/pos/Cdata.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGK7gI-ZYZVP"},"source":["dfS = pd.read_csv('/content/drive/MyDrive/Thesis/pos/Sdata.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wJ6RUmtZs7k","executionInfo":{"elapsed":189,"status":"ok","timestamp":1628218826638,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"b6e37707-2b9c-4e39-8f17-c2aa3fb1d867"},"source":["l=cv2.imread(dfS['image'][i])\n","l.shape"],"execution_count":null,"outputs":[{"data":{"text/plain":["(2384, 1312, 3)"]},"execution_count":27,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"m2nMbS40gHlM"},"source":["# EfficientUnet"]},{"cell_type":"markdown","metadata":{"id":"nIJUmq6zgXPz"},"source":["## Efficientnet"]},{"cell_type":"code","metadata":{"cellView":"form","id":"q8dhNhDdgW58"},"source":["#@title Efficientnet\n","from keras import models, layers\n","# import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from tensorflow.keras.utils import get_file\n","# from utils import *\n","\n","__all__ = ['get_model_by_name', 'get_efficientnet_b0_encoder', 'get_efficientnet_b1_encoder',\n","           'get_efficientnet_b2_encoder', 'get_efficientnet_b3_encoder', 'get_efficientnet_b4_encoder',\n","           'get_efficientnet_b5_encoder', 'get_efficientnet_b6_encoder', 'get_efficientnet_b7_encoder']\n","\n","\n","def _efficientnet(input_shape, blocks_args_list, global_params):\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","\n","    # Stem part\n","    model_input = layers.Input(shape=input_shape)\n","    x = layers.Conv2D(\n","        filters=round_filters(32, global_params),\n","        kernel_size=[3, 3],\n","        strides=[2, 2],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='stem_conv2d'\n","    )(model_input)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='stem_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='stem_swish')(x)\n","\n","    # Blocks part\n","    idx = 0\n","    drop_rate = global_params.drop_connect_rate\n","    n_blocks = sum([blocks_args.num_repeat for blocks_args in blocks_args_list])\n","    drop_rate_dx = drop_rate / n_blocks\n","\n","    for blocks_args in blocks_args_list:\n","        assert blocks_args.num_repeat > 0\n","        # Update block input and output filters based on depth multiplier.\n","        blocks_args = blocks_args._replace(\n","            input_filters=round_filters(blocks_args.input_filters, global_params),\n","            output_filters=round_filters(blocks_args.output_filters, global_params),\n","            num_repeat=round_repeats(blocks_args.num_repeat, global_params)\n","        )\n","\n","        # The first block needs to take care of stride and filter size increase.\n","        x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","        idx += 1\n","\n","        if blocks_args.num_repeat > 1:\n","            blocks_args = blocks_args._replace(input_filters=blocks_args.output_filters, strides=[1, 1])\n","\n","        for _ in range(blocks_args.num_repeat - 1):\n","            x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","            idx += 1\n","\n","    # Head part\n","    x = layers.Conv2D(\n","        filters=round_filters(1280, global_params),\n","        kernel_size=[1, 1],\n","        strides=[1, 1],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='head_conv2d'\n","    )(x)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='head_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='head_swish')(x)\n","\n","    x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n","\n","    if global_params.dropout_rate > 0:\n","        x = layers.Dropout(global_params.dropout_rate)(x)\n","\n","    x = layers.Dense(\n","        global_params.num_classes,\n","        kernel_initializer=dense_kernel_initializer,\n","        activation='softmax',\n","        name='head_dense'\n","    )(x)\n","\n","    model = models.Model(model_input, x)\n","\n","    return model\n","\n","\n","def get_model_by_name(model_name, input_shape, classes=3, pretrained=False):\n","    \"\"\"Get an EfficientNet model by its name.\n","    \"\"\"\n","    blocks_args, global_params = get_efficientnet_params(model_name, override_params={'num_classes': classes})\n","    model = _efficientnet(input_shape, blocks_args, global_params)\n","\n","    try:\n","        if pretrained:\n","            weights = IMAGENET_WEIGHTS[model_name]\n","            weights_path = get_file(\n","                weights['name'],\n","                weights['url'],\n","                cache_subdir='models',\n","                md5_hash=weights['md5'],\n","            )\n","            model.load_weights(weights_path)\n","    except KeyError as e:\n","        print(\"NOTE: Currently model {} doesn't have pretrained weights, therefore a model with randomly initialized\"\n","              \" weights is returned.\".format(e))\n","\n","    return model\n","\n","\n","def _get_efficientnet_encoder(model_name, input_shape, pretrained=False):\n","    model = get_model_by_name(model_name, input_shape, pretrained=pretrained)\n","    encoder = models.Model(model.input, model.get_layer('global_average_pooling2d').output)\n","    encoder.layers.pop()  # remove GAP layer\n","    return encoder\n","\n","\n","def get_efficientnet_b0_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b0', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b1_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b1', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b2_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b2', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b3_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b3', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b4_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b4', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b5_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b5', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b6_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b6', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b7_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b7', input_shape, pretrained=pretrained)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iz1mgr0_goYL"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"qje92-TUkfjU"},"source":["#@title number of classes\n","\n","n_classes=2 #@param {type:\"integer\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"PR4BA07zgNwD"},"source":["#@title Utils\n","import re\n","from collections import namedtuple\n","from keras import layers\n","import keras.backend as K\n","import tensorflow as tf\n","import math\n","import numpy as np\n","\n","GlobalParams = namedtuple('GlobalParams', ['batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'num_classes',\n","                                           'width_coefficient', 'depth_coefficient', 'depth_divisor', 'min_depth',\n","                                           'drop_connect_rate'])\n","global_params = None\n","GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n","\n","BlockArgs = namedtuple('BlockArgs', ['kernel_size', 'num_repeat', 'input_filters', 'output_filters', 'expand_ratio',\n","                                     'id_skip', 'strides', 'se_ratio'])\n","BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n","\n","IMAGENET_WEIGHTS = {\n","\n","    'efficientnet-b0': {\n","        'name': 'efficientnet-b0_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000.h5',\n","        'md5': 'bca04d16b1b8a7c607b1152fe9261af7',\n","    },\n","\n","    'efficientnet-b1': {\n","        'name': 'efficientnet-b1_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000.h5',\n","        'md5': 'bd4a2b82f6f6bada74fc754553c464fc',\n","    },\n","\n","    'efficientnet-b2': {\n","        'name': 'efficientnet-b2_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000.h5',\n","        'md5': '45b28b26f15958bac270ab527a376999',\n","    },\n","\n","    'efficientnet-b3': {\n","        'name': 'efficientnet-b3_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000.h5',\n","        'md5': 'decd2c8a23971734f9d3f6b4053bf424',\n","    },\n","\n","    'efficientnet-b4': {\n","        'name': 'efficientnet-b4_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000.h5',\n","        'md5': '01df77157a86609530aeb4f1f9527949',\n","    },\n","\n","    'efficientnet-b5': {\n","        'name': 'efficientnet-b5_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000.h5',\n","        'md5': 'c31311a1a38b5111e14457145fccdf32',\n","    }\n","\n","}\n","\n","\n","def round_filters(filters, global_params):\n","    \"\"\"Round number of filters.\"\"\"\n","    multiplier = global_params.width_coefficient\n","    divisor = global_params.depth_divisor\n","    min_depth = global_params.min_depth\n","    if not multiplier:\n","        return filters\n","\n","    filters *= multiplier\n","    min_depth = min_depth or divisor\n","    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_filters < 0.9 * filters:\n","        new_filters += divisor\n","    return int(new_filters)\n","\n","\n","def round_repeats(repeats, global_params):\n","    \"\"\"Round number of repeats.\"\"\"\n","    multiplier = global_params.depth_coefficient\n","    if not multiplier:\n","        return repeats\n","    return int(math.ceil(multiplier * repeats))\n","\n","\n","def get_efficientnet_params(model_name, override_params=None):\n","    \"\"\"Get efficientnet params based on model name.\"\"\"\n","    params_dict = {\n","        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n","        # Note: the resolution here is just for reference, its values won't be used.\n","        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n","        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n","        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n","        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n","        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n","        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n","        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n","        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n","    }\n","    if model_name not in params_dict.keys():\n","        raise KeyError('There is no model named {}.'.format(model_name))\n","\n","    width_coefficient, depth_coefficient, _, dropout_rate = params_dict[model_name]\n","\n","    blocks_args = [\n","        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n","        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n","        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n","        'r1_k3_s11_e6_i192_o320_se0.25',\n","    ]\n","    global_params = GlobalParams(\n","        batch_norm_momentum=0.99,\n","        batch_norm_epsilon=1e-3,\n","        dropout_rate=dropout_rate,\n","        drop_connect_rate=0.2,\n","        num_classes=n_classes,\n","        width_coefficient=width_coefficient,\n","        depth_coefficient=depth_coefficient,\n","        depth_divisor=8,\n","        min_depth=None)\n","\n","    if override_params:\n","        global_params = global_params._replace(**override_params)\n","\n","    decoder = BlockDecoder()\n","    return decoder.decode(blocks_args), global_params\n","\n","\n","class BlockDecoder(object):\n","    \"\"\"Block Decoder for readability.\"\"\"\n","\n","    @staticmethod\n","    def _decode_block_string(block_string):\n","        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n","        assert isinstance(block_string, str)\n","        ops = block_string.split('_')\n","        options = {}\n","        for op in ops:\n","            splits = re.split(r'(\\d.*)', op)\n","            if len(splits) >= 2:\n","                key, value = splits[:2]\n","                options[key] = value\n","\n","        if 's' not in options or len(options['s']) != 2:\n","            raise ValueError('Strides options should be a pair of integers.')\n","\n","        return BlockArgs(\n","            kernel_size=int(options['k']),\n","            num_repeat=int(options['r']),\n","            input_filters=int(options['i']),\n","            output_filters=int(options['o']),\n","            expand_ratio=int(options['e']),\n","            id_skip=('noskip' not in block_string),\n","            se_ratio=float(options['se']) if 'se' in options else None,\n","            strides=[int(options['s'][0]), int(options['s'][1])]\n","        )\n","\n","    @staticmethod\n","    def _encode_block_string(block):\n","        \"\"\"Encodes a block to a string.\"\"\"\n","        args = [\n","            'r%d' % block.num_repeat,\n","            'k%d' % block.kernel_size,\n","            's%d%d' % (block.strides[0], block.strides[1]),\n","            'e%s' % block.expand_ratio,\n","            'i%d' % block.input_filters,\n","            'o%d' % block.output_filters\n","        ]\n","        if 0 < block.se_ratio <= 1:\n","            args.append('se%s' % block.se_ratio)\n","        if block.id_skip is False:\n","            args.append('noskip')\n","        return '_'.join(args)\n","\n","    def decode(self, string_list):\n","        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n","        Args:\n","          string_list: a list of strings, each string is a notation of block.\n","        Returns:\n","          A list of namedtuples to represent blocks arguments.\n","        \"\"\"\n","        assert isinstance(string_list, list)\n","        blocks_args = []\n","        for block_string in string_list:\n","            blocks_args.append(self._decode_block_string(block_string))\n","        return blocks_args\n","\n","    def encode(self, blocks_args):\n","        \"\"\"Encodes a list of Blocks to a list of strings.\n","        Args:\n","          blocks_args: A list of namedtuples to represent blocks arguments.\n","        Returns:\n","          a list of strings, each string is a notation of block.\n","        \"\"\"\n","        block_strings = []\n","        for block in blocks_args:\n","            block_strings.append(self._encode_block_string(block))\n","        return block_strings\n","\n","\n","class Swish(layers.Layer):\n","    def __init__(self, name=None, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","    def call(self, inputs, **kwargs):\n","        return tf.nn.silu(inputs)#tf.nn.swish I have changed this why I don't know yet\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['name'] = self.name\n","        return config\n","\n","\n","def SEBlock(block_args, **kwargs):\n","    num_reduced_filters = max(\n","        1, int(block_args.input_filters * block_args.se_ratio))\n","    filters = block_args.input_filters * block_args.expand_ratio\n","\n","    spatial_dims = [1, 2]\n","\n","    try:\n","        block_name = kwargs['block_name']\n","    except KeyError:\n","        block_name = ''\n","\n","    def block(inputs):\n","        x = inputs\n","        x = layers.Lambda(lambda a: K.mean(a, axis=spatial_dims, keepdims=True))(x)\n","        x = layers.Conv2D(\n","            num_reduced_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_reduce_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = Swish(name=block_name + 'se_swish')(x)\n","\n","        x = layers.Conv2D(\n","            filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_expand_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = layers.Activation('sigmoid')(x)\n","        out = layers.Multiply()([x, inputs])\n","        return out\n","\n","    return block\n","\n","\n","class DropConnect(layers.Layer):\n","\n","    def __init__(self, drop_connect_rate, **kwargs):\n","        super().__init__(**kwargs)\n","        self.drop_connect_rate = drop_connect_rate\n","\n","    def call(self, inputs, **kwargs):\n","        def drop_connect():\n","            keep_prob = 1.0 - self.drop_connect_rate\n","\n","            # Compute drop_connect tensor\n","            batch_size = tf.shape(inputs)[0]\n","            random_tensor = keep_prob\n","            random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n","            binary_tensor = tf.floor(random_tensor)\n","            output = tf.math.divide(inputs, keep_prob) * binary_tensor\n","            return output\n","\n","        return K.in_train_phase(drop_connect(), inputs, training=None)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['drop_connect_rate'] = self.drop_connect_rate\n","        return config\n","\n","\n","def conv_kernel_initializer(shape, dtype=K.floatx()):\n","    \"\"\"Initialization for convolutional kernels.\n","    The main difference with tf.variance_scaling_initializer is that\n","    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n","    standard deviation, whereas here we use a normal distribution. Similarly,\n","    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n","    a corrected standard deviation.\n","    Args:\n","        shape: shape of variable\n","        dtype: dtype of variable\n","    Returns:\n","        an initialization for the variable\n","    \"\"\"\n","    kernel_height, kernel_width, _, out_filters = shape\n","    fan_out = int(kernel_height * kernel_width * out_filters)\n","    return tf.random.normal(\n","        shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n","\n","\n","def dense_kernel_initializer(shape, dtype=K.floatx()):\n","    init_range = 1.0 / np.sqrt(shape[1])\n","    return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)\n","\n","\n","def MBConvBlock(block_args, global_params, idx, drop_connect_rate=None):\n","    filters = block_args.input_filters * block_args.expand_ratio\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","    has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n","\n","    block_name = 'blocks_' + str(idx) + '_'\n","\n","    def block(inputs):\n","        x = inputs\n","\n","        # Expansion phase\n","        if block_args.expand_ratio != 1:\n","            expand_conv = layers.Conv2D(filters,\n","                                        kernel_size=[1, 1],\n","                                        strides=[1, 1],\n","                                        kernel_initializer=conv_kernel_initializer,\n","                                        padding='same',\n","                                        use_bias=False,\n","                                        name=block_name + 'expansion_conv2d'\n","                                        )(x)\n","            bn0 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                            epsilon=batch_norm_epsilon,\n","                                            name=block_name + 'expansion_batch_norm')(expand_conv)\n","\n","            x = Swish(name=block_name + 'expansion_swish')(bn0)\n","\n","        # Depth-wise convolution phase\n","        kernel_size = block_args.kernel_size\n","        depthwise_conv = layers.DepthwiseConv2D(\n","            [kernel_size, kernel_size],\n","            strides=block_args.strides,\n","            depthwise_initializer=conv_kernel_initializer,\n","            padding='same',\n","            use_bias=False,\n","            name=block_name + 'depthwise_conv2d'\n","        )(x)\n","        bn1 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                        epsilon=batch_norm_epsilon,\n","                                        name=block_name + 'depthwise_batch_norm'\n","                                        )(depthwise_conv)\n","        x = Swish(name=block_name + 'depthwise_swish')(bn1)\n","\n","        if has_se:\n","            x = SEBlock(block_args, block_name=block_name)(x)\n","\n","        # Output phase\n","        project_conv = layers.Conv2D(\n","            block_args.output_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'output_conv2d',\n","            use_bias=False)(x)\n","        x = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                      epsilon=batch_norm_epsilon,\n","                                      name=block_name + 'output_batch_norm'\n","                                      )(project_conv)\n","        if block_args.id_skip:\n","            if all(\n","                    s == 1 for s in block_args.strides\n","            ) and block_args.input_filters == block_args.output_filters:\n","                # only apply drop_connect if skip presents.\n","                if drop_connect_rate:\n","                    x = DropConnect(drop_connect_rate)(x)\n","                x = layers.add([x, inputs])\n","\n","        return x\n","\n","    return block\n","\n","\n","def freeze_efficientunet_first_n_blocks(model, n):\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            model.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","\n","    all_block_names = ['blocks_{}_output_batch_norm'.format(i) for i in range(mbblock_nr)]\n","    all_block_index = []\n","    for idx, layer in enumerate(model.layers):\n","        if layer.name == all_block_names[0]:\n","            all_block_index.append(idx)\n","            all_block_names.pop(0)\n","            if len(all_block_names) == 0:\n","                break\n","    n_blocks = len(all_block_index)\n","\n","    if n <= 0:\n","        print('n is less than or equal to 0, therefore no layer will be frozen.')\n","        return\n","    if n > n_blocks:\n","        raise ValueError(\"There are {} blocks in total, n cannot be greater than {}.\".format(n_blocks, n_blocks))\n","\n","    idx_of_last_block_to_be_frozen = all_block_index[n - 1]\n","    for layer in model.layers[:idx_of_last_block_to_be_frozen + 1]:\n","        layer.trainable = False\n","\n","\n","def unfreeze_efficientunet(model):\n","    for layer in model.layers:\n","        layer.trainable = True\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZ29zXFqh5sm"},"source":["## *Efficientunet*"]},{"cell_type":"code","metadata":{"cellView":"form","id":"TGg2DkR-g-2L"},"source":["#@markdown Efficientnet-unet\n","import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from keras.layers import *\n","from keras import models\n","# from efficientnet import *\n","# from utils import conv_kernel_initializer\n","\n","\n","__all__ = ['get_efficient_unet_b0', 'get_efficient_unet_b1', 'get_efficient_unet_b2', 'get_efficient_unet_b3',\n","           'get_efficient_unet_b4', 'get_efficient_unet_b5', 'get_efficient_unet_b6', 'get_efficient_unet_b7',\n","           'get_blocknr_of_skip_candidates']\n","\n","\n","def get_blocknr_of_skip_candidates(encoder, verbose=False):\n","    \"\"\"\n","    Get block numbers of the blocks which will be used for concatenation in the Unet.\n","    :param encoder: the encoder\n","    :param verbose: if set to True, the shape information of all blocks will be printed in the console\n","    :return: a list of block numbers\n","    \"\"\"\n","    shapes = []\n","    candidates = []\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","            shape = int(mbblock.shape[1]), int(mbblock.shape[2])\n","            if shape not in shapes:\n","                shapes.append(shape)\n","                candidates.append(mbblock_nr)\n","            if verbose:\n","                print('blocks_{}_output_shape: {}'.format(mbblock_nr, shape))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","    return candidates\n","\n","\n","def DoubleConv(filters, kernel_size, initializer='glorot_uniform'):\n","\n","    def layer(x):\n","\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","def UpSampling2D_block(filters, kernel_size=(3, 3), upsample_rate=(2, 2), interpolation='bilinear',\n","                       initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = UpSampling2D(size=upsample_rate, interpolation=interpolation)(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","    return layer\n","\n","\n","def Conv2DTranspose_block(filters, kernel_size=(3, 3), transpose_kernel_size=(2, 2), upsample_rate=(2, 2),\n","                          initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate, padding='same')(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","# noinspection PyTypeChecker\n","def _get_efficient_unet(encoder, out_channels=2, block_type='upsampling', concat_input=True):\n","    MBConvBlocks = []\n","\n","    skip_candidates = get_blocknr_of_skip_candidates(encoder)\n","\n","    for mbblock_nr in skip_candidates:\n","        mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","        MBConvBlocks.append(mbblock)\n","\n","    # delete the last block since it won't be used in the process of concatenation\n","    MBConvBlocks.pop()\n","\n","    input_ = encoder.input\n","    head = encoder.get_layer('head_swish').output\n","    blocks = [input_] + MBConvBlocks + [head]\n","\n","    if block_type == 'upsampling':\n","        UpBlock = UpSampling2D_block\n","    else:\n","        UpBlock = Conv2DTranspose_block\n","\n","    o = blocks.pop()\n","    o = UpBlock(512, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(256, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(128, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(64, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    if concat_input:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    else:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=None)(o)\n","    o = Conv2D(out_channels, (1, 1), padding='same', kernel_initializer=conv_kernel_initializer)(o)\n","\n","    model = models.Model(encoder.input, o)\n","\n","    return model\n","\n","\n","def get_efficient_unet_b0(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B0 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B0 model\n","    \"\"\"\n","    encoder = get_efficientnet_b0_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b1(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B1 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B1 model\n","    \"\"\"\n","    encoder = get_efficientnet_b1_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b2(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B2 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B2 model\n","    \"\"\"\n","    encoder = get_efficientnet_b2_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b3(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B3 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B3 model\n","    \"\"\"\n","    encoder = get_efficientnet_b3_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b4(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B4 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B4 model\n","    \"\"\"\n","    encoder = get_efficientnet_b4_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b5(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B5 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B5 model\n","    \"\"\"\n","    encoder = get_efficientnet_b5_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b6(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B6 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B6 model\n","    \"\"\"\n","    encoder = get_efficientnet_b6_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b7(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B7 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B7 model\n","    \"\"\"\n","    encoder = get_efficientnet_b7_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gg6Xw9x-iHkA"},"source":["## Create model"]},{"cell_type":"code","metadata":{"id":"mdJWcg-biObH"},"source":["Channels =  3#@param {type:\"integer\"}\n","Img_size = 416#@param {type:\"integer\"}\n","input_shape = (Img_size, Img_size, Channels) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPWwAz8SkX7P"},"source":["tf.keras.backend.clear_session()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rml63YcFiLxP","executionInfo":{"elapsed":7854,"status":"ok","timestamp":1628219451637,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"5c20bb38-9ab1-4d4d-a24a-114dba0e265b"},"source":["#@markdown Model Efficient unet\n","\n","classifier =  get_efficientnet_b5_encoder(input_shape, pretrained=False)\n","modelS = models.Sequential()\n","modelS.add(classifier)\n","modelS.add(layers.Dense(n_classes))\n","modelS.summary()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_1 (Functional)         (None, 2048)              28513520  \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 4098      \n","=================================================================\n","Total params: 28,517,618\n","Trainable params: 28,344,882\n","Non-trainable params: 172,736\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkXW82PKibDl","executionInfo":{"elapsed":6366,"status":"ok","timestamp":1628219457980,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"0d134991-c291-48bf-a8d5-9eac110f60be"},"source":["classifier_t =  get_efficientnet_b5_encoder(input_shape, pretrained=False)\n","modelT = models.Sequential()\n","modelT.add(classifier_t)\n","modelT.add(layers.Dense(n_classes))\n","modelT.summary()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_3 (Functional)         (None, 2048)              28513520  \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 4098      \n","=================================================================\n","Total params: 28,517,618\n","Trainable params: 28,344,882\n","Non-trainable params: 172,736\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"-j9x5-Py5RcX"},"source":["# Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNmm_kkr5RcZ","executionInfo":{"elapsed":13,"status":"ok","timestamp":1628219457981,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"68be4c5c-3e16-4a6a-bad1-fb5ee6fab1c3"},"source":["from tensorflow.python.client import device_lib\n","\n","def get_available_gpus():\n","    local_device_protos = device_lib.list_local_devices()\n","    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n","get_available_gpus()    "],"execution_count":null,"outputs":[{"data":{"text/plain":["[]"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1iJF97nG5Rcb","executionInfo":{"elapsed":11,"status":"ok","timestamp":1628219457982,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"2b3ad4d5-3e73-4a51-f7ba-ac7a5dc1fc5b"},"source":["%cd /content/drive/MyDrive/Thesis/MPL"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/Thesis/MPL'\n","/content\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"TvcW5Pc2_Cur"},"source":["#@title MPL config\n","import tensorflow as tf\n","\n","\n","\n","\n","# about dataset\n","IMG_SIZE = 416#@param {type:\"integer\"}\n","BATCH_SIZE = 4#@param {type:\"integer\"}\n","# LABEL_FILE_PATH = '/content/cifar/label4000.csv' # google\n","# UNLABEL_FILE_PATH = '/content/cifar/train.csv'\n","\n","_MAX_LEVEL = 10\n","CUTOUT_CONST = 40.\n","TRANSLATE_CONST = 100.\n","REPLACE_COLOR = [128, 128, 128]\n","\n","\n","# LABEL_FILE_PATH = '../input/cifar10/cifar/label4000.csv'  # kaggle\n","# UNLABEL_FILE_PATH = '../input/cifar10/cifar/train.csv'\n","\n","\n","AUGMENT_MAGNITUDE = 8\n","SHUFFLE_SIZE = BATCH_SIZE * 16\n","DATA_LEN = 400  # 数据集的总长度\n","\n","# about model\n","NUM_XLA_SHARDS = -1\n","BATCH_NORM_EPSILON = 1e-3\n","BATCH_NORM_DECAY = 0.999\n","DROPOUT_RATE = 0.\n","DROPOUT = 0.2\n","NUM_CLASSES = 2#@param {type:\"integer\"}\n","NUM_CLASS = 2#@param {type:\"integer\"}\n","\n","# about training\n","LOG_EVERY = 20\n","SAVE_EVERY = 5\n","TEA_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/PST'\n","STD_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/PSS'\n","\n","MAX_EPOCHS = 1920\n","MAX_STEPS = MAX_EPOCHS * (int(DATA_LEN / BATCH_SIZE)-1)\n","UDA_WEIGHT = 8  # uda的权重\n","UDA_STEPS = 2000\n","TEST_EVERY = 2\n","GRAD_BOUND = 1e9\n","EMA = 0.995\n","\n","\n","# continue train\n","TEA_CONTINUE = False\n","STD_CONTINUE = False\n","TEA_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/PST'\n","STD_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/PSS'\n","CONTINUE_EPOCH = 885\n","\n","\n","# about testing\n","# TEST_FILE_PATH = '/content/cifar/test.csv'\n","# TEST_FILE_PATH = '../input/cifar10/cifar/test.csv'\n","TEST_MODEL_PATH = '/content/drive/MyDrive/Thesis/weights/PS'\n","\n","# about UdaCrossEntroy\n","UDA_DATA = 1\n","LABEL_SMOOTHING = 0.15\n","UDA_TEMP = 0.7\n","UDA_THRESHOLD = 0.6\n","\n","# about learning rate\n","STUDENT_LR = 0.0005  # student\n","STUDENT_LR_WARMUP_STEPS = 4000\n","STUDENT_LR_WAIT_STEPS = 2000\n","TEACHER_LR = 0.0005  # teacher\n","TEACHER_LR_WARMUP_STEPS = 1000\n","TEACHER_NUM_WAIT_STEPS = 0\n","\n","LR_DECAY_TYPE = 'cosine'  # constant, exponential, cosine\n","NUM_DECAY_STEPS = 300\n","LR_DECAY_RATE = 0.97\n","\n","# about optimizer\n","OPTIM_TYPE = 'sgd'  # sgd, momentum, rmsprop\n","WEIGHT_DECAY = 5e-4\n","\n","\n","# dtype\n","DTYPE = tf.float32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"JzUpbjZDSWDh"},"source":["#@title Self_aug_func\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import math\n","import tensorflow_addons.image as image_ops\n","\n","# import \n","\n","\n","def autocontrast(image):\n","    lo = tf.cast(tf.reduce_min(image, axis=[0, 1]), tf.float32)\n","    hi = tf.cast(tf.reduce_max(image, axis=[0, 1]), tf.float32)\n","    scale = tf.math.divide(255.0, (hi - lo))\n","    offset = tf.math.multiply(-lo, scale)\n","    image = tf.math.add(\n","        tf.math.multiply(tf.cast(image, tf.float32), scale),\n","        offset\n","    )\n","    image = tf.clip_by_value(image, 0.0, 255.0)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def equalize(image):\n","    # image = tf.cast(image, tf.int32)\n","    # channel = tf.shape(image)[-1]\n","    # for i in range(channel):\n","    #     im = tf.cast(image[:, :, i], tf.int32)\n","    #     histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","    #     nonzero = tf.where(tf.not_equal(histo, 0))\n","    #     nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","    #     step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","    #     print(step)\n","    #     if step == 0:\n","    #         pass\n","    #     else:\n","    #         lut = (tf.cumsum(histo) + (step // 2)) // step\n","    #         lut = tf.concat([[0], lut[:-1]], 0)\n","    #         lut = tf.clip_by_value(lut, 0, 255)\n","    #         # print(lut)\n","    #         image[:, :, i] = tf.gather(lut, image[:, :, i])\n","    #         # image[:, :, i] = im\n","    #     # image[:, :, i] = im\n","\n","    def scale_channel(im, c=0):\n","        \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n","        im = tf.cast(im[:, :, 0], tf.int32)\n","        # Compute the histogram of the image channel.\n","        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","\n","        # For the purposes of computing the step, filter out the nonzeros.\n","        nonzero = tf.where(tf.not_equal(histo, 0))\n","        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","\n","        def build_lut(histo, step):\n","            # Compute the cumulative sum, shifting by step // 2\n","            # and then normalization by step.\n","            lut = (tf.cumsum(histo) + (step // 2)) // step\n","            # Shift lut, prepending with 0.\n","            lut = tf.concat([[0], lut[:-1]], 0)\n","            # Clip the counts to be in range.  This is done\n","            # in the C code for image.point.\n","            return tf.clip_by_value(lut, 0, 255)\n","\n","        # If step is zero, return the original image.  Otherwise, build\n","        # lut from the full histogram and step and then index from it.\n","        result = tf.cond(tf.equal(step, 0),\n","                         lambda: im,\n","                         lambda: tf.gather(build_lut(histo, step), im))\n","        return tf.cast(result, tf.uint8)\n","\n","    s1 = scale_channel(image, 0)\n","    s2 = scale_channel(image, 1)\n","    s3 = scale_channel(image, 2)\n","    image = tf.stack([s1, s2, s3], 2)\n","\n","    return image\n","\n","\n","def invert(image):\n","    image = 255 - image\n","    return image\n","\n","\n","def rotate(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    degree = tf.cond(should_filp, lambda: level, lambda: -level)\n","    degree_to_radians = tf.convert_to_tensor(math.pi / 180., tf.float32)\n","    radians = tf.math.multiply(degree, degree_to_radians)\n","    new_imgsize = tf.cast(tf.math.abs(tf.divide(IMG_SIZE, radians)), tf.int32)\n","    image = tf.image.resize(image, (new_imgsize, new_imgsize))\n","    image = image_ops.rotate(image, radians, fill_mode='constant')\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def posterize(image):\n","    bit = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 4, tf.float32)\n","    shift = tf.cast(8 - bit, image.dtype)\n","    image = tf.bitwise.right_shift(image, shift)\n","    image = tf.bitwise.left_shift(image, shift)\n","    return image\n","\n","\n","def solarize_arg(image):\n","    threahold = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 22, tf.float32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def solarize_add(image, threahold=128):\n","    addition = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 2, tf.int32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.add(tf.cast(image, tf.int32), addition)\n","    image = tf.cast(tf.clip_by_value(image, 0, 255), tf.uint8)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def color(image, degenetate=None):\n","    if degenetate is None:\n","        degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.8 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def contrast(image):\n","    degenerate = tf.image.rgb_to_grayscale(image)\n","    degenerate = tf.cast(degenerate, tf.int32)\n","\n","    hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n","    mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.\n","    degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n","    degenerate = tf.clip_by_value(degenerate, 0., 255.)\n","    degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.6 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def brightness(image):\n","    image = tf.image.adjust_brightness(image, 0.25)\n","    return image\n","\n","\n","def sharpness(image):\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.6 + 0.1, tf.float32)\n","    image = tf.cast(image, tf.float32)\n","    image = image_ops.sharpness(image, factor)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_x(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.2, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_x(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_y(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.1, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_y(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def translate_x(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [-pixels, 0])\n","    return image\n","\n","\n","def translate_y(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [0, -pixels])\n","    return image\n","\n","\n","def cutout(image):\n","    pad_size = tf.cast(\n","        tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * CUTOUT_CONST,\n","        tf.int32\n","    )\n","    image_height = tf.shape(image)[0]\n","    image_width = tf.shape(image)[1]\n","\n","    # Samples the center location in the image where the zero mask is applied.\n","    cutout_center_height = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_height,\n","        dtype=tf.int32)\n","\n","    cutout_center_width = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_width,\n","        dtype=tf.int32)\n","\n","    lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n","    upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n","    left_pad = tf.maximum(0, cutout_center_width - pad_size)\n","    right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n","\n","    cutout_shape = [image_height - (lower_pad + upper_pad),\n","                    image_width - (left_pad + right_pad)]\n","    padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n","    mask = tf.pad(\n","        tf.zeros(cutout_shape, dtype=image.dtype),\n","        padding_dims, constant_values=1)\n","    mask = tf.expand_dims(mask, -1)\n","    mask = tf.tile(mask, [1, 1, 3])\n","    image = tf.where(\n","        tf.equal(mask, 0),\n","        tf.ones_like(image, dtype=image.dtype) * REPLACE_COLOR,\n","        image)\n","    return image\n","\n","\n","def identity(image):\n","    return tf.identity(image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"8e7vEGndSLme","executionInfo":{"elapsed":6,"status":"ok","timestamp":1628219459021,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"274c335c-ad8e-4547-f4e3-ca65bec421c1"},"source":["#@title Self_aug_util\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import tensorflow as tf\n","\n","# from self_aug_func import *\n","\n","_MAX_LEVEL = 10\n","\n","\n","\n","def _enhance_level_to_arg(level):\n","    return (tf.cast((level / _MAX_LEVEL) * 1.8 + 0.1, tf.float32),)\n","\n","\n","def _translate_level_to_arg(level, translate_const):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * float(translate_const), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # 得到的结果为True和False\n","    final_tensor = tf.cond(should_flip, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _rotate_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _shear_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 0.3, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def level_to_arg(cutout_const, translate_const):\n","    '''\n","    将对image做变化的函数所用到的参数整理成字典形式\n","    :param cutout_const:\n","    :param translate_const:\n","    :return: type:dict\n","    '''\n","    no_arg = lambda level: ()\n","    posterize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 4,\n","        tf.float32\n","    )\n","    solarize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 256,\n","        tf.float32\n","    )\n","    solarize_add_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 110,\n","        tf.float32\n","    )\n","    cutout_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * cutout_const,\n","        tf.float32\n","    )\n","    translate_arg = lambda level: _translate_level_to_arg(level, translate_const)\n","\n","    args = {\n","        'Identity': no_arg,\n","        'AutoContrast': no_arg,\n","        'Equalize': no_arg,\n","        'Invert': no_arg,\n","        'Rotate': _rotate_level_to_arg,\n","        'Posterize': posterize_arg,\n","        'Solarize': solarize_arg,\n","        'SplarizeAdd': solarize_add_arg,\n","        'Color': _enhance_level_to_arg,\n","        'Contrast': _enhance_level_to_arg,\n","        'Brightness': _enhance_level_to_arg,\n","        'Sharpness': _enhance_level_to_arg,\n","        'ShearX': _shear_level_to_arg,\n","        'ShearY': _shear_level_to_arg,\n","        'Cutout': cutout_arg,\n","        'TranslateX': translate_arg,\n","        'TranslateY': translate_arg,\n","    }\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    NAME_TO_FUNC = {\n","        'AutoContrast': autocontrast,\n","        'Equalize': equalize,\n","        'Invert': invert,\n","        'Rotate': rotate,\n","        'Posterize': posterize,\n","        'Solarize': solarize_arg,\n","        'SolarizeAdd': solarize_add,\n","        'Color': color,\n","        'Contrast': contrast,\n","        'Brightness': brightness,\n","        'Sharpness': sharpness,\n","        'ShearX': shear_x,\n","        'ShearY': shear_y,\n","        'TranslateX': translate_x,\n","        'TranslateY': translate_y,\n","        'Cutout': cutout,\n","        'Identity': identity,\n","    }\n","\n","    available_ops = [\n","        'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","        'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","    ]\n","\n","    for (i, op_name) in enumerate(available_ops):\n","        func = NAME_TO_FUNC[op_name]\n","        args = level_to_arg(4, 4)[op_name](16)\n","        print(args)\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["()\n","()\n","()\n","tf.Tensor(48.0, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(409.6, shape=(), dtype=float32)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","tf.Tensor(-0.48, shape=(), dtype=float32)\n","tf.Tensor(0.48, shape=(), dtype=float32)\n","tf.Tensor(-6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"8lCvf2VERloi"},"source":["#@title Self_augment\n","'''\n","reference:\n","https://github.com/google-research/google-research/tree/1f1741a985a0f2e6264adae985bde664a7993bd2/flax_models/cifar/datasets\n","'''\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","\n","'''\n","可能augment.py中的内容有问题 涉及文件augment.py的line 53，54\n","引用的库不一样，因为tensorflow.contrib已经停用，\n","使用的第三方：pip install tensorflow-addons\n","'''\n","import os\n","\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","\n","# from self_aug_func import *\n","\n","# 将对图片做augment的函数变成一个字典\n","NAME_TO_FUNC = {\n","    'AutoContrast': autocontrast,\n","    'Equalize': equalize,\n","    'Invert': invert,\n","    'Rotate': rotate,\n","    'Posterize': posterize,\n","    'Solarize': solarize_arg,\n","    'SolarizeAdd': solarize_add,\n","    'Color': color,\n","    'Contrast': contrast,\n","    'Brightness': brightness,\n","    'Sharpness': sharpness,\n","    'ShearX': shear_x,\n","    'ShearY': shear_y,\n","    'TranslateX': translate_x,\n","    'TranslateY': translate_y,\n","    'Cutout': cutout,\n","    'Identity': identity,\n","}\n","# 在某些函数中有一些需要一个替换的值，比如旋转中有一些位置的像素值需要补充\n","REPLACE_FUNCS = frozenset({\n","    'Rotate',\n","    'TranslateX',\n","    'ShearX',\n","    'SHearY',\n","    'TranslateY',\n","    'Cutout',\n","})\n","\n","\n","class RandAugment(object):\n","    def __init__(self, num_layers=2, magnitude=None, cutout_const=40, translate_const=100., available_ops=None):\n","        '''\n","        reference: https://arxiv.org/abs/1909.13719\n","        :param num_layers:\n","        :param magnitude:\n","        :param cutout_const:\n","        :param translate_const:\n","        :param avalilable_ops:\n","        '''\n","        super(RandAugment, self).__init__()\n","        self.num_layers = num_layers\n","        self.cutout_const = float(cutout_const)\n","        self.translate_const = float(translate_const)\n","        if available_ops is None:\n","            available_ops = [\n","                'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","                'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","                'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","            ]\n","        self.available_ops = available_ops\n","        self.magnitude = magnitude\n","\n","    def distort(self, image):\n","        '''\n","\n","        :param image:  shape:[HWC] C=3\n","        :return: 返回一个经过变化后的图片\n","        '''\n","        input_image_type = image.dtype\n","        image = tf.clip_by_value(image, tf.cast(0, input_image_type), tf.cast(255, input_image_type))\n","        image = tf.cast(image, tf.uint8)\n","\n","        prob = tf.random.uniform([], 0.2, 0.8, tf.float32)\n","\n","        for _ in range(self.num_layers):\n","            op_to_select = tf.random.uniform([], minval=0, maxval=len(self.available_ops), dtype=tf.int32)\n","            for (i, op_name) in enumerate(self.available_ops):\n","                func = NAME_TO_FUNC[op_name]  # 得到函数名称\n","                if i == op_to_select:\n","                    flag = tf.random.uniform([], 0., 1., prob.dtype)\n","                    if tf.math.greater_equal(prob, flag):\n","                        image = func(image)\n","\n","        image = tf.cast(image, dtype=input_image_type)\n","        return image\n","\n","\n","def unlabel_image(img_file, label):\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","\n","    aug_image, some_info = aug.distort(img)\n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, tf.float32) / 255.0\n","    ori_image = tf.cast(ori_image, tf.float32) / 255.0\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"iSpsmsJEDxj1"},"source":["#@title UDa\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","\n","# import config\n","# from Model import Wrn28k\n","\n","\n","def UdaCrossEntroy(all_logits, l_labels, global_step):\n","    batch_size = BATCH_SIZE\n","    uda_data = UDA_DATA\n","    logits = {}\n","    labels = {}\n","    cross_entroy = {}\n","    masks = {}\n","    # 将网络的输出结果区分成 label ori aug 三个部分\n","    logits['l'], logits['ori'], logits['aug'] = tf.split(\n","        all_logits,\n","        [batch_size, batch_size * uda_data, batch_size * uda_data],\n","        axis=0,\n","    )\n","    # 对标签进行处理\n","    labels['l'] = l_labels\n","\n","    # ------------loss的计算---------\n","    # part1：有监督部分\n","    cross_entroy['l'] = tf.losses.CategoricalCrossentropy(\n","        from_logits=True,\n","        label_smoothing=LABEL_SMOOTHING,\n","        reduction=keras.losses.Reduction.NONE,)(labels['l'], logits['l'])\n","    '''\n","    probs = tf.nn.softmax(logits['l'], axis=-1)  # 将每张图片对应10个类别的输出转化为概率的形式\n","    correct_probs = tf.reduce_sum(labels['l'] * probs, axis=-1)  # 根据图片对应的label和概率计算出 预测正确类别的概率\n","    # 计算一个阈值l_threshold\n","    r = tf.cast(global_step, tf.float32) / tf.convert_to_tensor(MAX_STEPS, dtype=tf.float32)\n","    num_classes = tf.convert_to_tensor(NUM_CLASSES, tf.float32)\n","    l_threshold = r * (1. - 1. / num_classes) + 1. / num_classes\n","    masks['l'] = tf.math.less_equal(correct_probs, l_threshold)\n","    masks['l'] = tf.cast(masks['l'], tf.float32)\n","    masks['l'] = tf.stop_gradient(masks['l'])  # 如果对某图片预测的概率小于l_threahold,输出1，否则是0\n","    '''\n","    cross_entroy['l'] = tf.reduce_sum(cross_entroy['l']) / float(batch_size)\n","\n","    # part2: 无监督部分\n","    labels['ori'] = tf.nn.softmax(logits['ori'] / tf.convert_to_tensor(UDA_TEMP), axis=-1)\n","    labels['ori'] = tf.stop_gradient(labels['ori'])\n","    # tf.nn.log_softmax: 设一张图片对应3个类别的输出为o1，o2，o3 ==>\n","    # b = log(sum(exp(o1) + exp(o2) + exp(o3)))  new_o1=o1-b, new_o2=o2-b ... 恒负，大小关系不变\n","    cross_entroy['u'] = (\n","            labels['ori'] * tf.nn.log_softmax(logits['aug'], axis=-1)\n","    )\n","\n","    largest_probs = tf.reduce_max(labels['ori'], axis=-1, keepdims=True)\n","\n","    masks['u'] = tf.math.greater_equal(largest_probs, tf.constant(UDA_THRESHOLD))  # 判断最大概率是否大于阈值\n","    masks['u'] = tf.cast(masks['u'], DTYPE)\n","    masks['u'] = tf.stop_gradient(masks['u'])\n","    # 极端情况，当ori的预测完全准确，即class i = 1, 其他类别为0时，\n","    # aug的class i最大，即最大的负数，两者相乘再取负，就是一个非常接近于0的数字\n","    cross_entroy['u'] = tf.reduce_sum(-cross_entroy['u'] * masks['u']) / \\\n","                        tf.convert_to_tensor((batch_size * uda_data), dtype=DTYPE)\n","\n","    return logits, labels, masks, cross_entroy\n","\n","\n","# if __name__ == '__main__':\n","#     # 制作数据\n","#     l_images = np.random.random((1, 32, 32, 3))\n","#     l_images = tf.convert_to_tensor(l_images, dtype=DTYPE)\n","#     ori_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     ori_images = tf.convert_to_tensor(ori_images, dtype=DTYPE)\n","#     aug_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     aug_images = tf.convert_to_tensor(aug_images, dtype=DTYPE)\n","#     all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [3, 32, 32, 3]\n","\n","#     l_labels = np.array([2])\n","#     l_labels = tf.convert_to_tensor(l_labels, dtype=tf.int32)\n","#     l_labels = tf.raw_ops.OneHot(indices=l_labels, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","#     l_labels = tf.cast(l_labels, DTYPE)\n","\n","#     # 构建teacher模型，产生输出\n","#     teacher = Wrn28k(num_inp_filters=3, k=2)\n","#     output = teacher(x=all_images)  # shape=[15, 10]\n","\n","#     logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, 1)\n","#     print('logits: ', logits.keys())\n","#     print('labels: ', labels.keys())\n","#     print('masks: ', masks.keys())\n","#     # print('cross entroy: ', cross_entroy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"NEFmWImkDNid"},"source":["#@title Test\n","import os\n","\n","# from WideResnet import WideResnet\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import pandas as pd\n","\n","# import config\n","\n","\n","def test(student, file_paths, labels):\n","    student.training = False\n","    # 准备数据\n","    # df_label = pd.read_csv(TEST_FILE_PATH)\n","    # file_paths = df_label['file_name'].values\n","    # labels = df_label['label'].values\n","\n","    # testing\n","    total_num = int(len(labels)/2)\n","    corrent_num = 0\n","    for i in range(total_num):\n","        img_file = file_paths[i]\n","        label = int(labels[i])\n","\n","        # 对图片的处理\n","        img = tf.io.read_file(img_file)\n","        img = tf.image.decode_jpeg(img, channels=3)\n","        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","        img = tf.cast(img, dtype=DTYPE) / 255.0\n","        img = tf.expand_dims(img, axis=0)\n","        mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","        std = tf.expand_dims(tf.convert_to_tensor([0.0737, 0.0737, 0.0737], dtype=DTYPE), axis=0)\n","        img = (img - mean) / std\n","\n","        # 网络\n","        output = student(img)\n","        output = tf.nn.softmax(output)\n","        class_index = tf.squeeze(tf.math.argmax(output, axis=1))\n","\n","        if class_index == label:\n","            corrent_num += 1\n","    accuracy = float(corrent_num) / float(total_num) * 100.\n","    student.training = True\n","    return accuracy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"8xd5evraDERY","executionInfo":{"elapsed":9,"status":"ok","timestamp":1628219459452,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"5bd70304-5015-423b-90f5-4d79f1d9448f"},"source":["#@title learning rate\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","# import config\n","\n","\n","class LearningRate(object):\n","    def __init__(self, initial_lr, num_warmup_steps, num_wait_steps=None):\n","        if initial_lr is None:\n","            raise ValueError(f'initial_lr is error in learningRate file')\n","        if num_warmup_steps is None:\n","            raise ValueError(f'num_warmup_steps is error in learningRate file')\n","        if num_wait_steps is None:\n","            raise ValueError(f'num_wait_steps is error in learningRate file')\n","\n","        # initial_lr = initial_lr * BATCH_SIZE / 256\n","        self.initial_lr = initial_lr\n","        self.num_warmup_steps = num_warmup_steps\n","        self.num_wait_steps = num_wait_steps\n","\n","        if LR_DECAY_TYPE == 'constant':\n","            self.lr = tf.constant(self.initial_lr, dtype=tf.float32)\n","\n","        elif LR_DECAY_TYPE == 'exponential':\n","            self.lr = keras.optimizers.schedules.ExponentialDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=NUM_DECAY_STEPS,\n","                decay_rate=LR_DECAY_RATE,\n","            )\n","\n","        elif LR_DECAY_TYPE == 'cosine':\n","            self.lr = keras.experimental.CosineDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=MAX_STEPS - self.num_wait_steps - self.num_warmup_steps,\n","                alpha=0.0\n","            )\n","        else:\n","            raise ValueError(f'unknown lr_decay_type in py')\n","\n","    def __call__(self, global_step):\n","        global_step = global_step - self.num_wait_steps\n","        if LR_DECAY_TYPE == 'constant':\n","            learn_rate = self.lr\n","        else:\n","            learn_rate = self.lr.__call__(global_step)\n","\n","        r = tf.constant((global_step + 1), tf.float32) / tf.constant(self.num_warmup_steps, tf.float32)\n","        warmup_lr = self.initial_lr * r\n","        lr = tf.cond(\n","            tf.cast(global_step, tf.int32) < tf.cast(self.num_warmup_steps, tf.int32),\n","            lambda: warmup_lr,\n","            lambda: learn_rate,\n","        )\n","        lr = tf.cond(global_step < 0, lambda: tf.constant(0., tf.float32), lambda: lr)\n","        return lr\n","\n","\n","'''\n","def LearningRate(initial_lr, num_warmup_steps, num_wait_steps):\n","    if initial_lr is None:\n","        raise ValueError(f'initial_lr is error in learningRate file')\n","    if num_warmup_steps is None:\n","        raise ValueError(f'num_warmup_steps is error in learningRate file')\n","    if num_wait_steps is None:\n","        raise ValueError(f'num_wait_steps is error in learningRate file')\n","    initial_lr = initial_lr * BATCH_SIZE / 256\n","    if LR_DECAY_TYPE == 'constant':\n","        lr = tf.constant(initial_lr, dtype=tf.float32)\n","    elif LR_DECAY_TYPE == 'exponential':\n","        lr = keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=NUM_DECAY_STEPS,\n","            decay_rate=LR_DECAY_RATE,\n","        )\n","    elif LR_DECAY_TYPE == 'cosine':\n","        lr = keras.experimental.CosineDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=MAX_STEPS - num_wait_steps - num_warmup_steps,\n","            alpha=0.0\n","        )\n","    else:\n","        raise ValueError(f'unknown lr_decay_type in py')\n","    return lr\n","'''\n","\n","import math\n","def lr_lambda(current_step):\n","    if current_step < 0:\n","        return float(current_step) / float(max(1, 0))\n","\n","    progress = float(current_step - 0) / \\\n","               float(max(1, 10 - 0))\n","    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress)))\n","\n","\n","if __name__ == '__main__':\n","    for i in range(10):\n","        print(lr_lambda(i))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","0.9755282581475768\n","0.9045084971874737\n","0.7938926261462366\n","0.6545084971874737\n","0.5\n","0.34549150281252633\n","0.2061073738537635\n","0.09549150281252633\n","0.024471741852423234\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"9gnHT8aeA7sb"},"source":["#@title C Dataset MLP\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# import config\n","import sys  \n","sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL')\n","\n","# from Self_augment import RandAugment\n","\n","\n","def normalize_image(img, label):\n","    '''\n","    图片的归一化\n","    :param img:\n","    :param label:\n","    :return:\n","    '''\n","    return tf.cast(img, tf.float32) / 255.0, label\n","\n","\n","# 制作有标签的数据集\n","def label_image(img_file, label):\n","    '''\n","    获取图片，对图片做水平翻转 随机剪裁等， label变为onehot\n","    :param img_file:\n","    :param label:\n","    :return:\n","    '''\n","    # 对图片的处理\n","    img = tf.io.read_file(img_file)\n","    # img = tf.image.grayscale_to_rgb(img, name=None)\n","    # img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE, 3] )\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.random_flip_left_right(img)\n","    img = tf.image.resize(img, (IMG_SIZE + 5, IMG_SIZE + 5))\n","    img = tf.image.random_crop(img, (IMG_SIZE, IMG_SIZE, 3))\n","    img = tf.cast(img, DTYPE) / 255.0\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.0740, 0.0740, 0.0740], dtype=DTYPE), axis=0)\n","    img = (img-mean)/std\n","    # 对标签的处理\n","    label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","    label = tf.cast(label, dtype=DTYPE)\n","    return {'images': img, 'labels': label}\n","\n","\n","# 制作无标签的数据集\n","def unlabel_image(img_file, label):\n","    '''\n","    处理无标签数据\n","    :param img_file:\n","    :param label:\n","    :return: 两张图片，一张经过轻微变换后的图片称为ori_image 一张经过较为剧烈变化后的图片，称为aug_images\n","    '''\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # 此图片作为原始图片\n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","    # aug_image = mask_label(img)\n","    aug_image = aug.distort(img)\n","    \n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, DTYPE) / 255.0\n","    ori_image = tf.cast(ori_image, DTYPE) / 255.0\n","\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.2153,0.2153,0.2153], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.2196, 0.2196, 0.2196], dtype=DTYPE), axis=0)\n","\n","    aug_image = (aug_image-mean)/std\n","    ori_image = (ori_image-mean)/std\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n","def merge_dataset(label_data, unlabel_data):\n","    return label_data['images'], label_data['labels'], unlabel_data['ori_images'], unlabel_data['aug_images']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9pzS2iIWfvl"},"source":["dfS['label'].loc[(dfS.label != 'Normal')] = 'Tumor'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJRqMt48W7Ge","executionInfo":{"elapsed":11,"status":"ok","timestamp":1628219459734,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"d5c62d75-e73f-40b3-c794-e9eb0e151488"},"source":["dfS.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>mask</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/0.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/0.jpg</td>\n","      <td>Tumor</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/1.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/1.jpg</td>\n","      <td>Tumor</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/2.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/2.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/3.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/3.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/4.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/4.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        image  ...   label\n","0  /content/drive/MyDrive/Thesis/pos/xS/0.jpg  ...   Tumor\n","1  /content/drive/MyDrive/Thesis/pos/xS/1.jpg  ...   Tumor\n","2  /content/drive/MyDrive/Thesis/pos/xS/2.jpg  ...  Normal\n","3  /content/drive/MyDrive/Thesis/pos/xS/3.jpg  ...  Normal\n","4  /content/drive/MyDrive/Thesis/pos/xS/4.jpg  ...  Normal\n","\n","[5 rows x 3 columns]"]},"execution_count":25,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8nsCaSe87M0f","outputId":"c89c1316-d68f-4fca-95a5-9c5ba8bbf88a"},"source":["#@title C MLPTrain\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import tensorflow_addons as tfa\n","# from WideResnet import WideResnet\n","from copy import deepcopy\n","import sklearn\n","from sklearn import preprocessing\n","\n","# import config\n","# from Model import Wrn28k\n","# from UdaCrossEntroy import UdaCrossEntroy\n","# from learningRate import LearningRate\n","# from Dataset import label_image\n","# from Dataset import unlabel_image\n","# from Dataset import merge_dataset\n","# from test import test\n","\n","\n","def my_update(model, model_):\n","    for i in range(len(model_)):\n","        model.weights[i] = model.weights[i].assign(\n","            model.weights[i]*(1-EMA)+model_[i]*EMA)\n","    model_ = deepcopy(model.weights)\n","    return model, model_\n","\n","\n","if __name__ == '__main__':\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","    # 有标签的数据集 batch_size=BATCH_SIZE\n","    # df_label = pd.read_csv(LABEL_FILE_PATH)\n","    le = preprocessing.LabelEncoder()\n","    dfS['label'] = le.fit_transform(dfS.label.values)\n","\n","    u_file_paths = []\n","    u_labels = []\n","    for i in range(0, len(data)): \n","      path = root + data[\"fullPath\"][i]#2499\n","      path = path.replace('\\\\', '/')\n","      path = path.replace('.png', '.jpg')\n","      u_file_paths.append(path)\n","      if data[\"Status\"][i] ==\"Cancer\":\n","        u_labels.append(3)\n","      elif data[\"Status\"][i] == \"Normal\":\n","        u_labels.append(1)\n","      elif data[\"Status\"][i] == \"Benign\" :\n","        u_labels.append(2)\n","  \n","    train_dfS = dfS[:int(len(dfS)*0.7)] \n","    test_dfS = dfS[-int(len(dfS)*0.7):]\n","    t_file_paths = test_dfS['image'].values\n","    t_labels = test_dfS['label'].values\n","    file_paths = train_dfS['image'].values\n","    labels = train_dfS['label'].values\n","    ds_label_train = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n","    ds_label_train = ds_label_train \\\n","        .map(label_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=50) \\\n","        .batch(BATCH_SIZE, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 无标签的数据集 batch_size=BATCH_SIZE*UDA_DATA\n","    # df_unlabel = pd.read_csv(UNLABEL_FILE_PATH)\n","    # file_paths = df_unlabel['name'].values\n","    # labels = df_unlabel['label'].values\n","    ds_unlabel_train = tf.data.Dataset.from_tensor_slices((u_file_paths, u_labels))\n","    ds_unlabel_train = ds_unlabel_train \\\n","        .map(unlabel_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=50) \\\n","        .batch(BATCH_SIZE * UDA_DATA, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # 将有标签数据和无标签数据整合成最终的数据形式\n","    ds_train = tf.data.Dataset.zip((ds_label_train, ds_unlabel_train))\n","    ds_train = ds_train.map(merge_dataset)\n","\n","    # 构建teacher模型\n","    if TEA_CONTINUE:\n","        print('continue teacher training')\n","        teacher = modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","        teacher.load_weights(TEA_LOAD_PATH)\n","        teacher.training = True\n","    else:\n","        # teacher = Wrn28k(num_inp_filters=3, k=2)\n","        teacher =  modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","\n","    # 构建student模型\n","    if STD_CONTINUE:\n","        print('continue student training')\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape,  pretrained=False)\n","        student.load_weights(STD_LOAD_PATH)\n","        student.training = True\n","        student = tf.saved_model.load(STD_LOAD_PATH)\n","    else:\n","        # student = Wrn28k(num_inp_filters=3, k=2)\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","    student_ = student.weights\n","\n","    # 定义teacher的损失函数，损失函数之一为UdaCrossEntroy\n","    mpl_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","    # 定义student的损失函数， PS：teacher的损失函数为UdaCrossEntroy\n","    s_unlabel_loss = tf.losses.CategoricalCrossentropy(\n","        label_smoothing=LABEL_SMOOTHING,\n","        from_logits=True,\n","        reduction=tf.keras.losses.Reduction.NONE,\n","    )\n","\n","    s_label_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.keras.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","\n","    # 定义teacher的学习率\n","    Tea_lr_fun = LearningRate(\n","        TEACHER_LR,\n","        TEACHER_LR_WARMUP_STEPS,\n","        TEACHER_NUM_WAIT_STEPS\n","    )\n","    # 定义student的学习率\n","    Std_lr_fun = LearningRate(\n","        STUDENT_LR,\n","        STUDENT_LR_WARMUP_STEPS,\n","        STUDENT_LR_WAIT_STEPS\n","    )\n","\n","    global_step = 62*CONTINUE_EPOCH\n","    print(f'start training from global step {global_step}......')\n","    TBacc = 0.78\n","    Tacc = 0\n","    SBacc = 0.31\n","    Sacc = 0\n","    epochs = MAX_EPOCHS - CONTINUE_EPOCH\n","    for epoch in range(epochs):\n","        TLOSS = 0\n","        TLOSS_1 = 0\n","        TLOSS_2 = 0\n","        TLOSS_3 = 0\n","        SLOSS = 0\n","        for batch_idx, (l_images, l_labels, ori_images, aug_images) in enumerate(ds_train):\n","            global_step += 1\n","            all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [15, 32, 32, 3]\n","            u_aug_and_l_images = tf.concat([aug_images, l_images], axis=0)\n","            # step1：经过teacher，得到输出\n","            with tf.GradientTape() as t_tape:\n","                output = teacher(all_images)  # shape=[15, 10]\n","                logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, global_step)\n","            # step2：1st call student -----------------------------\n","            with tf.GradientTape() as s_tape:\n","                logits['s_on_aug_and_l'] = student(u_aug_and_l_images)  # shape=[8, 10]\n","                logits['s_on_u'], logits['s_on_l_old'] = tf.split(\n","                    logits['s_on_aug_and_l'],\n","                    [aug_images.shape[0], l_images.shape[0]],\n","                    axis=0\n","                )\n","                cross_entroy['s_on_u'] = s_unlabel_loss(\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], -1)),\n","                    y_pred=logits['s_on_u']\n","                )\n","                # 计算损失函数\n","                cross_entroy['s_on_u'] = tf.reduce_sum(cross_entroy['s_on_u']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=tf.float32)\n","                SLOSS += cross_entroy['s_on_u']\n","                # for taylor\n","                cross_entroy['s_on_l_old'] = s_label_loss(\n","                    y_true=labels['l'],\n","                    y_pred=logits['s_on_l_old']\n","                )\n","\n","                cross_entroy['s_on_l_old'] = tf.reduce_sum(cross_entroy['s_on_l_old']) / \\\n","                                             tf.convert_to_tensor(BATCH_SIZE, dtype=tf.float32)\n","            # 反向传播，更新student的参数-------\n","            StudentLR = Std_lr_fun.__call__(global_step=global_step)\n","            StdOptim = keras.optimizers.SGD(\n","                learning_rate=StudentLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # StdOptim = keras.optimizers.Adam(learning_rate=StudentLR)\n","            GStud_unlabel = s_tape.gradient(cross_entroy['s_on_u'], student.trainable_variables)\n","            GStud_unlabel, _ = tf.clip_by_global_norm(GStud_unlabel, GRAD_BOUND)\n","            StdOptim.apply_gradients(zip(GStud_unlabel, student.trainable_variables))\n","            # 如何更新参数\n","            student, student_ = my_update(student, student_)\n","\n","            # step3: 2nd call student ------------------------------\n","            logits['s_on_l_new'] = student(l_images)\n","            cross_entroy['s_on_l_new'] = s_label_loss(\n","                y_true=labels['l'],\n","                y_pred=logits['s_on_l_new']\n","            )\n","            cross_entroy['s_on_l_new'] = tf.reduce_sum(cross_entroy['s_on_l_new']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE, dtype=DTYPE)\n","            dot_product = cross_entroy['s_on_l_new'] - cross_entroy['s_on_l_old']\n","            limit = 3.0**(0.5)\n","            moving_dot_product = tf.random_uniform_initializer(minval=-limit, maxval=limit)(shape=dot_product.shape)\n","            moving_dot_product = tf.Variable(initial_value=moving_dot_product, trainable=False, dtype=DTYPE)\n","            moving_dot_product_update = moving_dot_product.assign_sub(0.01 * (moving_dot_product - dot_product))\n","            dot_product = dot_product - moving_dot_product\n","            dot_product = tf.stop_gradient(dot_product)\n","            # step4: 求teacher的损失函数\n","            with t_tape:\n","                # label = tf.math.argmax(tf.nn.softmax(logits['aug'], axis=-1), axis=-1)\n","                # label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","                cross_entroy['mpl'] = mpl_loss(\n","                    # y_true=tf.stop_gradient(label),\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], axis=-1)),\n","                    y_pred=logits['aug']\n","                )  # 恒正\n","                cross_entroy['mpl'] = tf.reduce_sum(cross_entroy['mpl']) / \\\n","                                      tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=DTYPE)\n","                uda_weight = UDA_WEIGHT * tf.math.minimum(\n","                    1., tf.cast(global_step, DTYPE) / float(UDA_STEPS)\n","                )\n","                # if StudentLR == 0:\n","                #     dot_product = 0\n","                teacher_loss = cross_entroy['u'] * uda_weight + \\\n","                               cross_entroy['l'] + \\\n","                               cross_entroy['mpl'] * dot_product\n","\n","                TLOSS += teacher_loss\n","                TLOSS_1 += (cross_entroy['u'] * uda_weight)\n","                TLOSS_2 += cross_entroy['l']\n","                TLOSS_3 += cross_entroy['mpl'] * dot_product\n","            # 反向传播，更新teacher的参数-------\n","            TeacherLR = Tea_lr_fun.__call__(global_step=global_step)\n","            TeaOptim = keras.optimizers.SGD(\n","                learning_rate=TeacherLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # TeaOptim = keras.optimizers.Adam(learning_rate=TeacherLR)\n","            GTea = t_tape.gradient(teacher_loss, teacher.trainable_variables)\n","            GTea, _ = tf.clip_by_global_norm(GTea, GRAD_BOUND)\n","            TeaOptim.apply_gradients(zip(GTea, teacher.trainable_variables))\n","\n","            if (batch_idx + 1) % LOG_EVERY == 0:\n","                TLOSS = TLOSS / LOG_EVERY\n","                TLOSS_1 = TLOSS_1 / LOG_EVERY\n","                TLOSS_2 = TLOSS_2 / LOG_EVERY\n","                TLOSS_3 = TLOSS_3 / LOG_EVERY\n","                SLOSS = SLOSS / LOG_EVERY\n","                print(f'global: %4d' % global_step + ',[epoch:%4d/' % (epoch+CONTINUE_EPOCH) + 'EPOCH: %4d] \\t' % epochs\n","                      + '[U:%.4f' % (TLOSS_1) + ', L:%.4f' % (TLOSS_2) + ', M:%.4f' % (\n","                          TLOSS_3) + ']' + '[TLoss: %.4f]' % TLOSS + '/[SLoss: %.4f]' % SLOSS\n","                      + '\\t[TLR: %.6f' % TeacherLR + ']/[SLR: %.6f]' % StudentLR)\n","                TLOSS = 0\n","                TLOSS_1 = 0\n","                TLOSS_2 = 0\n","                TLOSS_3 = 0\n","                SLOSS = 0\n","        # 测试teacher在test上的acc\n","        if epoch % 5 == 0:\n","            Tacc = test(teacher, t_file_paths, t_labels)\n","            print(f'testing teacher model ... acc: {Tacc}')\n","        # 测试student在test上的acc，当student开始训练的时候\n","        if (StudentLR > 0) and (epoch % 5 == 0):\n","            Sacc = test(student, t_file_paths, t_labels)\n","            print(f'testing ... acc: {Sacc}')\n","        # 保存weights\n","        if Tacc > TBacc:\n","            Tsave_path = TEA_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            teacher.save_weights(Tsave_path)\n","            # tf.saved_model.save(teacher, Tsave_path)\n","            TBacc = Tacc\n","            print(f'saving for TBacc {TBacc}, Tpath:{Tsave_path}')\n","        if Sacc > SBacc:\n","            Ssave_path = STD_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            student.save_weights(Ssave_path)\n","            SBacc = Sacc\n","            print(f'saving for SBacc {SBacc}, Spath:{Ssave_path}')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","start training from global step 54870......\n"]}]}]}
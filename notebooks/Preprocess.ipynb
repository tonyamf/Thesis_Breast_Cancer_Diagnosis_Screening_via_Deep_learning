{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Preprocess.ipynb","provenance":[{"file_id":"1yhEeWMmcKO-sOIAo3tTeAQiOdbPwQPgu","timestamp":1627719755769}],"collapsed_sections":["Bf0ES9ZmG0DP","iEg4Ykf4Vyd6","ACEvRQcgbbKN","CV0-LhNMBBSA","m2nMbS40gHlM","-j9x5-Py5RcX"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNbWMEwcXI-C","executionInfo":{"elapsed":4053,"status":"ok","timestamp":1628219421662,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"1a7a9f94-1c5f-4da9-da5b-002028a1414a"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.13.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"dExRVQI8aLbl"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"K_X6Tj9TaBOF","executionInfo":{"elapsed":2082,"status":"ok","timestamp":1628219575226,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"3963a234-fe9a-42cc-8ca6-854f0a3d4888"},"source":["#@title Libraries\n","%pylab inline\n","import imutils\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import pylab as pylab\n","import matplotlib.image as mpimg\n","from PIL import Image as im\n","# import segmentation_models_pytorch as smp\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from scipy import ndimage, misc\n","from skimage.filters import threshold_otsu\n","from skimage.segmentation import clear_border\n","from skimage.measure import label, regionprops\n","from skimage.morphology import closing, square\n","from skimage.color import label2rgb\n","import matplotlib.patches as mpatches\n","from scipy.misc import face\n","from scipy.signal.signaltools import wiener\n","import sys\n","import numpy as np\n","import skimage.color\n","import skimage.filters\n","import skimage.io\n","import skimage.viewer\n","from skimage import feature, io, color, filters\n","from skimage.transform import hough_line, hough_line_peaks\n","from skimage.feature import canny\n","from skimage.filters import sobel\n","from skimage.draw import polygon\n","from skimage import exposure\n","from skimage.transform import resize\n","from PIL import Image\n","import scipy.ndimage as snd\n","#from meta-pseudo-labels.\n","from random import seed\n","from random import random\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: UserWarning: Viewer requires Qt\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bf0ES9ZmG0DP"},"source":["# Mount file syste,"]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"u2yW4Ik2Gy22","executionInfo":{"elapsed":1722,"status":"error","timestamp":1628220550573,"user":{"displayName":"Antonio Franco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpZWpPk6ug7_9xt90pVX8z1iULSGekWi2cTqST=s64","userId":"09756366898940169615"},"user_tz":-60},"outputId":"5de773f9-41bc-46ba-af33-e2e0dd428a80"},"source":["#@title Driver mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b12896192220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Driver mount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"w7kfgMnUadk0"},"source":["# Import data"]},{"cell_type":"code","metadata":{"id":"X30cgZKOacpN"},"source":["\n","# import os\n","# input_dir = \"/content/drive/MyDrive/Thesis/MINI-DDSM-Complete-JPEG-8/Data.xlsx\"\n","root = '\\\\content\\\\drive\\\\MyDrive\\\\Thesis\\\\MINI-DDSM-Complete-JPEG-8\\\\'\n","# dfAll = pd.read_excel(input_dir)\n","# dfAll.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"mToRCE_3ACK8","executionInfo":{"elapsed":526,"status":"error","timestamp":1628219586509,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"96780497-1cdf-46e1-bc3d-18f9b187a445"},"source":["data = pd.read_csv('/content/drive/MyDrive/Thesis/pos/data.csv')"],"execution_count":null,"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-329df88fe703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Thesis/pos/data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Thesis/pos/data.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"iEg4Ykf4Vyd6"},"source":["# Enchecement"]},{"cell_type":"code","metadata":{"id":"1FTwjLg3ESUy"},"source":["#@title contrast_streching\n","def contrast_streching(img):\n","  img1 = img\n","  minmax_img = np.zeros((img1.shape[0],img1.shape[1]),dtype = 'uint8')\n","  for i in range(img1.shape[0]):\n","      for j in range(img1.shape[1]):\n","          minmax_img[i,j] = 255*(img1[i,j]-np.min(img1))/(np.max(img1)-np.min(img1))\n","  \n","  return minmax_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"tZJv2OGmV1pu"},"source":["#@title morphological_enhancement\n","def morphological_enhancement(imag):\n","  # imag = cv2.imread(path, 0)\n","  rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n","  tophat = cv2.morphologyEx(imag, cv2.MORPH_TOPHAT, rectKernel)\n","  blackhat = cv2.morphologyEx(imag, cv2.MORPH_BLACKHAT, rectKernel)\n","  imag = imag + tophat - blackhat\n","\n","\n","  return imag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"fAE_0Wg24lgc"},"source":["#@title clahe\n","def clahe(img, i):\n","\n","  clahe = cv2.createCLAHE(clipLimit=i, tileGridSize=( grid_l,  grid_w))\n","  cl1 = clahe.apply(img)\n","  return cl1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACEvRQcgbbKN"},"source":["# Image pre processing"]},{"cell_type":"code","metadata":{"cellView":"form","id":"qaZRfCZGou7i"},"source":["#@title Mask_label\n","def mask_label(image):\n","  #grid = int((image.shape[0]+image.shape[1])/1000)*3\n","  \n","  # img_clahe = clahe(image)\n","  # img_md_n = ndimage.median_filter(image, 3)\n","  # img_md = cv2.dilate(image, None, iterations=3)\n","\n","  img_md = cv2.medianBlur(image, 7)\n","  thresh = cv2.adaptiveThreshold(img_md, 255,\n","    cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 10)\n","  thresh = cv2.dilate(thresh, None, iterations=5)\n","  cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n","  cv2.CHAIN_APPROX_SIMPLE)[0]\n","  for cnt in cnts: \n","    cv2.drawContours(thresh, [cnt], 0, 255,-1)\n","\n","  cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n","  cv2.CHAIN_APPROX_SIMPLE)[0]\n","  for cnt in cnts: \n","      cv2.drawContours(image, [cnt], 0, 0,-1)\n","\n","  img_md = ndimage.median_filter(image, 7)\n","  # img_md = cv2.GaussianBlur(img,(5,5),0)\n","\n","  left_nonzero = cv2.countNonZero(image[:, 0:int(image.shape[1]/2)])\n","  right_nonzero = cv2.countNonZero(image[:, int(image.shape[1]/2):])\n","  # flip_n = left_nonzero\n","  n =  np.median(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > 0] )\n","  n_l = len(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > n] )\n","  \n","  if(left_nonzero < right_nonzero):\n","    # flip_n = right_nonzero\n","    n =  np.median(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","    n_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > n] )\n","  #     print('wtf')\n","  #     flip = True\n","  #     image = cv2.flip(image, 1)\n","  # img_md = cv2.dilate(img_md, None, iterations=7)\n","  # image_eq = cv2.equalizeHist(img_md)\n","  # ret,thresh1 = cv2.threshold(image_eq, 0,255, cv2.THRESH_OTSU)\n","  # np.mean(cv2.countNonZero(image[:, 0:int(image.shape[1]/2)]))\n","  # np.where(np.nonzero(image[:, 0:int(image.shape[1]/2)]\n","\n","  ret,thresh  = cv2.threshold(img_md,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n","  thresh = cv2.dilate(thresh, None, iterations=5)\n","  # contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n","  # contours2, hier2 = cv2.findContours(thresh2,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n","  # cnts = cv2.findContours(thresh1.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","  # cnts = imutils.grab_contours(cnts)\n","  # contours,hierarchy = cv2.findContours(thresh2, 1, 2)\n","\n","  # find contours in thresholded image, then grab the largest\n","  # one\n","  cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n","    cv2.CHAIN_APPROX_SIMPLE)\n","  cnts = imutils.grab_contours(cnts)\n","  c = max(cnts, key=cv2.contourArea)\n","  max2 = 0\n","  for cnt in cnts:\n","    if cv2.contourArea(cnt) > max2 and cv2.contourArea(cnt) < cv2.contourArea(c):\n","      max2 = cv2.contourArea(cnt)\n","  # # x_m = image.shape[0] - image.shape[0]/5\n","  # # x_n = image.shape[0]/5\n","  # # y_m =  image.shape[1] - image.shape[1]/4.2\n","  # d_image = np.sqrt( (image.shape[0]**2) + (image.shape[1]**2) )\n","  # max1 = 0\n","  # # max2 = 0\n","  # for cnt in contours:\n","  #   if cv2.contourArea(cnt) > max1:\n","  #     max1 = cv2.contourArea(cnt)\n","  # left_nonzero = cv2.countNonZero(image[:, 0:int(image.shape[1]/2)])\n","  # right_nonzero = cv2.countNonZero(image[:, int(image.shape[1]/2):])\n","  # # flip_n = left_nonzero\n","  # n =  np.mean(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > 0] )\n","  # n_l = len(image[:, 0:int(image.shape[1]/2)][image[:, 0:int(image.shape[1]/2)] > n] )\n","  # # s =  np.mean(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > 0] )\n","  # # s_l = len(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > s] )\n","  # # o =  np.mean(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","  # # o_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > o] )\n","  # if(left_nonzero < right_nonzero):\n","  #   # flip_n = right_nonzero\n","  #   n =  np.mean(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","  #   n_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > n] )\n","    # o =  np.mean(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > 0] )\n","    # o_l = len(thresh[:, 0:int(thresh.shape[1]/2)][thresh[:, 0:int(thresh.shape[1]/2)] > o] )\n","    # s =  np.mean(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > 0] )\n","    # s_l = len(image[:, int(image.shape[1]/2):][image[:, int(image.shape[1]/2):] > s] )\n","\n","\n","\n","  # if cv2.countNonZero(thresh) < n_l:\n","  #   pass\n","    # for cnt in cnts: \n","    #   if cv2.contourArea(cnt) < cv2.contourArea(c):\n","    #   #   hausdorff_sd = cv2.createHausdorffDistanceExtractor()\n","    #     (x,y,w,h) = cv2.boundingRect(cnt)\n","    #     cv2.drawContours(img_md, [cnt], 0, 0,-1)\n","\n","  \n","  for cnt in cnts: \n","    if cv2.contourArea(cnt) < max2:\n","      cv2.drawContours(thresh, [cnt], 0, 0,-1)\n","  if cv2.countNonZero(thresh) < n_l:\n","    pass\n","  else:\n","    image[np.where(thresh == 0)] = 0 \n","\n","      # cv2.(img_md,(x,y),(x+w,y+h),0,-1)\n","      # mask = np.zeros((img_md.shape), dtype=np.uint8)\n","      # cv2.fillPoly(img_md, (x,y),(x+w,y+h),0,-1)\n","      # img_md[np.where(mask\n","    # # 4. Calculate the distance between contours\n","    #   if hausdorff_sd.computeDistance(c, cnt) > d_image/5:\n","  # hausdorff_sd = cv2.createHausdorffDistanceExtractor()  \n","  # for x in range(img_md.shape[0]):\n","  #   for y in range(img_md.shape[1]):\n","  #           # for the given pixel at w,h, lets check its value against the threshold\n","            \n","  #     if hausdorff_sd.computeDistance((x,y,1,1) , cnt) > d_image/5:\n","  #       img_md[x, y] = 0\n","        # M = cv2.moments(cnt)\n","        # if M[\"m00\"] < 1:\n","        #   M[\"m00\"] =1\n","        # cX = int(M[\"m10\"] / M[\"m00\"])\n","        # cY = int(M[\"m01\"] / M[\"m00\"])\n","        # (x,y,w,h) = cv2.boundingRect(cnt)\n","        # if cX > x_m or cX < x_n or cY > y_m:\n","      # cv2.rectangle(img_md,(x,y),(x+w,y+h),0,-1)\n","\n","\n","  # for cnt in contours:\n","  #   if cv2.contourArea(cnt) > max2 and cv2.contourArea(cnt) < max1:\n","  #     max2 = cv2.contourArea(cnt)\n","  # x_m = image.shape[0] - image.shape[0]/5\n","  # x_n = image.shape[0]/5\n","  # y_m =  image.shape[1] - image.shape[1]/4.2\n","  # for cnt in contours: \n","  #   M = cv2.moments(cnt)\n","  #   if M[\"m00\"] < 1:\n","  #     M[\"m00\"] =1\n","  #   cX = int(M[\"m10\"] / M[\"m00\"])\n","  #   cY = int(M[\"m01\"] / M[\"m00\"])\n","  #   (x,y,w,h) = cv2.boundingRect(cnt)\n","  #   if cX > x_m or cX < x_n or cY > y_m:\n","  #     print(x_m, x_n, y_m)\n","  #     print()\n","  #     print(cX, cY)\n","  #     cv2.rectangle(thresh2,(x,y),(x+w,y+h),0,-1)\n","\n","  # max1 = 0\n","  # max2 = 0\n","  # for cnt in contours2:\n","  #   if cv2.contourArea(cnt) > max1:\n","  #     max1 = cv2.contourArea(cnt)\n","\n","  # for cnt in contours2:\n","  #   if cv2.contourArea(cnt) > max2 and cv2.contourArea(cnt) < max1:\n","  #     max2 = cv2.contourArea(cnt)\n","\n","  # for cnt in contours2:      \n","  #     if cv2.contourArea(cnt) < max2 * (max2/max1):\n","  #         (x,y,w,h) = cv2.boundingRect(cnt)\n","  #         cv2.rectangle(thresh2,(x,y),(x+w,y+h),0,-1)\n","\n","  \n","  # lab_val = 255\n","  # kernel_size = 15\n","  # _, mammo_binary = cv2.threshold(img_md, 0, maxval=255, type=cv2.THRESH_BINARY)\n","  # n_labels, img_labeled, lab_stats, _ = cv2.connectedComponentsWithStats(\n","  #     thresh1, connectivity=8, ltype=cv2.CV_32S)\n","  # largest_obj_lab = np.argmax(lab_stats[1:, 4]) + 1\n","  # largest_mask = np.zeros(thresh1.shape, dtype=np.uint8)\n","  # largest_mask[img_labeled == largest_obj_lab] = lab_val\n","  \n","    \n","\n","  # fig, axes = plt.subplots(1, 2, figsize=(15,10))\n","  # fig.tight_layout(pad=3.0)\n","  # axes[0].set_title('Image')\n","  # axes[0].imshow(img_md, cmap=pylab.cm.gray)\n","  # axes[0].axis('on')\n","  # axes[1].set_title('Mask')\n","  # axes[1].imshow(thresh, cmap=pylab.cm.gray)\n","  # axes[1].axis('on')\n","  # plt.show()\n","  # if flip == True:\n","  #   image = cv2.flip(image, 1)\n","\n","  # plt.imshow(img, 'gray')\n","\n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"-f1kEIlmeiH0"},"source":["#@title right_orient_mammogram\n","\n","def right_orient_mammogram(image):\n","    flip = False\n","    left_nonzero = cv2.countNonZero(image[:, 0:int(image.shape[1]/2)])\n","    right_nonzero = cv2.countNonZero(image[:, int(image.shape[1]/2):])\n","    \n","    if(left_nonzero < right_nonzero):\n","        # print('wtf')\n","        flip = True\n","        image = cv2.flip(image, 1)\n","    # print(flip)    \n","    return [image, flip]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"vbh8_RuAzmMU"},"source":["#@title Apply_canny\n","def apply_canny(image):\n","    #img_eq = exposure.equalize_hist(image)\n","    canny_img = canny(image, 6)\n","    return sobel(canny_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"YC7QSQJf0MPQ"},"source":["#@title get_hough_lines\n","\n","def get_hough_lines(canny_img):\n","    h, theta, d = hough_line(canny_img)\n","    lines = list()\n","    # print('\\nAll hough lines')\n","    for _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n","        # print(\"Angle: {:.2f}, Dist: {:.2f}\".format(np.degrees(angle), dist))\n","        x1 = 0\n","        y1 = (dist - x1 * np.cos(angle)) / np.sin(angle)\n","        x2 = canny_img.shape[1]\n","        y2 = (dist - x2 * np.cos(angle)) / np.sin(angle)\n","        lines.append({\n","            'dist': dist,\n","            'angle': np.degrees(angle),\n","            'point1': [x1, y1],\n","            'point2': [x2, y2]\n","        })\n","    \n","    return lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"GKmaOOag0Vhx"},"source":["#@title shortlist_lines\n","def shortlist_lines(lines, max_dist):\n","    MIN_ANGLE = 1\n","    MAX_ANGLE = 30\n","    MIN_DIST  = max_dist/100\n","    MAX_DIST  = max_dist*1.2\n","    \n","    shortlisted_lines = [x for x in lines if \n","                          (x['dist']>=abs(MIN_DIST)) &\n","                          (x['dist']<=abs(MAX_DIST)) &\n","                          (x['angle']>=abs(MIN_ANGLE)) &\n","                          (x['angle']<=abs(MAX_ANGLE))\n","                        ]\n","    # print('\\nShorlisted lines')\n","    # for i in shortlisted_lines:\n","    #     print(\"Angle: {:.2f}, Dist: {:.2f}\".format(i['angle'], i['dist']))\n","        \n","    return shortlisted_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"ezTfIhIY1KyQ"},"source":["#@title remove_pectoral\n","def remove_pectoral(shortlisted_lines):\n","    shortlisted_lines.sort(key = lambda x: x['dist'])\n","    pectoral_line = shortlisted_lines[0].copy()\n","    # shortlisted_lines.pop(0)\n","    d = pectoral_line['dist']\n","    theta = np.radians(pectoral_line['angle'])\n","    \n","    x_intercept = d/np.cos(theta)\n","    y_intercept = d/np.sin(theta)\n","    \n","    return polygon([0, 0, y_intercept], [0, x_intercept, 0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sh6HC6skbk3_"},"source":["#@title breast_snip\n","def breast_snip(image, plotting=True):\n","    # img = io.imread(filename)\n","    # img = color.rgb2gray(img)\n","    # plt.imshow(img, 'gray')\n","    image = mask_label(image)\n","    flip_array = right_orient_mammogram(image)\n","    image = flip_array[0]\n","    # img = clahe(image.copy(), 2)\n","    img = image.copy()\n","    canny_img = apply_canny(img)\n","    lines = get_hough_lines(canny_img)\n","    shortlisted_lines = shortlist_lines(lines, img.shape[0]/1.5)\n","\n","    if plotting:\n","      fig, axes = plt.subplots(1, 4, figsize=(15,10))\n","      fig.tight_layout(pad=3.0)\n","      plt.xlim(0,img.shape[1])\n","      plt.ylim(img.shape[0])\n","      \n","      \n","      axes[0].set_title('Right-oriented mammogram')\n","      axes[0].imshow(img, cmap=pylab.cm.gray)\n","      axes[0].axis('on') \n","      \n","      axes[1].set_title('Hough Lines on Canny Edge img')\n","      axes[1].imshow(canny_img, cmap=pylab.cm.gray)\n","      axes[1].axis('on')\n","      axes[1].set_xlim(0,img.shape[1])\n","      axes[1].set_ylim(img.shape[0])\n","      for line in lines:\n","          axes[1].plot((line['point1'][0],line['point2'][0]), (line['point1'][1],line['point2'][1]), '-r')\n","          \n","      axes[2].set_title('Shortlisted Lines')\n","      axes[2].imshow(canny_img, cmap=pylab.cm.gray)\n","      axes[2].axis('on')\n","      axes[2].set_xlim(0,img.shape[1])\n","      axes[2].set_ylim(img.shape[0])\n","      for line in shortlisted_lines:\n","          axes[2].plot((line['point1'][0],line['point2'][0]), (line['point1'][1],line['point2'][1]), '-r')\n","    if size(shortlisted_lines) > 0:\n","      rr, cc = remove_pectoral(shortlisted_lines)\n","      try:\n","        image[rr, cc] = 0\n","      except:\n","        pass\n","    if plotting:\n","      axes[3].set_title('Pectoral muscle removed')\n","      axes[3].imshow(img, cmap=pylab.cm.gray)\n","      axes[3].axis('on')\n","      plt.show()\n","    if flip_array[1]:\n","      # print('yes')\n","      image = cv2.flip(image, 1)\n","    image = clahe(image, 2)\n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CV0-LhNMBBSA"},"source":["# Train data"]},{"cell_type":"code","metadata":{"cellView":"form","id":"5DEVOhArNWlY"},"source":["#@title check_path\n","\n","def check_path(path):\n","  path = path.replace('\\\\', '/')\n","  try:\n","    # mask_ = imread(path,0)    \n","    im1 = Image.open(path)\n","    #rgb_im = mask_.convert('RGB')\n","    im1.save(path.replace('.png', '.jpg'))\n","\n","    return path.replace('.png', '.jpg')\n","  except :\n","  \n","    try:\n","      # mask_ = imread(path,0) \n","      path = path.replace('MASK', 'Mask')\n","      path = path.replace('.png', '.jpg')   \n","      im1 = Image.open(path)\n","\n","      return path\n","    except :\n","      try:\n","        # mask_ = imread(path,0) \n","        path = path.replace('Mask', 'MASK')\n","        path = path.replace('.png', '.jpg')   \n","        im1 = Image.open(path)\n","\n","        return path\n","      except :\n","        try:\n","          path = path.replace('.png', '.jpg')\n","          return path\n","        except:\n","          pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"FroSgt4FJjn3"},"source":["#@title Variable for mask add\n","i_c = 0\n","i_s = 0\n","# last_mask = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9x0jd0jxvwX"},"source":["#@title add_image_and_mask 3 channels mask\n","def mask_add(img, data, index):\n","  i = index\n","  global i_c\n","  global i_s\n","  path = root + data[\"fullPath\"][i]#2499 2\n","  path = path.replace('\\\\', '/')\n","  path = path.replace('.png', '.jpg')\n","  Main_mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.uint8)\n","  masks = []\n","  if data['Tumour_Contour'][i] != '-':\n","    masks.append(check_path(root + data['Tumour_Contour'][i]))\n","  if data['Tumour_Contour2'][i] != '-':\n","    masks.append(check_path(root + data['Tumour_Contour2'][i]))\n","  if pd.isnull(data[\"Tumour_Contour3\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour3'][i]))\n","  if pd.isnull(data[\"Tumour_Contour4\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour4'][i]))\n","  if pd.isnull(data[\"Tumour_Contour5\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour5'][i]))\n","  if pd.isnull(data[\"Tumour_Contour6\"][i]) == False:\n","    masks.append(check_path(root + data['Tumour_Contour6'][i]))\n","  for mask in masks:\n","    ini_img = img.copy()\n","    mask_ = imread(mask)\n","    mask_ = cv2.resize(mask_, (ini_img.shape[0], ini_img.shape[1]))\n","    mask_[np.where(mask_ !=0)] = 255\n","    ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","    contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE) \n","    for cnt in contours:      \n","      (x,y,w,h) = cv2.boundingRect(cnt)\n","      cv2.drawContours(mask_, [cnt], 0,(255, 0, 0),-1)\n","    mask_ = np.expand_dims(resize(mask_, (img.shape[0], img.shape[1]), mode='constant',  \n","                                preserve_range=True), axis=-1)\n","    Main_mask = np.maximum(Main_mask, mask_)\n","    Main_mask[np.where(np.squeeze(Main_mask) !=0)] = 255\n","  cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i_s) + \".jpg\", img)\n","  cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i_s) + \".jpg\", np.squeeze(Main_mask))\n","  dfS.loc[i_s] = [\"/content/drive/MyDrive/Thesis/pos/Sx/\" + str(i_s) + \".jpg\", \"/content/drive/MyDrive/Thesis/pos/Sy/\" + str(i_s) + \".jpg\", data['Status'][i]]\n","  i_s = i_s + 1\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciedbWzIsjEw"},"source":["    try:\n","      ret2,thresh2 = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","      cnts = cv2.findContours(thresh2.copy(), cv2.RETR_EXTERNAL,\n","        cv2.CHAIN_APPROX_SIMPLE)\n","      cnts = imutils.grab_contours(cnts)\n","      c = max(cnts, key=cv2.contourArea)\n","      (x,y,w,h) = cv2.boundingRect(c)\n","      # print( np.where(np.nonzero(np.squeeze(Main_mask))))\n","      # ini_img = ini_img * mask_\n","      ini_img[np.where(mask_==0), 0] = 0\n","    \n","      gray = ini_img[y:y+h,x:x+w]\n","\n","\n","      img2 = np.zeros((gray.shape[0], gray.shape[1], 3), dtype=np.uint8)\n","      img2[:,:,0] = gray\n","      img2[:,:,1] = gray\n","      img2[:,:,2] = gray\n","\n","      cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", img2)\n","      dfC.loc[i_c] = [\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", data['Status'][i]]\n","      i_c = i_c+1\n","    except:\n","      pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4_C38jMsDOq"},"source":["  path = root + data[\"fullPath\"][364]#2499 2\n","  path = path.replace('\\\\', '/')\n","  path = path.replace('.png', '.jpg')\n","  img = cv2.imread('/content/drive/MyDrive/Thesis/MINI-DDSM-Complete-JPEG-8/Cancer/0044/C_0044_1.LEFT_CC.jpg')\n","  print(path)\n","  img.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wdPNw8UgAIa"},"source":["# Action pre processing"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Cn131CvgMGHw"},"source":["#@title Iniciate Dataframe\n","\n","dfC = pd.DataFrame(columns=['image', 'label'])\n","dfS = pd.DataFrame(columns=['image', 'mask', 'label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"IzwitLBI3X_8"},"source":["#@title Default title text\n","\n","for i in range(0, len(data)):\n","    path = root + data['fullPath'][i]  # 2499\n","    path = path.replace('\\\\', '/')\n","    path = path.replace('.png', '.jpg')\n","    img = cv2.imread(path, 0)\n","    ini_img = img.copy()\n","    grid_l = int(img.shape[0] * img.shape[0] / img.shape[1] / 600)\n","    grid_w = int(img.shape[1] * img.shape[1] / img.shape[0] / 200)\n","    if grid_l < 2:\n","        grid_l = 2\n","    if grid_w < 2:\n","        grid_w = 2\n","    thresh = clahe(ini_img, 2)\n","    # for r in range(0, 8):\n","    mask_add(thresh, data, i)\n","    #print(i)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"47lvrAtjM34W","executionInfo":{"elapsed":179,"status":"error","timestamp":1628219033587,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"27a20fa7-b782-4872-e039-5b49eb856fcf"},"source":["#@title Saving dataframe\n","# dfC.to_csv('/content/drive/MyDrive/Thesis/pos/dataC.csv', index = False)\n","dfS.to_csv('/content/drive/MyDrive/Thesis/pos/Sdata.csv', index = False)"],"execution_count":null,"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ad82cdafa270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Saving dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# dfC.to_csv('/content/drive/MyDrive/Thesis/pos/dataC.csv', index = False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Thesis/pos/Sdata.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dfS' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"0jZNus_PI3Y7","executionInfo":{"elapsed":188,"status":"ok","timestamp":1628214369517,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"4c9705e2-7088-4e1a-a178-ad854fc2a1d5"},"source":["dfS.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>mask</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/0.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/0.jpg</td>\n","      <td>Benign</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/1.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/1.jpg</td>\n","      <td>Cancer</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/2.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/2.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/3.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/3.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/4.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/4.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        image  ...   label\n","0  /content/drive/MyDrive/Thesis/pos/xS/0.jpg  ...  Benign\n","1  /content/drive/MyDrive/Thesis/pos/xS/1.jpg  ...  Cancer\n","2  /content/drive/MyDrive/Thesis/pos/xS/2.jpg  ...  Normal\n","3  /content/drive/MyDrive/Thesis/pos/xS/3.jpg  ...  Normal\n","4  /content/drive/MyDrive/Thesis/pos/xS/4.jpg  ...  Normal\n","\n","[5 rows x 3 columns]"]},"execution_count":122,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"ME-MDADsKVM0"},"source":["i_c = 0\n","for i in range(0, len(dfS)):\n","    img = cv2.imread(dfS['image'][i], 0)\n","    ini_img = img.copy()\n","\n","    mask_ = cv2.imread(dfS['mask'][i], 0)\n","    ret,thresh = cv2.threshold(mask_, 0,255, cv2.THRESH_OTSU)\n","    contours, hier = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)                   \n","    if dfS[\"label\"][i] != 'Normal':\n","      for cnt in contours:      \n","        (x,y,w,h) = cv2.boundingRect(cnt)\n","        cv2.drawContours(mask_, [cnt], 0,(255, 0, 0),-1)\n","      ini_img[np.where(mask_==0)] = 0\n","      cv2.rectangle(ini_img,(x,y),(x+w,y+h),(255,0,0),2)\n","      cv2.imwrite(\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", ini_img[y:y+h,x:x+w])\n","      dfC.loc[i_c] = [\"/content/drive/MyDrive/Thesis/pos/xC/\" + str(i_c) + \".jpg\", dfS['label'][i]]\n","      i_c = i_c + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYc-HBGyMDko"},"source":["dfC.to_csv('/content/drive/MyDrive/Thesis/pos/Cdata.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGK7gI-ZYZVP"},"source":["dfS = pd.read_csv('/content/drive/MyDrive/Thesis/pos/Sdata.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wJ6RUmtZs7k","executionInfo":{"elapsed":189,"status":"ok","timestamp":1628218826638,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"b6e37707-2b9c-4e39-8f17-c2aa3fb1d867"},"source":["l=cv2.imread(dfS['image'][i])\n","l.shape"],"execution_count":null,"outputs":[{"data":{"text/plain":["(2384, 1312, 3)"]},"execution_count":27,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"m2nMbS40gHlM"},"source":["# EfficientUnet"]},{"cell_type":"markdown","metadata":{"id":"nIJUmq6zgXPz"},"source":["## Efficientnet"]},{"cell_type":"code","metadata":{"cellView":"form","id":"q8dhNhDdgW58"},"source":["#@title Efficientnet\n","from keras import models, layers\n","# import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from tensorflow.keras.utils import get_file\n","# from utils import *\n","\n","__all__ = ['get_model_by_name', 'get_efficientnet_b0_encoder', 'get_efficientnet_b1_encoder',\n","           'get_efficientnet_b2_encoder', 'get_efficientnet_b3_encoder', 'get_efficientnet_b4_encoder',\n","           'get_efficientnet_b5_encoder', 'get_efficientnet_b6_encoder', 'get_efficientnet_b7_encoder']\n","\n","\n","def _efficientnet(input_shape, blocks_args_list, global_params):\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","\n","    # Stem part\n","    model_input = layers.Input(shape=input_shape)\n","    x = layers.Conv2D(\n","        filters=round_filters(32, global_params),\n","        kernel_size=[3, 3],\n","        strides=[2, 2],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='stem_conv2d'\n","    )(model_input)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='stem_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='stem_swish')(x)\n","\n","    # Blocks part\n","    idx = 0\n","    drop_rate = global_params.drop_connect_rate\n","    n_blocks = sum([blocks_args.num_repeat for blocks_args in blocks_args_list])\n","    drop_rate_dx = drop_rate / n_blocks\n","\n","    for blocks_args in blocks_args_list:\n","        assert blocks_args.num_repeat > 0\n","        # Update block input and output filters based on depth multiplier.\n","        blocks_args = blocks_args._replace(\n","            input_filters=round_filters(blocks_args.input_filters, global_params),\n","            output_filters=round_filters(blocks_args.output_filters, global_params),\n","            num_repeat=round_repeats(blocks_args.num_repeat, global_params)\n","        )\n","\n","        # The first block needs to take care of stride and filter size increase.\n","        x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","        idx += 1\n","\n","        if blocks_args.num_repeat > 1:\n","            blocks_args = blocks_args._replace(input_filters=blocks_args.output_filters, strides=[1, 1])\n","\n","        for _ in range(blocks_args.num_repeat - 1):\n","            x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n","            idx += 1\n","\n","    # Head part\n","    x = layers.Conv2D(\n","        filters=round_filters(1280, global_params),\n","        kernel_size=[1, 1],\n","        strides=[1, 1],\n","        kernel_initializer=conv_kernel_initializer,\n","        padding='same',\n","        use_bias=False,\n","        name='head_conv2d'\n","    )(x)\n","\n","    x = layers.BatchNormalization(\n","        momentum=batch_norm_momentum,\n","        epsilon=batch_norm_epsilon,\n","        name='head_batch_norm'\n","    )(x)\n","\n","    x = Swish(name='head_swish')(x)\n","\n","    x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n","\n","    if global_params.dropout_rate > 0:\n","        x = layers.Dropout(global_params.dropout_rate)(x)\n","\n","    x = layers.Dense(\n","        global_params.num_classes,\n","        kernel_initializer=dense_kernel_initializer,\n","        activation='softmax',\n","        name='head_dense'\n","    )(x)\n","\n","    model = models.Model(model_input, x)\n","\n","    return model\n","\n","\n","def get_model_by_name(model_name, input_shape, classes=3, pretrained=False):\n","    \"\"\"Get an EfficientNet model by its name.\n","    \"\"\"\n","    blocks_args, global_params = get_efficientnet_params(model_name, override_params={'num_classes': classes})\n","    model = _efficientnet(input_shape, blocks_args, global_params)\n","\n","    try:\n","        if pretrained:\n","            weights = IMAGENET_WEIGHTS[model_name]\n","            weights_path = get_file(\n","                weights['name'],\n","                weights['url'],\n","                cache_subdir='models',\n","                md5_hash=weights['md5'],\n","            )\n","            model.load_weights(weights_path)\n","    except KeyError as e:\n","        print(\"NOTE: Currently model {} doesn't have pretrained weights, therefore a model with randomly initialized\"\n","              \" weights is returned.\".format(e))\n","\n","    return model\n","\n","\n","def _get_efficientnet_encoder(model_name, input_shape, pretrained=False):\n","    model = get_model_by_name(model_name, input_shape, pretrained=pretrained)\n","    encoder = models.Model(model.input, model.get_layer('global_average_pooling2d').output)\n","    encoder.layers.pop()  # remove GAP layer\n","    return encoder\n","\n","\n","def get_efficientnet_b0_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b0', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b1_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b1', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b2_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b2', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b3_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b3', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b4_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b4', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b5_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b5', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b6_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b6', input_shape, pretrained=pretrained)\n","\n","\n","def get_efficientnet_b7_encoder(input_shape, pretrained=False):\n","    return _get_efficientnet_encoder('efficientnet-b7', input_shape, pretrained=pretrained)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iz1mgr0_goYL"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"qje92-TUkfjU"},"source":["#@title number of classes\n","\n","n_classes=2 #@param {type:\"integer\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"PR4BA07zgNwD"},"source":["#@title Utils\n","import re\n","from collections import namedtuple\n","from keras import layers\n","import keras.backend as K\n","import tensorflow as tf\n","import math\n","import numpy as np\n","\n","GlobalParams = namedtuple('GlobalParams', ['batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'num_classes',\n","                                           'width_coefficient', 'depth_coefficient', 'depth_divisor', 'min_depth',\n","                                           'drop_connect_rate'])\n","global_params = None\n","GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n","\n","BlockArgs = namedtuple('BlockArgs', ['kernel_size', 'num_repeat', 'input_filters', 'output_filters', 'expand_ratio',\n","                                     'id_skip', 'strides', 'se_ratio'])\n","BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n","\n","IMAGENET_WEIGHTS = {\n","\n","    'efficientnet-b0': {\n","        'name': 'efficientnet-b0_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000.h5',\n","        'md5': 'bca04d16b1b8a7c607b1152fe9261af7',\n","    },\n","\n","    'efficientnet-b1': {\n","        'name': 'efficientnet-b1_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000.h5',\n","        'md5': 'bd4a2b82f6f6bada74fc754553c464fc',\n","    },\n","\n","    'efficientnet-b2': {\n","        'name': 'efficientnet-b2_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000.h5',\n","        'md5': '45b28b26f15958bac270ab527a376999',\n","    },\n","\n","    'efficientnet-b3': {\n","        'name': 'efficientnet-b3_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000.h5',\n","        'md5': 'decd2c8a23971734f9d3f6b4053bf424',\n","    },\n","\n","    'efficientnet-b4': {\n","        'name': 'efficientnet-b4_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000.h5',\n","        'md5': '01df77157a86609530aeb4f1f9527949',\n","    },\n","\n","    'efficientnet-b5': {\n","        'name': 'efficientnet-b5_imagenet_1000.h5',\n","        'url': 'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000.h5',\n","        'md5': 'c31311a1a38b5111e14457145fccdf32',\n","    }\n","\n","}\n","\n","\n","def round_filters(filters, global_params):\n","    \"\"\"Round number of filters.\"\"\"\n","    multiplier = global_params.width_coefficient\n","    divisor = global_params.depth_divisor\n","    min_depth = global_params.min_depth\n","    if not multiplier:\n","        return filters\n","\n","    filters *= multiplier\n","    min_depth = min_depth or divisor\n","    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_filters < 0.9 * filters:\n","        new_filters += divisor\n","    return int(new_filters)\n","\n","\n","def round_repeats(repeats, global_params):\n","    \"\"\"Round number of repeats.\"\"\"\n","    multiplier = global_params.depth_coefficient\n","    if not multiplier:\n","        return repeats\n","    return int(math.ceil(multiplier * repeats))\n","\n","\n","def get_efficientnet_params(model_name, override_params=None):\n","    \"\"\"Get efficientnet params based on model name.\"\"\"\n","    params_dict = {\n","        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n","        # Note: the resolution here is just for reference, its values won't be used.\n","        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n","        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n","        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n","        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n","        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n","        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n","        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n","        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n","    }\n","    if model_name not in params_dict.keys():\n","        raise KeyError('There is no model named {}.'.format(model_name))\n","\n","    width_coefficient, depth_coefficient, _, dropout_rate = params_dict[model_name]\n","\n","    blocks_args = [\n","        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n","        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n","        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n","        'r1_k3_s11_e6_i192_o320_se0.25',\n","    ]\n","    global_params = GlobalParams(\n","        batch_norm_momentum=0.99,\n","        batch_norm_epsilon=1e-3,\n","        dropout_rate=dropout_rate,\n","        drop_connect_rate=0.2,\n","        num_classes=n_classes,\n","        width_coefficient=width_coefficient,\n","        depth_coefficient=depth_coefficient,\n","        depth_divisor=8,\n","        min_depth=None)\n","\n","    if override_params:\n","        global_params = global_params._replace(**override_params)\n","\n","    decoder = BlockDecoder()\n","    return decoder.decode(blocks_args), global_params\n","\n","\n","class BlockDecoder(object):\n","    \"\"\"Block Decoder for readability.\"\"\"\n","\n","    @staticmethod\n","    def _decode_block_string(block_string):\n","        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n","        assert isinstance(block_string, str)\n","        ops = block_string.split('_')\n","        options = {}\n","        for op in ops:\n","            splits = re.split(r'(\\d.*)', op)\n","            if len(splits) >= 2:\n","                key, value = splits[:2]\n","                options[key] = value\n","\n","        if 's' not in options or len(options['s']) != 2:\n","            raise ValueError('Strides options should be a pair of integers.')\n","\n","        return BlockArgs(\n","            kernel_size=int(options['k']),\n","            num_repeat=int(options['r']),\n","            input_filters=int(options['i']),\n","            output_filters=int(options['o']),\n","            expand_ratio=int(options['e']),\n","            id_skip=('noskip' not in block_string),\n","            se_ratio=float(options['se']) if 'se' in options else None,\n","            strides=[int(options['s'][0]), int(options['s'][1])]\n","        )\n","\n","    @staticmethod\n","    def _encode_block_string(block):\n","        \"\"\"Encodes a block to a string.\"\"\"\n","        args = [\n","            'r%d' % block.num_repeat,\n","            'k%d' % block.kernel_size,\n","            's%d%d' % (block.strides[0], block.strides[1]),\n","            'e%s' % block.expand_ratio,\n","            'i%d' % block.input_filters,\n","            'o%d' % block.output_filters\n","        ]\n","        if 0 < block.se_ratio <= 1:\n","            args.append('se%s' % block.se_ratio)\n","        if block.id_skip is False:\n","            args.append('noskip')\n","        return '_'.join(args)\n","\n","    def decode(self, string_list):\n","        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n","        Args:\n","          string_list: a list of strings, each string is a notation of block.\n","        Returns:\n","          A list of namedtuples to represent blocks arguments.\n","        \"\"\"\n","        assert isinstance(string_list, list)\n","        blocks_args = []\n","        for block_string in string_list:\n","            blocks_args.append(self._decode_block_string(block_string))\n","        return blocks_args\n","\n","    def encode(self, blocks_args):\n","        \"\"\"Encodes a list of Blocks to a list of strings.\n","        Args:\n","          blocks_args: A list of namedtuples to represent blocks arguments.\n","        Returns:\n","          a list of strings, each string is a notation of block.\n","        \"\"\"\n","        block_strings = []\n","        for block in blocks_args:\n","            block_strings.append(self._encode_block_string(block))\n","        return block_strings\n","\n","\n","class Swish(layers.Layer):\n","    def __init__(self, name=None, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","    def call(self, inputs, **kwargs):\n","        return tf.nn.silu(inputs)#tf.nn.swish I have changed this why I don't know yet\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['name'] = self.name\n","        return config\n","\n","\n","def SEBlock(block_args, **kwargs):\n","    num_reduced_filters = max(\n","        1, int(block_args.input_filters * block_args.se_ratio))\n","    filters = block_args.input_filters * block_args.expand_ratio\n","\n","    spatial_dims = [1, 2]\n","\n","    try:\n","        block_name = kwargs['block_name']\n","    except KeyError:\n","        block_name = ''\n","\n","    def block(inputs):\n","        x = inputs\n","        x = layers.Lambda(lambda a: K.mean(a, axis=spatial_dims, keepdims=True))(x)\n","        x = layers.Conv2D(\n","            num_reduced_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_reduce_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = Swish(name=block_name + 'se_swish')(x)\n","\n","        x = layers.Conv2D(\n","            filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'se_expand_conv2d',\n","            use_bias=True\n","        )(x)\n","\n","        x = layers.Activation('sigmoid')(x)\n","        out = layers.Multiply()([x, inputs])\n","        return out\n","\n","    return block\n","\n","\n","class DropConnect(layers.Layer):\n","\n","    def __init__(self, drop_connect_rate, **kwargs):\n","        super().__init__(**kwargs)\n","        self.drop_connect_rate = drop_connect_rate\n","\n","    def call(self, inputs, **kwargs):\n","        def drop_connect():\n","            keep_prob = 1.0 - self.drop_connect_rate\n","\n","            # Compute drop_connect tensor\n","            batch_size = tf.shape(inputs)[0]\n","            random_tensor = keep_prob\n","            random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n","            binary_tensor = tf.floor(random_tensor)\n","            output = tf.math.divide(inputs, keep_prob) * binary_tensor\n","            return output\n","\n","        return K.in_train_phase(drop_connect(), inputs, training=None)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config['drop_connect_rate'] = self.drop_connect_rate\n","        return config\n","\n","\n","def conv_kernel_initializer(shape, dtype=K.floatx()):\n","    \"\"\"Initialization for convolutional kernels.\n","    The main difference with tf.variance_scaling_initializer is that\n","    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n","    standard deviation, whereas here we use a normal distribution. Similarly,\n","    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n","    a corrected standard deviation.\n","    Args:\n","        shape: shape of variable\n","        dtype: dtype of variable\n","    Returns:\n","        an initialization for the variable\n","    \"\"\"\n","    kernel_height, kernel_width, _, out_filters = shape\n","    fan_out = int(kernel_height * kernel_width * out_filters)\n","    return tf.random.normal(\n","        shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n","\n","\n","def dense_kernel_initializer(shape, dtype=K.floatx()):\n","    init_range = 1.0 / np.sqrt(shape[1])\n","    return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)\n","\n","\n","def MBConvBlock(block_args, global_params, idx, drop_connect_rate=None):\n","    filters = block_args.input_filters * block_args.expand_ratio\n","    batch_norm_momentum = global_params.batch_norm_momentum\n","    batch_norm_epsilon = global_params.batch_norm_epsilon\n","    has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n","\n","    block_name = 'blocks_' + str(idx) + '_'\n","\n","    def block(inputs):\n","        x = inputs\n","\n","        # Expansion phase\n","        if block_args.expand_ratio != 1:\n","            expand_conv = layers.Conv2D(filters,\n","                                        kernel_size=[1, 1],\n","                                        strides=[1, 1],\n","                                        kernel_initializer=conv_kernel_initializer,\n","                                        padding='same',\n","                                        use_bias=False,\n","                                        name=block_name + 'expansion_conv2d'\n","                                        )(x)\n","            bn0 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                            epsilon=batch_norm_epsilon,\n","                                            name=block_name + 'expansion_batch_norm')(expand_conv)\n","\n","            x = Swish(name=block_name + 'expansion_swish')(bn0)\n","\n","        # Depth-wise convolution phase\n","        kernel_size = block_args.kernel_size\n","        depthwise_conv = layers.DepthwiseConv2D(\n","            [kernel_size, kernel_size],\n","            strides=block_args.strides,\n","            depthwise_initializer=conv_kernel_initializer,\n","            padding='same',\n","            use_bias=False,\n","            name=block_name + 'depthwise_conv2d'\n","        )(x)\n","        bn1 = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                        epsilon=batch_norm_epsilon,\n","                                        name=block_name + 'depthwise_batch_norm'\n","                                        )(depthwise_conv)\n","        x = Swish(name=block_name + 'depthwise_swish')(bn1)\n","\n","        if has_se:\n","            x = SEBlock(block_args, block_name=block_name)(x)\n","\n","        # Output phase\n","        project_conv = layers.Conv2D(\n","            block_args.output_filters,\n","            kernel_size=[1, 1],\n","            strides=[1, 1],\n","            kernel_initializer=conv_kernel_initializer,\n","            padding='same',\n","            name=block_name + 'output_conv2d',\n","            use_bias=False)(x)\n","        x = layers.BatchNormalization(momentum=batch_norm_momentum,\n","                                      epsilon=batch_norm_epsilon,\n","                                      name=block_name + 'output_batch_norm'\n","                                      )(project_conv)\n","        if block_args.id_skip:\n","            if all(\n","                    s == 1 for s in block_args.strides\n","            ) and block_args.input_filters == block_args.output_filters:\n","                # only apply drop_connect if skip presents.\n","                if drop_connect_rate:\n","                    x = DropConnect(drop_connect_rate)(x)\n","                x = layers.add([x, inputs])\n","\n","        return x\n","\n","    return block\n","\n","\n","def freeze_efficientunet_first_n_blocks(model, n):\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            model.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","\n","    all_block_names = ['blocks_{}_output_batch_norm'.format(i) for i in range(mbblock_nr)]\n","    all_block_index = []\n","    for idx, layer in enumerate(model.layers):\n","        if layer.name == all_block_names[0]:\n","            all_block_index.append(idx)\n","            all_block_names.pop(0)\n","            if len(all_block_names) == 0:\n","                break\n","    n_blocks = len(all_block_index)\n","\n","    if n <= 0:\n","        print('n is less than or equal to 0, therefore no layer will be frozen.')\n","        return\n","    if n > n_blocks:\n","        raise ValueError(\"There are {} blocks in total, n cannot be greater than {}.\".format(n_blocks, n_blocks))\n","\n","    idx_of_last_block_to_be_frozen = all_block_index[n - 1]\n","    for layer in model.layers[:idx_of_last_block_to_be_frozen + 1]:\n","        layer.trainable = False\n","\n","\n","def unfreeze_efficientunet(model):\n","    for layer in model.layers:\n","        layer.trainable = True\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZ29zXFqh5sm"},"source":["## *Efficientunet*"]},{"cell_type":"code","metadata":{"cellView":"form","id":"TGg2DkR-g-2L"},"source":["#@markdown Efficientnet-unet\n","import sys  \n","# sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL/EfficientUnet/efficientunet')\n","from keras.layers import *\n","from keras import models\n","# from efficientnet import *\n","# from utils import conv_kernel_initializer\n","\n","\n","__all__ = ['get_efficient_unet_b0', 'get_efficient_unet_b1', 'get_efficient_unet_b2', 'get_efficient_unet_b3',\n","           'get_efficient_unet_b4', 'get_efficient_unet_b5', 'get_efficient_unet_b6', 'get_efficient_unet_b7',\n","           'get_blocknr_of_skip_candidates']\n","\n","\n","def get_blocknr_of_skip_candidates(encoder, verbose=False):\n","    \"\"\"\n","    Get block numbers of the blocks which will be used for concatenation in the Unet.\n","    :param encoder: the encoder\n","    :param verbose: if set to True, the shape information of all blocks will be printed in the console\n","    :return: a list of block numbers\n","    \"\"\"\n","    shapes = []\n","    candidates = []\n","    mbblock_nr = 0\n","    while True:\n","        try:\n","            mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","            shape = int(mbblock.shape[1]), int(mbblock.shape[2])\n","            if shape not in shapes:\n","                shapes.append(shape)\n","                candidates.append(mbblock_nr)\n","            if verbose:\n","                print('blocks_{}_output_shape: {}'.format(mbblock_nr, shape))\n","            mbblock_nr += 1\n","        except ValueError:\n","            break\n","    return candidates\n","\n","\n","def DoubleConv(filters, kernel_size, initializer='glorot_uniform'):\n","\n","    def layer(x):\n","\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","def UpSampling2D_block(filters, kernel_size=(3, 3), upsample_rate=(2, 2), interpolation='bilinear',\n","                       initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = UpSampling2D(size=upsample_rate, interpolation=interpolation)(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","    return layer\n","\n","\n","def Conv2DTranspose_block(filters, kernel_size=(3, 3), transpose_kernel_size=(2, 2), upsample_rate=(2, 2),\n","                          initializer='glorot_uniform', skip=None):\n","    def layer(input_tensor):\n","\n","        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate, padding='same')(input_tensor)\n","\n","        if skip is not None:\n","            x = Concatenate()([x, skip])\n","\n","        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n","\n","        return x\n","\n","    return layer\n","\n","\n","# noinspection PyTypeChecker\n","def _get_efficient_unet(encoder, out_channels=2, block_type='upsampling', concat_input=True):\n","    MBConvBlocks = []\n","\n","    skip_candidates = get_blocknr_of_skip_candidates(encoder)\n","\n","    for mbblock_nr in skip_candidates:\n","        mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n","        MBConvBlocks.append(mbblock)\n","\n","    # delete the last block since it won't be used in the process of concatenation\n","    MBConvBlocks.pop()\n","\n","    input_ = encoder.input\n","    head = encoder.get_layer('head_swish').output\n","    blocks = [input_] + MBConvBlocks + [head]\n","\n","    if block_type == 'upsampling':\n","        UpBlock = UpSampling2D_block\n","    else:\n","        UpBlock = Conv2DTranspose_block\n","\n","    o = blocks.pop()\n","    o = UpBlock(512, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(256, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(128, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    o = UpBlock(64, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    if concat_input:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n","    else:\n","        o = UpBlock(32, initializer=conv_kernel_initializer, skip=None)(o)\n","    o = Conv2D(out_channels, (1, 1), padding='same', kernel_initializer=conv_kernel_initializer)(o)\n","\n","    model = models.Model(encoder.input, o)\n","\n","    return model\n","\n","\n","def get_efficient_unet_b0(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B0 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B0 model\n","    \"\"\"\n","    encoder = get_efficientnet_b0_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b1(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B1 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B1 model\n","    \"\"\"\n","    encoder = get_efficientnet_b1_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b2(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B2 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B2 model\n","    \"\"\"\n","    encoder = get_efficientnet_b2_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b3(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B3 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B3 model\n","    \"\"\"\n","    encoder = get_efficientnet_b3_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b4(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B4 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B4 model\n","    \"\"\"\n","    encoder = get_efficientnet_b4_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b5(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B5 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B5 model\n","    \"\"\"\n","    encoder = get_efficientnet_b5_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b6(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B6 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B6 model\n","    \"\"\"\n","    encoder = get_efficientnet_b6_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n","\n","\n","def get_efficient_unet_b7(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n","    \"\"\"Get a Unet model with Efficient-B7 encoder\n","    :param input_shape: shape of input (cannot have None element)\n","    :param out_channels: the number of output channels\n","    :param pretrained: True for ImageNet pretrained weights\n","    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n","    :param concat_input: if True, input image will be concatenated with the last conv layer\n","    :return: an EfficientUnet_B7 model\n","    \"\"\"\n","    encoder = get_efficientnet_b7_encoder(input_shape, pretrained=pretrained)\n","    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gg6Xw9x-iHkA"},"source":["## Create model"]},{"cell_type":"code","metadata":{"id":"mdJWcg-biObH"},"source":["Channels =  3#@param {type:\"integer\"}\n","Img_size = 416#@param {type:\"integer\"}\n","input_shape = (Img_size, Img_size, Channels) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPWwAz8SkX7P"},"source":["tf.keras.backend.clear_session()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rml63YcFiLxP","executionInfo":{"elapsed":7854,"status":"ok","timestamp":1628219451637,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"5c20bb38-9ab1-4d4d-a24a-114dba0e265b"},"source":["#@markdown Model Efficient unet\n","\n","classifier =  get_efficientnet_b5_encoder(input_shape, pretrained=False)\n","modelS = models.Sequential()\n","modelS.add(classifier)\n","modelS.add(layers.Dense(n_classes))\n","modelS.summary()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_1 (Functional)         (None, 2048)              28513520  \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 4098      \n","=================================================================\n","Total params: 28,517,618\n","Trainable params: 28,344,882\n","Non-trainable params: 172,736\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkXW82PKibDl","executionInfo":{"elapsed":6366,"status":"ok","timestamp":1628219457980,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"0d134991-c291-48bf-a8d5-9eac110f60be"},"source":["classifier_t =  get_efficientnet_b5_encoder(input_shape, pretrained=False)\n","modelT = models.Sequential()\n","modelT.add(classifier_t)\n","modelT.add(layers.Dense(n_classes))\n","modelT.summary()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_3 (Functional)         (None, 2048)              28513520  \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 4098      \n","=================================================================\n","Total params: 28,517,618\n","Trainable params: 28,344,882\n","Non-trainable params: 172,736\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"-j9x5-Py5RcX"},"source":["# Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNmm_kkr5RcZ","executionInfo":{"elapsed":13,"status":"ok","timestamp":1628219457981,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"68be4c5c-3e16-4a6a-bad1-fb5ee6fab1c3"},"source":["from tensorflow.python.client import device_lib\n","\n","def get_available_gpus():\n","    local_device_protos = device_lib.list_local_devices()\n","    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n","get_available_gpus()    "],"execution_count":null,"outputs":[{"data":{"text/plain":["[]"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1iJF97nG5Rcb","executionInfo":{"elapsed":11,"status":"ok","timestamp":1628219457982,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"2b3ad4d5-3e73-4a51-f7ba-ac7a5dc1fc5b"},"source":["%cd /content/drive/MyDrive/Thesis/MPL"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/Thesis/MPL'\n","/content\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"TvcW5Pc2_Cur"},"source":["#@title MPL config\n","import tensorflow as tf\n","\n","\n","\n","\n","# about dataset\n","IMG_SIZE = 416#@param {type:\"integer\"}\n","BATCH_SIZE = 4#@param {type:\"integer\"}\n","# LABEL_FILE_PATH = '/content/cifar/label4000.csv' # google\n","# UNLABEL_FILE_PATH = '/content/cifar/train.csv'\n","\n","_MAX_LEVEL = 10\n","CUTOUT_CONST = 40.\n","TRANSLATE_CONST = 100.\n","REPLACE_COLOR = [128, 128, 128]\n","\n","\n","# LABEL_FILE_PATH = '../input/cifar10/cifar/label4000.csv'  # kaggle\n","# UNLABEL_FILE_PATH = '../input/cifar10/cifar/train.csv'\n","\n","\n","AUGMENT_MAGNITUDE = 8\n","SHUFFLE_SIZE = BATCH_SIZE * 16\n","DATA_LEN = 400  # \n","\n","# about model\n","NUM_XLA_SHARDS = -1\n","BATCH_NORM_EPSILON = 1e-3\n","BATCH_NORM_DECAY = 0.999\n","DROPOUT_RATE = 0.\n","DROPOUT = 0.2\n","NUM_CLASSES = 2#@param {type:\"integer\"}\n","NUM_CLASS = 2#@param {type:\"integer\"}\n","\n","# about training\n","LOG_EVERY = 20\n","SAVE_EVERY = 5\n","TEA_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/PST'\n","STD_SAVE_PATH = '/content/drive/MyDrive/Thesis/weights/PSS'\n","\n","MAX_EPOCHS = 1920\n","MAX_STEPS = MAX_EPOCHS * (int(DATA_LEN / BATCH_SIZE)-1)\n","UDA_WEIGHT = 8  # uda\n","UDA_STEPS = 2000\n","TEST_EVERY = 2\n","GRAD_BOUND = 1e9\n","EMA = 0.995\n","\n","\n","# continue train\n","TEA_CONTINUE = False\n","STD_CONTINUE = False\n","TEA_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/PST'\n","STD_LOAD_PATH = '/content/drive/MyDrive/Thesis/weights/PSS'\n","CONTINUE_EPOCH = 885\n","\n","\n","# about testing\n","# TEST_FILE_PATH = '/content/cifar/test.csv'\n","# TEST_FILE_PATH = '../input/cifar10/cifar/test.csv'\n","TEST_MODEL_PATH = '/content/drive/MyDrive/Thesis/weights/PS'\n","\n","# about UdaCrossEntroy\n","UDA_DATA = 1\n","LABEL_SMOOTHING = 0.15\n","UDA_TEMP = 0.7\n","UDA_THRESHOLD = 0.6\n","\n","# about learning rate\n","STUDENT_LR = 0.0005  # student\n","STUDENT_LR_WARMUP_STEPS = 4000\n","STUDENT_LR_WAIT_STEPS = 2000\n","TEACHER_LR = 0.0005  # teacher\n","TEACHER_LR_WARMUP_STEPS = 1000\n","TEACHER_NUM_WAIT_STEPS = 0\n","\n","LR_DECAY_TYPE = 'cosine'  # constant, exponential, cosine\n","NUM_DECAY_STEPS = 300\n","LR_DECAY_RATE = 0.97\n","\n","# about optimizer\n","OPTIM_TYPE = 'sgd'  # sgd, momentum, rmsprop\n","WEIGHT_DECAY = 5e-4\n","\n","\n","# dtype\n","DTYPE = tf.float32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"JzUpbjZDSWDh"},"source":["#@title Self_aug_func\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import math\n","import tensorflow_addons.image as image_ops\n","\n","# import \n","\n","\n","def autocontrast(image):\n","    lo = tf.cast(tf.reduce_min(image, axis=[0, 1]), tf.float32)\n","    hi = tf.cast(tf.reduce_max(image, axis=[0, 1]), tf.float32)\n","    scale = tf.math.divide(255.0, (hi - lo))\n","    offset = tf.math.multiply(-lo, scale)\n","    image = tf.math.add(\n","        tf.math.multiply(tf.cast(image, tf.float32), scale),\n","        offset\n","    )\n","    image = tf.clip_by_value(image, 0.0, 255.0)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def equalize(image):\n","    # image = tf.cast(image, tf.int32)\n","    # channel = tf.shape(image)[-1]\n","    # for i in range(channel):\n","    #     im = tf.cast(image[:, :, i], tf.int32)\n","    #     histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","    #     nonzero = tf.where(tf.not_equal(histo, 0))\n","    #     nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","    #     step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","    #     print(step)\n","    #     if step == 0:\n","    #         pass\n","    #     else:\n","    #         lut = (tf.cumsum(histo) + (step // 2)) // step\n","    #         lut = tf.concat([[0], lut[:-1]], 0)\n","    #         lut = tf.clip_by_value(lut, 0, 255)\n","    #         # print(lut)\n","    #         image[:, :, i] = tf.gather(lut, image[:, :, i])\n","    #         # image[:, :, i] = im\n","    #     # image[:, :, i] = im\n","\n","    def scale_channel(im, c=0):\n","        \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n","        im = tf.cast(im[:, :, 0], tf.int32)\n","        # Compute the histogram of the image channel.\n","        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n","\n","        # For the purposes of computing the step, filter out the nonzeros.\n","        nonzero = tf.where(tf.not_equal(histo, 0))\n","        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n","        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n","\n","        def build_lut(histo, step):\n","            # Compute the cumulative sum, shifting by step // 2\n","            # and then normalization by step.\n","            lut = (tf.cumsum(histo) + (step // 2)) // step\n","            # Shift lut, prepending with 0.\n","            lut = tf.concat([[0], lut[:-1]], 0)\n","            # Clip the counts to be in range.  This is done\n","            # in the C code for image.point.\n","            return tf.clip_by_value(lut, 0, 255)\n","\n","        # If step is zero, return the original image.  Otherwise, build\n","        # lut from the full histogram and step and then index from it.\n","        result = tf.cond(tf.equal(step, 0),\n","                         lambda: im,\n","                         lambda: tf.gather(build_lut(histo, step), im))\n","        return tf.cast(result, tf.uint8)\n","\n","    s1 = scale_channel(image, 0)\n","    s2 = scale_channel(image, 1)\n","    s3 = scale_channel(image, 2)\n","    image = tf.stack([s1, s2, s3], 2)\n","\n","    return image\n","\n","\n","def invert(image):\n","    image = 255 - image\n","    return image\n","\n","\n","def rotate(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    degree = tf.cond(should_filp, lambda: level, lambda: -level)\n","    degree_to_radians = tf.convert_to_tensor(math.pi / 180., tf.float32)\n","    radians = tf.math.multiply(degree, degree_to_radians)\n","    new_imgsize = tf.cast(tf.math.abs(tf.divide(IMG_SIZE, radians)), tf.int32)\n","    image = tf.image.resize(image, (new_imgsize, new_imgsize))\n","    image = image_ops.rotate(image, radians, fill_mode='constant')\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def posterize(image):\n","    bit = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 4, tf.float32)\n","    shift = tf.cast(8 - bit, image.dtype)\n","    image = tf.bitwise.right_shift(image, shift)\n","    image = tf.bitwise.left_shift(image, shift)\n","    return image\n","\n","\n","def solarize_arg(image):\n","    threahold = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 22, tf.float32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def solarize_add(image, threahold=128):\n","    addition = tf.cast(tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * 2, tf.int32)\n","    threahold = tf.cast(threahold, image.dtype)\n","    image = tf.add(tf.cast(image, tf.int32), addition)\n","    image = tf.cast(tf.clip_by_value(image, 0, 255), tf.uint8)\n","    image = tf.where(image < threahold, image, 255 - image)\n","    return image\n","\n","\n","def color(image, degenetate=None):\n","    if degenetate is None:\n","        degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.8 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def contrast(image):\n","    degenerate = tf.image.rgb_to_grayscale(image)\n","    degenerate = tf.cast(degenerate, tf.int32)\n","\n","    hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n","    mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.\n","    degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n","    degenerate = tf.clip_by_value(degenerate, 0., 255.)\n","    degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n","\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.6 + 0.1, tf.float32)\n","\n","    def _blend():\n","        degen = tf.image.convert_image_dtype(degenerate, tf.float32)\n","        img = tf.image.convert_image_dtype(image, tf.float32)\n","        output = degen + factor * (img - degen)\n","        output = tf.where(\n","            tf.logical_and(tf.less(0., factor), tf.less(factor, 1.)),\n","            x=output,\n","            y=tf.clip_by_value(output, 0., 255.)\n","        )\n","        return tf.image.convert_image_dtype(output, tf.uint8)\n","\n","    pred_fn_pairs = [\n","        (tf.equal(factor, 0.), lambda: degenerate),\n","        (tf.equal(factor, 1.), lambda: image),\n","    ]\n","    image = tf.case(\n","        pred_fn_pairs=pred_fn_pairs,\n","        default=_blend,\n","        exclusive=True,\n","        strict=True,\n","    )\n","    return image\n","\n","\n","def brightness(image):\n","    image = tf.image.adjust_brightness(image, 0.25)\n","    return image\n","\n","\n","def sharpness(image):\n","    factor = tf.cast((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 1.6 + 0.1, tf.float32)\n","    image = tf.cast(image, tf.float32)\n","    image = image_ops.sharpness(image, factor)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_x(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.2, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_x(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def shear_y(image):\n","    level = tf.convert_to_tensor((AUGMENT_MAGNITUDE / _MAX_LEVEL) * 0.2, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    level = tf.cond(should_filp, lambda: level, lambda: -level)\n","\n","    new_size = tf.cast(IMG_SIZE * 1.1, dtype=tf.int32)\n","    image = tf.image.resize(image, (new_size, new_size))\n","    image = image_ops.shear_y(\n","        image,\n","        level,\n","        replace=tf.convert_to_tensor(REPLACE_COLOR, image.dtype)\n","    )\n","    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n","    image = tf.cast(image, tf.uint8)\n","    return image\n","\n","\n","def translate_x(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # TrueFalse\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [-pixels, 0])\n","    return image\n","\n","\n","def translate_y(image):\n","    level = tf.convert_to_tensor(\n","        (AUGMENT_MAGNITUDE / _MAX_LEVEL) * float(TRANSLATE_CONST), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # TrueFalse\n","    pixels = tf.cond(should_flip, lambda: level, lambda: -level)\n","    image = image_ops.translate(image, [0, -pixels])\n","    return image\n","\n","\n","def cutout(image):\n","    pad_size = tf.cast(\n","        tf.cast(AUGMENT_MAGNITUDE, tf.float32) / _MAX_LEVEL * CUTOUT_CONST,\n","        tf.int32\n","    )\n","    image_height = tf.shape(image)[0]\n","    image_width = tf.shape(image)[1]\n","\n","    # Samples the center location in the image where the zero mask is applied.\n","    cutout_center_height = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_height,\n","        dtype=tf.int32)\n","\n","    cutout_center_width = tf.random.uniform(\n","        shape=[], minval=0, maxval=image_width,\n","        dtype=tf.int32)\n","\n","    lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n","    upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n","    left_pad = tf.maximum(0, cutout_center_width - pad_size)\n","    right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n","\n","    cutout_shape = [image_height - (lower_pad + upper_pad),\n","                    image_width - (left_pad + right_pad)]\n","    padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n","    mask = tf.pad(\n","        tf.zeros(cutout_shape, dtype=image.dtype),\n","        padding_dims, constant_values=1)\n","    mask = tf.expand_dims(mask, -1)\n","    mask = tf.tile(mask, [1, 1, 3])\n","    image = tf.where(\n","        tf.equal(mask, 0),\n","        tf.ones_like(image, dtype=image.dtype) * REPLACE_COLOR,\n","        image)\n","    return image\n","\n","\n","def identity(image):\n","    return tf.identity(image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"8e7vEGndSLme","executionInfo":{"elapsed":6,"status":"ok","timestamp":1628219459021,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"274c335c-ad8e-4547-f4e3-ca65bec421c1"},"source":["#@title Self_aug_util\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import tensorflow as tf\n","\n","# from self_aug_func import *\n","\n","_MAX_LEVEL = 10\n","\n","\n","\n","def _enhance_level_to_arg(level):\n","    return (tf.cast((level / _MAX_LEVEL) * 1.8 + 0.1, tf.float32),)\n","\n","\n","def _translate_level_to_arg(level, translate_const):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * float(translate_const), tf.float32)\n","    should_flip = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool)  # TrueFalse\n","    final_tensor = tf.cond(should_flip, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _rotate_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 30, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def _shear_level_to_arg(level):\n","    level = tf.convert_to_tensor((level / _MAX_LEVEL) * 0.3, tf.float32)\n","    should_filp = tf.cast(\n","        tf.floor(tf.random.uniform([]) + 0.5),\n","        tf.bool\n","    )\n","    final_tensor = tf.cond(should_filp, lambda: level, lambda: -level)\n","    return final_tensor\n","\n","\n","def level_to_arg(cutout_const, translate_const):\n","    '''\n","    image\n","    :param cutout_const:\n","    :param translate_const:\n","    :return: type:dict\n","    '''\n","    no_arg = lambda level: ()\n","    posterize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 4,\n","        tf.float32\n","    )\n","    solarize_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 256,\n","        tf.float32\n","    )\n","    solarize_add_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * 110,\n","        tf.float32\n","    )\n","    cutout_arg = lambda level: tf.cast(\n","        tf.cast(level, tf.float32) / _MAX_LEVEL * cutout_const,\n","        tf.float32\n","    )\n","    translate_arg = lambda level: _translate_level_to_arg(level, translate_const)\n","\n","    args = {\n","        'Identity': no_arg,\n","        'AutoContrast': no_arg,\n","        'Equalize': no_arg,\n","        'Invert': no_arg,\n","        'Rotate': _rotate_level_to_arg,\n","        'Posterize': posterize_arg,\n","        'Solarize': solarize_arg,\n","        'SplarizeAdd': solarize_add_arg,\n","        'Color': _enhance_level_to_arg,\n","        'Contrast': _enhance_level_to_arg,\n","        'Brightness': _enhance_level_to_arg,\n","        'Sharpness': _enhance_level_to_arg,\n","        'ShearX': _shear_level_to_arg,\n","        'ShearY': _shear_level_to_arg,\n","        'Cutout': cutout_arg,\n","        'TranslateX': translate_arg,\n","        'TranslateY': translate_arg,\n","    }\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    NAME_TO_FUNC = {\n","        'AutoContrast': autocontrast,\n","        'Equalize': equalize,\n","        'Invert': invert,\n","        'Rotate': rotate,\n","        'Posterize': posterize,\n","        'Solarize': solarize_arg,\n","        'SolarizeAdd': solarize_add,\n","        'Color': color,\n","        'Contrast': contrast,\n","        'Brightness': brightness,\n","        'Sharpness': sharpness,\n","        'ShearX': shear_x,\n","        'ShearY': shear_y,\n","        'TranslateX': translate_x,\n","        'TranslateY': translate_y,\n","        'Cutout': cutout,\n","        'Identity': identity,\n","    }\n","\n","    available_ops = [\n","        'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","        'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","    ]\n","\n","    for (i, op_name) in enumerate(available_ops):\n","        func = NAME_TO_FUNC[op_name]\n","        args = level_to_arg(4, 4)[op_name](16)\n","        print(args)\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["()\n","()\n","()\n","tf.Tensor(48.0, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(409.6, shape=(), dtype=float32)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","(<tf.Tensor: shape=(), dtype=float32, numpy=2.98>,)\n","tf.Tensor(-0.48, shape=(), dtype=float32)\n","tf.Tensor(0.48, shape=(), dtype=float32)\n","tf.Tensor(-6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n","tf.Tensor(6.4, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"8lCvf2VERloi"},"source":["#@title Self_augment\n","'''\n","reference:\n","https://github.com/google-research/google-research/tree/1f1741a985a0f2e6264adae985bde664a7993bd2/flax_models/cifar/datasets\n","'''\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","\n","'''\n","augment.py augment.pyline 5354\n","tensorflow.contrib\n","pip install tensorflow-addons\n","'''\n","import os\n","\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","\n","# from self_aug_func import *\n","\n","# augment\n","NAME_TO_FUNC = {\n","    'AutoContrast': autocontrast,\n","    'Equalize': equalize,\n","    'Invert': invert,\n","    'Rotate': rotate,\n","    'Posterize': posterize,\n","    'Solarize': solarize_arg,\n","    'SolarizeAdd': solarize_add,\n","    'Color': color,\n","    'Contrast': contrast,\n","    'Brightness': brightness,\n","    'Sharpness': sharpness,\n","    'ShearX': shear_x,\n","    'ShearY': shear_y,\n","    'TranslateX': translate_x,\n","    'TranslateY': translate_y,\n","    'Cutout': cutout,\n","    'Identity': identity,\n","}\n","# \n","REPLACE_FUNCS = frozenset({\n","    'Rotate',\n","    'TranslateX',\n","    'ShearX',\n","    'SHearY',\n","    'TranslateY',\n","    'Cutout',\n","})\n","\n","\n","class RandAugment(object):\n","    def __init__(self, num_layers=2, magnitude=None, cutout_const=40, translate_const=100., available_ops=None):\n","        '''\n","        reference: https://arxiv.org/abs/1909.13719\n","        :param num_layers:\n","        :param magnitude:\n","        :param cutout_const:\n","        :param translate_const:\n","        :param avalilable_ops:\n","        '''\n","        super(RandAugment, self).__init__()\n","        self.num_layers = num_layers\n","        self.cutout_const = float(cutout_const)\n","        self.translate_const = float(translate_const)\n","        if available_ops is None:\n","            available_ops = [\n","                'AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize',\n","                'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout',\n","                'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness',\n","            ]\n","        self.available_ops = available_ops\n","        self.magnitude = magnitude\n","\n","    def distort(self, image):\n","        '''\n","\n","        :param image:  shape:[HWC] C=3\n","        :return: \n","        '''\n","        input_image_type = image.dtype\n","        image = tf.clip_by_value(image, tf.cast(0, input_image_type), tf.cast(255, input_image_type))\n","        image = tf.cast(image, tf.uint8)\n","\n","        prob = tf.random.uniform([], 0.2, 0.8, tf.float32)\n","\n","        for _ in range(self.num_layers):\n","            op_to_select = tf.random.uniform([], minval=0, maxval=len(self.available_ops), dtype=tf.int32)\n","            for (i, op_name) in enumerate(self.available_ops):\n","                func = NAME_TO_FUNC[op_name]  # \n","                if i == op_to_select:\n","                    flag = tf.random.uniform([], 0., 1., prob.dtype)\n","                    if tf.math.greater_equal(prob, flag):\n","                        image = func(image)\n","\n","        image = tf.cast(image, dtype=input_image_type)\n","        return image\n","\n","\n","def unlabel_image(img_file, label):\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # \n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","\n","    aug_image, some_info = aug.distort(img)\n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, tf.float32) / 255.0\n","    ori_image = tf.cast(ori_image, tf.float32) / 255.0\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"iSpsmsJEDxj1"},"source":["#@title UDa\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","\n","# import config\n","# from Model import Wrn28k\n","\n","\n","def UdaCrossEntroy(all_logits, l_labels, global_step):\n","    batch_size = BATCH_SIZE\n","    uda_data = UDA_DATA\n","    logits = {}\n","    labels = {}\n","    cross_entroy = {}\n","    masks = {}\n","    #  label ori aug \n","    logits['l'], logits['ori'], logits['aug'] = tf.split(\n","        all_logits,\n","        [batch_size, batch_size * uda_data, batch_size * uda_data],\n","        axis=0,\n","    )\n","    # \n","    labels['l'] = l_labels\n","\n","    # ------------loss---------\n","    # part1\n","    cross_entroy['l'] = tf.losses.CategoricalCrossentropy(\n","        from_logits=True,\n","        label_smoothing=LABEL_SMOOTHING,\n","        reduction=keras.losses.Reduction.NONE,)(labels['l'], logits['l'])\n","    '''\n","    probs = tf.nn.softmax(logits['l'], axis=-1)  # 10\n","    correct_probs = tf.reduce_sum(labels['l'] * probs, axis=-1)  # label \n","    # l_threshold\n","    r = tf.cast(global_step, tf.float32) / tf.convert_to_tensor(MAX_STEPS, dtype=tf.float32)\n","    num_classes = tf.convert_to_tensor(NUM_CLASSES, tf.float32)\n","    l_threshold = r * (1. - 1. / num_classes) + 1. / num_classes\n","    masks['l'] = tf.math.less_equal(correct_probs, l_threshold)\n","    masks['l'] = tf.cast(masks['l'], tf.float32)\n","    masks['l'] = tf.stop_gradient(masks['l'])  # l_threahold,10\n","    '''\n","    cross_entroy['l'] = tf.reduce_sum(cross_entroy['l']) / float(batch_size)\n","\n","    # part2: \n","    labels['ori'] = tf.nn.softmax(logits['ori'] / tf.convert_to_tensor(UDA_TEMP), axis=-1)\n","    labels['ori'] = tf.stop_gradient(labels['ori'])\n","    # tf.nn.log_softmax: 3o1o2o3 ==>\n","    # b = log(sum(exp(o1) + exp(o2) + exp(o3)))  new_o1=o1-b, new_o2=o2-b ... \n","    cross_entroy['u'] = (\n","            labels['ori'] * tf.nn.log_softmax(logits['aug'], axis=-1)\n","    )\n","\n","    largest_probs = tf.reduce_max(labels['ori'], axis=-1, keepdims=True)\n","\n","    masks['u'] = tf.math.greater_equal(largest_probs, tf.constant(UDA_THRESHOLD))  # \n","    masks['u'] = tf.cast(masks['u'], DTYPE)\n","    masks['u'] = tf.stop_gradient(masks['u'])\n","    # oriclass i = 1, 0\n","    # augclass i0\n","    cross_entroy['u'] = tf.reduce_sum(-cross_entroy['u'] * masks['u']) / \\\n","                        tf.convert_to_tensor((batch_size * uda_data), dtype=DTYPE)\n","\n","    return logits, labels, masks, cross_entroy\n","\n","\n","# if __name__ == '__main__':\n","#     # \n","#     l_images = np.random.random((1, 32, 32, 3))\n","#     l_images = tf.convert_to_tensor(l_images, dtype=DTYPE)\n","#     ori_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     ori_images = tf.convert_to_tensor(ori_images, dtype=DTYPE)\n","#     aug_images = np.random.random((1 * UDA_DATA, 32, 32, 3))\n","#     aug_images = tf.convert_to_tensor(aug_images, dtype=DTYPE)\n","#     all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [3, 32, 32, 3]\n","\n","#     l_labels = np.array([2])\n","#     l_labels = tf.convert_to_tensor(l_labels, dtype=tf.int32)\n","#     l_labels = tf.raw_ops.OneHot(indices=l_labels, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","#     l_labels = tf.cast(l_labels, DTYPE)\n","\n","#     # teacher\n","#     teacher = Wrn28k(num_inp_filters=3, k=2)\n","#     output = teacher(x=all_images)  # shape=[15, 10]\n","\n","#     logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, 1)\n","#     print('logits: ', logits.keys())\n","#     print('labels: ', labels.keys())\n","#     print('masks: ', masks.keys())\n","#     # print('cross entroy: ', cross_entroy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"NEFmWImkDNid"},"source":["#@title Test\n","import os\n","\n","# from WideResnet import WideResnet\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","import pandas as pd\n","\n","# import config\n","\n","\n","def test(student, file_paths, labels):\n","    student.training = False\n","    # \n","    # df_label = pd.read_csv(TEST_FILE_PATH)\n","    # file_paths = df_label['file_name'].values\n","    # labels = df_label['label'].values\n","\n","    # testing\n","    total_num = int(len(labels)/2)\n","    corrent_num = 0\n","    for i in range(total_num):\n","        img_file = file_paths[i]\n","        label = int(labels[i])\n","\n","        # \n","        img = tf.io.read_file(img_file)\n","        img = tf.image.decode_jpeg(img, channels=3)\n","        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","        img = tf.cast(img, dtype=DTYPE) / 255.0\n","        img = tf.expand_dims(img, axis=0)\n","        mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","        std = tf.expand_dims(tf.convert_to_tensor([0.0737, 0.0737, 0.0737], dtype=DTYPE), axis=0)\n","        img = (img - mean) / std\n","\n","        # \n","        output = student(img)\n","        output = tf.nn.softmax(output)\n","        class_index = tf.squeeze(tf.math.argmax(output, axis=1))\n","\n","        if class_index == label:\n","            corrent_num += 1\n","    accuracy = float(corrent_num) / float(total_num) * 100.\n","    student.training = True\n","    return accuracy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"8xd5evraDERY","executionInfo":{"elapsed":9,"status":"ok","timestamp":1628219459452,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"5bd70304-5015-423b-90f5-4d79f1d9448f"},"source":["#@title learning rate\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","# import config\n","\n","\n","class LearningRate(object):\n","    def __init__(self, initial_lr, num_warmup_steps, num_wait_steps=None):\n","        if initial_lr is None:\n","            raise ValueError(f'initial_lr is error in learningRate file')\n","        if num_warmup_steps is None:\n","            raise ValueError(f'num_warmup_steps is error in learningRate file')\n","        if num_wait_steps is None:\n","            raise ValueError(f'num_wait_steps is error in learningRate file')\n","\n","        # initial_lr = initial_lr * BATCH_SIZE / 256\n","        self.initial_lr = initial_lr\n","        self.num_warmup_steps = num_warmup_steps\n","        self.num_wait_steps = num_wait_steps\n","\n","        if LR_DECAY_TYPE == 'constant':\n","            self.lr = tf.constant(self.initial_lr, dtype=tf.float32)\n","\n","        elif LR_DECAY_TYPE == 'exponential':\n","            self.lr = keras.optimizers.schedules.ExponentialDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=NUM_DECAY_STEPS,\n","                decay_rate=LR_DECAY_RATE,\n","            )\n","\n","        elif LR_DECAY_TYPE == 'cosine':\n","            self.lr = keras.experimental.CosineDecay(\n","                initial_learning_rate=self.initial_lr,\n","                decay_steps=MAX_STEPS - self.num_wait_steps - self.num_warmup_steps,\n","                alpha=0.0\n","            )\n","        else:\n","            raise ValueError(f'unknown lr_decay_type in py')\n","\n","    def __call__(self, global_step):\n","        global_step = global_step - self.num_wait_steps\n","        if LR_DECAY_TYPE == 'constant':\n","            learn_rate = self.lr\n","        else:\n","            learn_rate = self.lr.__call__(global_step)\n","\n","        r = tf.constant((global_step + 1), tf.float32) / tf.constant(self.num_warmup_steps, tf.float32)\n","        warmup_lr = self.initial_lr * r\n","        lr = tf.cond(\n","            tf.cast(global_step, tf.int32) < tf.cast(self.num_warmup_steps, tf.int32),\n","            lambda: warmup_lr,\n","            lambda: learn_rate,\n","        )\n","        lr = tf.cond(global_step < 0, lambda: tf.constant(0., tf.float32), lambda: lr)\n","        return lr\n","\n","\n","'''\n","def LearningRate(initial_lr, num_warmup_steps, num_wait_steps):\n","    if initial_lr is None:\n","        raise ValueError(f'initial_lr is error in learningRate file')\n","    if num_warmup_steps is None:\n","        raise ValueError(f'num_warmup_steps is error in learningRate file')\n","    if num_wait_steps is None:\n","        raise ValueError(f'num_wait_steps is error in learningRate file')\n","    initial_lr = initial_lr * BATCH_SIZE / 256\n","    if LR_DECAY_TYPE == 'constant':\n","        lr = tf.constant(initial_lr, dtype=tf.float32)\n","    elif LR_DECAY_TYPE == 'exponential':\n","        lr = keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=NUM_DECAY_STEPS,\n","            decay_rate=LR_DECAY_RATE,\n","        )\n","    elif LR_DECAY_TYPE == 'cosine':\n","        lr = keras.experimental.CosineDecay(\n","            initial_learning_rate=initial_lr,\n","            decay_steps=MAX_STEPS - num_wait_steps - num_warmup_steps,\n","            alpha=0.0\n","        )\n","    else:\n","        raise ValueError(f'unknown lr_decay_type in py')\n","    return lr\n","'''\n","\n","import math\n","def lr_lambda(current_step):\n","    if current_step < 0:\n","        return float(current_step) / float(max(1, 0))\n","\n","    progress = float(current_step - 0) / \\\n","               float(max(1, 10 - 0))\n","    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress)))\n","\n","\n","if __name__ == '__main__':\n","    for i in range(10):\n","        print(lr_lambda(i))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","0.9755282581475768\n","0.9045084971874737\n","0.7938926261462366\n","0.6545084971874737\n","0.5\n","0.34549150281252633\n","0.2061073738537635\n","0.09549150281252633\n","0.024471741852423234\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"9gnHT8aeA7sb"},"source":["#@title C Dataset MLP\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# import config\n","import sys  \n","sys.path.insert(0, '/content/drive/MyDrive/Thesis/MPL')\n","\n","# from Self_augment import RandAugment\n","\n","\n","def normalize_image(img, label):\n","    '''\n","    \n","    :param img:\n","    :param label:\n","    :return:\n","    '''\n","    return tf.cast(img, tf.float32) / 255.0, label\n","\n","\n","# \n","def label_image(img_file, label):\n","    '''\n","      labelonehot\n","    :param img_file:\n","    :param label:\n","    :return:\n","    '''\n","    # \n","    img = tf.io.read_file(img_file)\n","    # img = tf.image.grayscale_to_rgb(img, name=None)\n","    # img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE, 3] )\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.random_flip_left_right(img)\n","    img = tf.image.resize(img, (IMG_SIZE + 5, IMG_SIZE + 5))\n","    img = tf.image.random_crop(img, (IMG_SIZE, IMG_SIZE, 3))\n","    img = tf.cast(img, DTYPE) / 255.0\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.3684, 0.3684, 0.3684], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.0740, 0.0740, 0.0740], dtype=DTYPE), axis=0)\n","    img = (img-mean)/std\n","    # \n","    label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","    label = tf.cast(label, dtype=DTYPE)\n","    return {'images': img, 'labels': label}\n","\n","\n","# \n","def unlabel_image(img_file, label):\n","    '''\n","    \n","    :param img_file:\n","    :param label:\n","    :return: ori_image aug_images\n","    '''\n","    img = tf.io.read_file(img_file)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ori_image = img  # \n","\n","    aug = RandAugment(\n","        cutout_const=IMG_SIZE // 8,\n","        translate_const=IMG_SIZE // 8,\n","        magnitude=AUGMENT_MAGNITUDE,\n","    )\n","    # aug_image = mask_label(img)\n","    aug_image = aug.distort(img)\n","    \n","    # aug_image = augment.cutout(aug_image, pad_size=IMG_SIZE // 8, replace=128)\n","    aug_image = tf.image.random_flip_left_right(aug_image)\n","\n","    aug_image = tf.cast(aug_image, DTYPE) / 255.0\n","    ori_image = tf.cast(ori_image, DTYPE) / 255.0\n","\n","    mean = tf.expand_dims(tf.convert_to_tensor([0.2153,0.2153,0.2153], dtype=DTYPE), axis=0)\n","    std = tf.expand_dims(tf.convert_to_tensor([0.2196, 0.2196, 0.2196], dtype=DTYPE), axis=0)\n","\n","    aug_image = (aug_image-mean)/std\n","    ori_image = (ori_image-mean)/std\n","\n","    return {'ori_images': ori_image, 'aug_images': aug_image}\n","\n","\n","def merge_dataset(label_data, unlabel_data):\n","    return label_data['images'], label_data['labels'], unlabel_data['ori_images'], unlabel_data['aug_images']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9pzS2iIWfvl"},"source":["dfS['label'].loc[(dfS.label != 'Normal')] = 'Tumor'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJRqMt48W7Ge","executionInfo":{"elapsed":11,"status":"ok","timestamp":1628219459734,"user":{"displayName":"Paula Mpembe","photoUrl":"","userId":"04981801455453252449"},"user_tz":-60},"outputId":"d5c62d75-e73f-40b3-c794-e9eb0e151488"},"source":["dfS.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>mask</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/0.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/0.jpg</td>\n","      <td>Tumor</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/1.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/1.jpg</td>\n","      <td>Tumor</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/2.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/2.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/3.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/3.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Thesis/pos/xS/4.jpg</td>\n","      <td>/content/drive/MyDrive/Thesis/pos/yS/4.jpg</td>\n","      <td>Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        image  ...   label\n","0  /content/drive/MyDrive/Thesis/pos/xS/0.jpg  ...   Tumor\n","1  /content/drive/MyDrive/Thesis/pos/xS/1.jpg  ...   Tumor\n","2  /content/drive/MyDrive/Thesis/pos/xS/2.jpg  ...  Normal\n","3  /content/drive/MyDrive/Thesis/pos/xS/3.jpg  ...  Normal\n","4  /content/drive/MyDrive/Thesis/pos/xS/4.jpg  ...  Normal\n","\n","[5 rows x 3 columns]"]},"execution_count":25,"metadata":{"tags":[]},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8nsCaSe87M0f","outputId":"c89c1316-d68f-4fca-95a5-9c5ba8bbf88a"},"source":["#@title C MLPTrain\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import tensorflow_addons as tfa\n","# from WideResnet import WideResnet\n","from copy import deepcopy\n","import sklearn\n","from sklearn import preprocessing\n","\n","# import config\n","# from Model import Wrn28k\n","# from UdaCrossEntroy import UdaCrossEntroy\n","# from learningRate import LearningRate\n","# from Dataset import label_image\n","# from Dataset import unlabel_image\n","# from Dataset import merge_dataset\n","# from test import test\n","\n","\n","def my_update(model, model_):\n","    for i in range(len(model_)):\n","        model.weights[i] = model.weights[i].assign(\n","            model.weights[i]*(1-EMA)+model_[i]*EMA)\n","    model_ = deepcopy(model.weights)\n","    return model, model_\n","\n","\n","if __name__ == '__main__':\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","    #  batch_size=BATCH_SIZE\n","    # df_label = pd.read_csv(LABEL_FILE_PATH)\n","    le = preprocessing.LabelEncoder()\n","    dfS['label'] = le.fit_transform(dfS.label.values)\n","\n","    u_file_paths = []\n","    u_labels = []\n","    for i in range(0, len(data)): \n","      path = root + data[\"fullPath\"][i]#2499\n","      path = path.replace('\\\\', '/')\n","      path = path.replace('.png', '.jpg')\n","      u_file_paths.append(path)\n","      if data[\"Status\"][i] ==\"Cancer\":\n","        u_labels.append(3)\n","      elif data[\"Status\"][i] == \"Normal\":\n","        u_labels.append(1)\n","      elif data[\"Status\"][i] == \"Benign\" :\n","        u_labels.append(2)\n","  \n","    train_dfS = dfS[:int(len(dfS)*0.7)] \n","    test_dfS = dfS[-int(len(dfS)*0.7):]\n","    t_file_paths = test_dfS['image'].values\n","    t_labels = test_dfS['label'].values\n","    file_paths = train_dfS['image'].values\n","    labels = train_dfS['label'].values\n","    ds_label_train = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n","    ds_label_train = ds_label_train \\\n","        .map(label_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=50) \\\n","        .batch(BATCH_SIZE, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    #  batch_size=BATCH_SIZE*UDA_DATA\n","    # df_unlabel = pd.read_csv(UNLABEL_FILE_PATH)\n","    # file_paths = df_unlabel['name'].values\n","    # labels = df_unlabel['label'].values\n","    ds_unlabel_train = tf.data.Dataset.from_tensor_slices((u_file_paths, u_labels))\n","    ds_unlabel_train = ds_unlabel_train \\\n","        .map(unlabel_image, num_parallel_calls=AUTOTUNE) \\\n","        .shuffle(buffer_size=50) \\\n","        .batch(BATCH_SIZE * UDA_DATA, drop_remainder=True) \\\n","        .prefetch(AUTOTUNE)\n","\n","    # \n","    ds_train = tf.data.Dataset.zip((ds_label_train, ds_unlabel_train))\n","    ds_train = ds_train.map(merge_dataset)\n","\n","    # teacher\n","    if TEA_CONTINUE:\n","        print('continue teacher training')\n","        teacher = modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","        teacher.load_weights(TEA_LOAD_PATH)\n","        teacher.training = True\n","    else:\n","        # teacher = Wrn28k(num_inp_filters=3, k=2)\n","        teacher =  modelT#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","\n","    # student\n","    if STD_CONTINUE:\n","        print('continue student training')\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape,  pretrained=False)\n","        student.load_weights(STD_LOAD_PATH)\n","        student.training = True\n","        student = tf.saved_model.load(STD_LOAD_PATH)\n","    else:\n","        # student = Wrn28k(num_inp_filters=3, k=2)\n","        student =  modelS#efficientunet.get_efficient_unet_b0(input_shape, pretrained=False)\n","    student_ = student.weights\n","\n","    # teacherUdaCrossEntroy\n","    mpl_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","    # student PSteacherUdaCrossEntroy\n","    s_unlabel_loss = tf.losses.CategoricalCrossentropy(\n","        label_smoothing=LABEL_SMOOTHING,\n","        from_logits=True,\n","        reduction=tf.keras.losses.Reduction.NONE,\n","    )\n","\n","    s_label_loss = tf.losses.CategoricalCrossentropy(\n","        reduction=tf.keras.losses.Reduction.NONE,\n","        from_logits=True,\n","    )\n","\n","    # teacher\n","    Tea_lr_fun = LearningRate(\n","        TEACHER_LR,\n","        TEACHER_LR_WARMUP_STEPS,\n","        TEACHER_NUM_WAIT_STEPS\n","    )\n","    # student\n","    Std_lr_fun = LearningRate(\n","        STUDENT_LR,\n","        STUDENT_LR_WARMUP_STEPS,\n","        STUDENT_LR_WAIT_STEPS\n","    )\n","\n","    global_step = 62*CONTINUE_EPOCH\n","    print(f'start training from global step {global_step}......')\n","    TBacc = 0.78\n","    Tacc = 0\n","    SBacc = 0.31\n","    Sacc = 0\n","    epochs = MAX_EPOCHS - CONTINUE_EPOCH\n","    for epoch in range(epochs):\n","        TLOSS = 0\n","        TLOSS_1 = 0\n","        TLOSS_2 = 0\n","        TLOSS_3 = 0\n","        SLOSS = 0\n","        for batch_idx, (l_images, l_labels, ori_images, aug_images) in enumerate(ds_train):\n","            global_step += 1\n","            all_images = tf.concat([l_images, ori_images, aug_images], axis=0)  # shape [15, 32, 32, 3]\n","            u_aug_and_l_images = tf.concat([aug_images, l_images], axis=0)\n","            # step1teacher\n","            with tf.GradientTape() as t_tape:\n","                output = teacher(all_images)  # shape=[15, 10]\n","                logits, labels, masks, cross_entroy = UdaCrossEntroy(output, l_labels, global_step)\n","            # step21st call student -----------------------------\n","            with tf.GradientTape() as s_tape:\n","                logits['s_on_aug_and_l'] = student(u_aug_and_l_images)  # shape=[8, 10]\n","                logits['s_on_u'], logits['s_on_l_old'] = tf.split(\n","                    logits['s_on_aug_and_l'],\n","                    [aug_images.shape[0], l_images.shape[0]],\n","                    axis=0\n","                )\n","                cross_entroy['s_on_u'] = s_unlabel_loss(\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], -1)),\n","                    y_pred=logits['s_on_u']\n","                )\n","                # \n","                cross_entroy['s_on_u'] = tf.reduce_sum(cross_entroy['s_on_u']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=tf.float32)\n","                SLOSS += cross_entroy['s_on_u']\n","                # for taylor\n","                cross_entroy['s_on_l_old'] = s_label_loss(\n","                    y_true=labels['l'],\n","                    y_pred=logits['s_on_l_old']\n","                )\n","\n","                cross_entroy['s_on_l_old'] = tf.reduce_sum(cross_entroy['s_on_l_old']) / \\\n","                                             tf.convert_to_tensor(BATCH_SIZE, dtype=tf.float32)\n","            # student-------\n","            StudentLR = Std_lr_fun.__call__(global_step=global_step)\n","            StdOptim = keras.optimizers.SGD(\n","                learning_rate=StudentLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # StdOptim = keras.optimizers.Adam(learning_rate=StudentLR)\n","            GStud_unlabel = s_tape.gradient(cross_entroy['s_on_u'], student.trainable_variables)\n","            GStud_unlabel, _ = tf.clip_by_global_norm(GStud_unlabel, GRAD_BOUND)\n","            StdOptim.apply_gradients(zip(GStud_unlabel, student.trainable_variables))\n","            # \n","            student, student_ = my_update(student, student_)\n","\n","            # step3: 2nd call student ------------------------------\n","            logits['s_on_l_new'] = student(l_images)\n","            cross_entroy['s_on_l_new'] = s_label_loss(\n","                y_true=labels['l'],\n","                y_pred=logits['s_on_l_new']\n","            )\n","            cross_entroy['s_on_l_new'] = tf.reduce_sum(cross_entroy['s_on_l_new']) / \\\n","                                         tf.convert_to_tensor(BATCH_SIZE, dtype=DTYPE)\n","            dot_product = cross_entroy['s_on_l_new'] - cross_entroy['s_on_l_old']\n","            limit = 3.0**(0.5)\n","            moving_dot_product = tf.random_uniform_initializer(minval=-limit, maxval=limit)(shape=dot_product.shape)\n","            moving_dot_product = tf.Variable(initial_value=moving_dot_product, trainable=False, dtype=DTYPE)\n","            moving_dot_product_update = moving_dot_product.assign_sub(0.01 * (moving_dot_product - dot_product))\n","            dot_product = dot_product - moving_dot_product\n","            dot_product = tf.stop_gradient(dot_product)\n","            # step4: teacher\n","            with t_tape:\n","                # label = tf.math.argmax(tf.nn.softmax(logits['aug'], axis=-1), axis=-1)\n","                # label = tf.raw_ops.OneHot(indices=label, depth=NUM_CLASSES, on_value=1.0, off_value=0)\n","                cross_entroy['mpl'] = mpl_loss(\n","                    # y_true=tf.stop_gradient(label),\n","                    y_true=tf.stop_gradient(tf.nn.softmax(logits['aug'], axis=-1)),\n","                    y_pred=logits['aug']\n","                )  # \n","                cross_entroy['mpl'] = tf.reduce_sum(cross_entroy['mpl']) / \\\n","                                      tf.convert_to_tensor(BATCH_SIZE * UDA_DATA, dtype=DTYPE)\n","                uda_weight = UDA_WEIGHT * tf.math.minimum(\n","                    1., tf.cast(global_step, DTYPE) / float(UDA_STEPS)\n","                )\n","                # if StudentLR == 0:\n","                #     dot_product = 0\n","                teacher_loss = cross_entroy['u'] * uda_weight + \\\n","                               cross_entroy['l'] + \\\n","                               cross_entroy['mpl'] * dot_product\n","\n","                TLOSS += teacher_loss\n","                TLOSS_1 += (cross_entroy['u'] * uda_weight)\n","                TLOSS_2 += cross_entroy['l']\n","                TLOSS_3 += cross_entroy['mpl'] * dot_product\n","            # teacher-------\n","            TeacherLR = Tea_lr_fun.__call__(global_step=global_step)\n","            TeaOptim = keras.optimizers.SGD(\n","                learning_rate=TeacherLR,\n","                momentum=0.9,\n","                nesterov=True,\n","                # weight_decay=5e-4,\n","            )\n","            # TeaOptim = keras.optimizers.Adam(learning_rate=TeacherLR)\n","            GTea = t_tape.gradient(teacher_loss, teacher.trainable_variables)\n","            GTea, _ = tf.clip_by_global_norm(GTea, GRAD_BOUND)\n","            TeaOptim.apply_gradients(zip(GTea, teacher.trainable_variables))\n","\n","            if (batch_idx + 1) % LOG_EVERY == 0:\n","                TLOSS = TLOSS / LOG_EVERY\n","                TLOSS_1 = TLOSS_1 / LOG_EVERY\n","                TLOSS_2 = TLOSS_2 / LOG_EVERY\n","                TLOSS_3 = TLOSS_3 / LOG_EVERY\n","                SLOSS = SLOSS / LOG_EVERY\n","                print(f'global: %4d' % global_step + ',[epoch:%4d/' % (epoch+CONTINUE_EPOCH) + 'EPOCH: %4d] \\t' % epochs\n","                      + '[U:%.4f' % (TLOSS_1) + ', L:%.4f' % (TLOSS_2) + ', M:%.4f' % (\n","                          TLOSS_3) + ']' + '[TLoss: %.4f]' % TLOSS + '/[SLoss: %.4f]' % SLOSS\n","                      + '\\t[TLR: %.6f' % TeacherLR + ']/[SLR: %.6f]' % StudentLR)\n","                TLOSS = 0\n","                TLOSS_1 = 0\n","                TLOSS_2 = 0\n","                TLOSS_3 = 0\n","                SLOSS = 0\n","        # teachertestacc\n","        if epoch % 5 == 0:\n","            Tacc = test(teacher, t_file_paths, t_labels)\n","            print(f'testing teacher model ... acc: {Tacc}')\n","        # studenttestaccstudent\n","        if (StudentLR > 0) and (epoch % 5 == 0):\n","            Sacc = test(student, t_file_paths, t_labels)\n","            print(f'testing ... acc: {Sacc}')\n","        # weights\n","        if Tacc > TBacc:\n","            Tsave_path = TEA_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            teacher.save_weights(Tsave_path)\n","            # tf.saved_model.save(teacher, Tsave_path)\n","            TBacc = Tacc\n","            print(f'saving for TBacc {TBacc}, Tpath:{Tsave_path}')\n","        if Sacc > SBacc:\n","            Ssave_path = STD_SAVE_PATH  # + str(epoch + 1) + '_' + str(batch_idx + 1)\n","            student.save_weights(Ssave_path)\n","            SBacc = Sacc\n","            print(f'saving for SBacc {SBacc}, Spath:{Ssave_path}')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","start training from global step 54870......\n"]}]}]}